nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_en_train_pdtb_pt_test/data/', 'log_file': 'data/pdtb_en_train_pdtb_pt_test/log/', 'save_file': 'data/pdtb_en_train_pdtb_pt_test/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 30, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March09-13:32:54', 'log': 'data/pdtb_en_train_pdtb_pt_test/log/March09-13:32:54.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]108it [00:00, 1079.18it/s]304it [00:00, 1590.80it/s]503it [00:00, 1769.70it/s]716it [00:00, 1908.50it/s]930it [00:00, 1991.73it/s]1136it [00:00, 2013.96it/s]1366it [00:00, 2105.97it/s]1577it [00:00, 2101.91it/s]1788it [00:00, 2072.24it/s]1999it [00:01, 2081.84it/s]2208it [00:01, 2031.90it/s]2412it [00:01, 2025.21it/s]2621it [00:01, 2042.81it/s]2834it [00:01, 2066.31it/s]3042it [00:01, 2066.39it/s]3249it [00:01, 2067.17it/s]3463it [00:01, 2088.37it/s]3672it [00:01, 2065.62it/s]3879it [00:01, 2023.07it/s]4088it [00:02, 2040.89it/s]4293it [00:02, 1971.35it/s]4494it [00:02, 1981.09it/s]4693it [00:02, 1980.59it/s]4902it [00:02, 2012.13it/s]5112it [00:02, 2036.42it/s]5316it [00:02, 1353.81it/s]5533it [00:02, 1533.21it/s]5719it [00:03, 1610.68it/s]5945it [00:03, 1774.11it/s]6147it [00:03, 1837.72it/s]6358it [00:03, 1910.59it/s]6559it [00:03, 1928.13it/s]6768it [00:03, 1972.20it/s]6973it [00:03, 1994.10it/s]7179it [00:03, 2011.44it/s]7386it [00:03, 2019.28it/s]7590it [00:03, 2013.14it/s]7793it [00:04, 2013.20it/s]7996it [00:04, 2015.35it/s]8199it [00:04, 2014.57it/s]8401it [00:04, 2002.51it/s]8605it [00:04, 2010.96it/s]8819it [00:04, 2049.17it/s]9025it [00:04, 2032.73it/s]9229it [00:04, 2021.67it/s]9432it [00:04, 1985.17it/s]9648it [00:04, 2035.09it/s]9864it [00:05, 2071.85it/s]10078it [00:05, 2090.65it/s]10288it [00:05, 2066.50it/s]10506it [00:05, 2099.22it/s]10717it [00:05, 2056.99it/s]10930it [00:05, 2076.42it/s]11140it [00:05, 2082.93it/s]11350it [00:05, 2087.34it/s]11559it [00:05, 2085.20it/s]11768it [00:05, 2086.31it/s]11977it [00:06, 2018.76it/s]12180it [00:06, 1960.21it/s]12379it [00:06, 1967.90it/s]12547it [00:06, 1972.44it/s]
0it [00:00, ?it/s]195it [00:00, 1943.90it/s]403it [00:00, 2019.49it/s]605it [00:00, 2012.09it/s]807it [00:00, 1987.53it/s]1006it [00:00, 1972.48it/s]1165it [00:00, 1983.84it/s]
0it [00:00, ?it/s]146it [00:00, 1442.89it/s]313it [00:00, 1575.11it/s]478it [00:00, 1608.06it/s]641it [00:00, 1614.33it/s]803it [00:00, 1565.69it/s]960it [00:00, 1512.57it/s]1039it [00:00, 1550.93it/s]
Time usage: 17.790910720825195
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/30]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 55.88%, Val F1: 17.92% Time: 100.81797313690186 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   7.5,  Val Acc: 24.21%, Val F1:  3.54% Time: 100.81797313690186 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  0.00%,Val Loss:   7.5,  Val Acc:  3.52%, Val F1:  0.28% Time: 100.81797313690186 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 55.88%, Val F1: 17.92% Time: 194.0648410320282 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.6,  Val Acc: 27.55%, Val F1:  5.86% Time: 194.0648410320282 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.6,  Val Acc: 12.88%, Val F1:  0.34% Time: 194.0648410320282 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 68.75%,Val Loss:   6.4,  Val Acc: 55.79%, Val F1: 20.73% Time: 287.3241980075836 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 25.00%,Val Loss:   6.4,  Val Acc: 30.82%, Val F1:  9.69% Time: 287.3241980075836 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 15.62%,Val Loss:   6.4,  Val Acc: 16.74%, Val F1:  1.08% Time: 287.3241980075836 *
 
 
Train time usage: 371.71026849746704
Test time usage: 2.391719341278076
TOP: Test Loss:   6.4,  Test Acc: 53.61%, Test F1: 17.45%
SEC: Test Loss:   6.4,  Test Acc: 33.40%, Test F1: 11.25%
CONN: Test Loss:   6.4,  Test Acc: 14.82%, Test F1:  1.00%
consistency_top_sec: 20.21%,  consistency_sec_conn:  9.53%, consistency_top_sec_conn:  3.56%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        68
 Contingency     0.0000    0.0000    0.0000       270
  Comparison     0.0000    0.0000    0.0000       144
   Expansion     0.5361    1.0000    0.6980       557

    accuracy                         0.5361      1039
   macro avg     0.1340    0.2500    0.1745      1039
weighted avg     0.2874    0.5361    0.3742      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4795    0.5204    0.4991       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.0000    0.0000    0.0000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4270    0.3800    0.4021       200
    Expansion.Instantiation     0.0000    0.0000    0.0000       118
      Expansion.Restatement     0.2302    0.6209    0.3359       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.3340      1039
                  macro avg     0.1033    0.1383    0.1125      1039
               weighted avg     0.2531    0.3340    0.2748      1039

Epoch [2/30]
top-down:TOP: Iter:    400,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   6.3,  Val Acc: 55.02%, Val F1: 30.59% Time: 10.59673261642456 *
top-down:SEC: Iter:    400,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   6.3,  Val Acc: 34.85%, Val F1: 11.55% Time: 10.59673261642456 *
top-down:CONN: Iter:    400,  Train Loss: 3e+01,  Train Acc:  6.25%,Val Loss:   6.3,  Val Acc: 17.77%, Val F1:  1.68% Time: 10.59673261642456 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 53.22%, Val F1: 29.80% Time: 103.63120293617249 *
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.2,  Val Acc: 35.19%, Val F1: 13.05% Time: 103.63120293617249 *
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   6.2,  Val Acc: 19.83%, Val F1:  2.16% Time: 103.63120293617249 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   6.0,  Val Acc: 55.19%, Val F1: 36.93% Time: 196.77323079109192 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 43.75%,Val Loss:   6.0,  Val Acc: 38.88%, Val F1: 17.73% Time: 196.77323079109192 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 25.00%,Val Loss:   6.0,  Val Acc: 21.37%, Val F1:  2.55% Time: 196.77323079109192 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   5.8,  Val Acc: 53.05%, Val F1: 40.28% Time: 290.4407970905304 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   5.8,  Val Acc: 37.60%, Val F1: 16.86% Time: 290.4407970905304 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   5.8,  Val Acc: 22.06%, Val F1:  2.93% Time: 290.4407970905304 *
 
 
Train time usage: 367.97269320487976
Test time usage: 2.403533458709717
TOP: Test Loss:   5.5,  Test Acc: 56.98%, Test F1: 33.62%
SEC: Test Loss:   5.5,  Test Acc: 44.66%, Test F1: 23.55%
CONN: Test Loss:   5.5,  Test Acc: 23.58%, Test F1:  3.72%
consistency_top_sec: 33.11%,  consistency_sec_conn: 19.25%, consistency_top_sec_conn: 13.38%
              precision    recall  f1-score   support

    Temporal     0.3191    0.2206    0.2609        68
 Contingency     0.5758    0.0704    0.1254       270
  Comparison     0.8000    0.1389    0.2367       144
   Expansion     0.5760    0.9659    0.7217       557

    accuracy                         0.5698      1039
   macro avg     0.5677    0.3489    0.3362      1039
weighted avg     0.5902    0.5698    0.4693      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3333    0.4259    0.3740        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5159    0.4869    0.5010       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.8947    0.1328    0.2313       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4000    0.7000    0.5091       200
    Expansion.Instantiation     0.6182    0.5714    0.5939       119
      Expansion.Restatement     0.3598    0.4057    0.3814       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4466      1039
                  macro avg     0.2838    0.2475    0.2355      1039
               weighted avg     0.4813    0.4466    0.4205      1039

Epoch [3/30]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 58.88%, Val F1: 49.04% Time: 16.862776041030884 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   5.6,  Val Acc: 42.15%, Val F1: 21.83% Time: 16.862776041030884 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 28.12%,Val Loss:   5.6,  Val Acc: 23.26%, Val F1:  3.96% Time: 16.862776041030884 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.5,  Val Acc: 60.77%, Val F1: 48.77% Time: 110.14239430427551 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.5,  Val Acc: 44.98%, Val F1: 24.96% Time: 110.14239430427551 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 15.62%,Val Loss:   5.5,  Val Acc: 25.67%, Val F1:  4.28% Time: 110.14239430427551 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   5.4,  Val Acc: 60.34%, Val F1: 39.77% Time: 201.79352927207947 
top-down:SEC: Iter:   1000,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   5.4,  Val Acc: 46.01%, Val F1: 24.97% Time: 201.79352927207947 
top-down:CONN: Iter:   1000,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   5.4,  Val Acc: 25.67%, Val F1:  4.26% Time: 201.79352927207947 
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   5.4,  Val Acc: 62.40%, Val F1: 49.32% Time: 294.8829126358032 *
top-down:SEC: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   5.4,  Val Acc: 48.15%, Val F1: 26.99% Time: 294.8829126358032 *
top-down:CONN: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 37.50%,Val Loss:   5.4,  Val Acc: 26.78%, Val F1:  4.73% Time: 294.8829126358032 *
 
 
Train time usage: 366.1725764274597
Test time usage: 2.3970589637756348
TOP: Test Loss:   5.3,  Test Acc: 60.54%, Test F1: 46.43%
SEC: Test Loss:   5.3,  Test Acc: 48.51%, Test F1: 26.73%
CONN: Test Loss:   5.3,  Test Acc: 20.89%, Test F1:  4.00%
consistency_top_sec: 39.27%,  consistency_sec_conn: 16.84%, consistency_top_sec_conn: 13.47%
              precision    recall  f1-score   support

    Temporal     0.5000    0.2941    0.3704        68
 Contingency     0.5859    0.2132    0.3127       272
  Comparison     0.5140    0.3819    0.4382       144
   Expansion     0.6255    0.8937    0.7359       555

    accuracy                         0.6054      1039
   macro avg     0.5563    0.4457    0.4643      1039
weighted avg     0.5914    0.6054    0.5599      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4286    0.3333    0.3750        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5410    0.5390    0.5400       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4545    0.4688    0.4615       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4074    0.6600    0.5038       200
    Expansion.Instantiation     0.7500    0.5593    0.6408       118
      Expansion.Restatement     0.4486    0.3934    0.4192       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4851      1039
                  macro avg     0.2755    0.2685    0.2673      1039
               weighted avg     0.4731    0.4851    0.4711      1039

Epoch [4/30]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.3,  Val Acc: 61.89%, Val F1: 52.07% Time: 22.98560094833374 *
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 48.84%, Val F1: 25.35% Time: 22.98560094833374 *
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 28.76%, Val F1:  5.43% Time: 22.98560094833374 *
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 62.40%, Val F1: 48.19% Time: 114.66479825973511 
top-down:SEC: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 53.12%,Val Loss:   5.3,  Val Acc: 48.15%, Val F1: 26.73% Time: 114.66479825973511 
top-down:CONN: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 28.50%, Val F1:  5.68% Time: 114.66479825973511 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.4,  Val Acc: 61.63%, Val F1: 50.85% Time: 207.91286826133728 *
top-down:SEC: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 46.78%, Val F1: 29.77% Time: 207.91286826133728 *
top-down:CONN: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 40.62%,Val Loss:   5.4,  Val Acc: 28.93%, Val F1:  6.31% Time: 207.91286826133728 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 60.52%, Val F1: 50.65% Time: 299.4917621612549 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 43.75%,Val Loss:   5.4,  Val Acc: 48.24%, Val F1: 31.12% Time: 299.4917621612549 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 31.25%,Val Loss:   5.4,  Val Acc: 26.87%, Val F1:  5.82% Time: 299.4917621612549 
 
 
Train time usage: 364.36544013023376
Test time usage: 2.38708233833313
TOP: Test Loss:   5.2,  Test Acc: 63.04%, Test F1: 48.54%
SEC: Test Loss:   5.2,  Test Acc: 48.99%, Test F1: 30.96%
CONN: Test Loss:   5.2,  Test Acc: 24.16%, Test F1:  6.93%
consistency_top_sec: 44.85%,  consistency_sec_conn: 18.38%, consistency_top_sec_conn: 16.55%
              precision    recall  f1-score   support

    Temporal     0.7500    0.1324    0.2250        68
 Contingency     0.6071    0.3736    0.4626       273
  Comparison     0.5214    0.5069    0.5141       144
   Expansion     0.6551    0.8502    0.7400       554

    accuracy                         0.6304      1039
   macro avg     0.6334    0.4658    0.4854      1039
weighted avg     0.6302    0.6304    0.6021      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6429    0.1667    0.2647        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5315    0.5056    0.5182       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4631    0.5391    0.4982       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4921    0.4650    0.4781       200
    Expansion.Instantiation     0.7188    0.5798    0.6419       119
      Expansion.Restatement     0.3945    0.6085    0.4787       212
      Expansion.Alternative     0.5000    0.5556    0.5263         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4899      1039
                  macro avg     0.3402    0.3109    0.3096      1039
               weighted avg     0.4889    0.4899    0.4761      1039

Epoch [5/30]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   5.3,  Val Acc: 63.35%, Val F1: 50.74% Time: 29.2791268825531 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   5.3,  Val Acc: 50.13%, Val F1: 31.74% Time: 29.2791268825531 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 25.00%,Val Loss:   5.3,  Val Acc: 29.70%, Val F1:  6.89% Time: 29.2791268825531 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 62.58%, Val F1: 54.13% Time: 122.61919403076172 *
top-down:SEC: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 50.21%, Val F1: 32.15% Time: 122.61919403076172 *
top-down:CONN: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   5.4,  Val Acc: 29.01%, Val F1:  6.82% Time: 122.61919403076172 *
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 60.26%, Val F1: 51.53% Time: 214.199227809906 
top-down:SEC: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   5.6,  Val Acc: 47.98%, Val F1: 30.14% Time: 214.199227809906 
top-down:CONN: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 25.84%, Val F1:  5.62% Time: 214.199227809906 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.4,  Val Acc: 61.46%, Val F1: 51.72% Time: 305.82845544815063 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 49.36%, Val F1: 31.56% Time: 305.82845544815063 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 28.12%,Val Loss:   5.4,  Val Acc: 29.53%, Val F1:  6.67% Time: 305.82845544815063 
 
 
Train time usage: 364.84508419036865
Test time usage: 2.3910810947418213
TOP: Test Loss:   5.1,  Test Acc: 63.52%, Test F1: 52.73%
SEC: Test Loss:   5.1,  Test Acc: 50.05%, Test F1: 31.57%
CONN: Test Loss:   5.1,  Test Acc: 26.66%, Test F1:  8.67%
consistency_top_sec: 47.16%,  consistency_sec_conn: 21.94%, consistency_top_sec_conn: 20.98%
              precision    recall  f1-score   support

    Temporal     0.5926    0.2353    0.3368        68
 Contingency     0.5703    0.5348    0.5520       273
  Comparison     0.4797    0.4897    0.4846       145
   Expansion     0.7023    0.7722    0.7356       553

    accuracy                         0.6352      1039
   macro avg     0.5862    0.5080    0.5273      1039
weighted avg     0.6294    0.6352    0.6262      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.3333    0.4286        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5190    0.6119    0.5616       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4641    0.5547    0.5053       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4722    0.5100    0.4904       200
    Expansion.Instantiation     0.7010    0.5714    0.6296       119
      Expansion.Restatement     0.4306    0.4408    0.4356       211
      Expansion.Alternative     0.4000    0.4444    0.4211         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5005      1039
                  macro avg     0.3261    0.3151    0.3157      1039
               weighted avg     0.4843    0.5005    0.4880      1039

Epoch [6/30]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   5.4,  Val Acc: 63.35%, Val F1: 52.07% Time: 33.98965668678284 
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 50.47%, Val F1: 31.50% Time: 33.98965668678284 
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   5.4,  Val Acc: 29.87%, Val F1:  7.47% Time: 33.98965668678284 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 63.52%, Val F1: 51.44% Time: 127.58605861663818 *
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 51.33%, Val F1: 32.93% Time: 127.58605861663818 *
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 28.76%, Val F1:  7.20% Time: 127.58605861663818 *
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 62.58%, Val F1: 52.80% Time: 219.1011598110199 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 50.30%, Val F1: 31.44% Time: 219.1011598110199 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   5.6,  Val Acc: 29.36%, Val F1:  7.18% Time: 219.1011598110199 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 62.23%, Val F1: 50.72% Time: 310.82837653160095 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 49.53%, Val F1: 32.15% Time: 310.82837653160095 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 29.10%, Val F1:  7.08% Time: 310.82837653160095 
 
 
Train time usage: 363.46108627319336
Test time usage: 2.3843250274658203
TOP: Test Loss:   5.2,  Test Acc: 63.91%, Test F1: 52.58%
SEC: Test Loss:   5.2,  Test Acc: 50.82%, Test F1: 34.54%
CONN: Test Loss:   5.2,  Test Acc: 25.31%, Test F1:  8.53%
consistency_top_sec: 46.68%,  consistency_sec_conn: 19.83%, consistency_top_sec_conn: 18.38%
              precision    recall  f1-score   support

    Temporal     0.6667    0.2941    0.4082        68
 Contingency     0.6275    0.3516    0.4507       273
  Comparison     0.5962    0.4306    0.5000       144
   Expansion     0.6463    0.8773    0.7443       554

    accuracy                         0.6391      1039
   macro avg     0.6341    0.4884    0.5258      1039
weighted avg     0.6357    0.6391    0.6113      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6774    0.3889    0.4941        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5981    0.4664    0.5241       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5323    0.5156    0.5238       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4433    0.6650    0.5320       200
    Expansion.Instantiation     0.6857    0.6102    0.6457       118
      Expansion.Restatement     0.4086    0.4953    0.4478       212
      Expansion.Alternative     0.6000    0.6667    0.6316         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5082      1039
                  macro avg     0.3587    0.3462    0.3454      1039
               weighted avg     0.5068    0.5082    0.4980      1039

Epoch [7/30]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 84.38%,Val Loss:   5.7,  Val Acc: 61.29%, Val F1: 52.07% Time: 40.03179168701172 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   5.7,  Val Acc: 48.41%, Val F1: 31.12% Time: 40.03179168701172 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 27.64%, Val F1:  6.70% Time: 40.03179168701172 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 62.92%, Val F1: 51.92% Time: 131.8855381011963 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   5.7,  Val Acc: 49.61%, Val F1: 31.36% Time: 131.8855381011963 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.7,  Val Acc: 28.24%, Val F1:  7.26% Time: 131.8855381011963 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 62.83%, Val F1: 53.54% Time: 225.15093398094177 *
top-down:SEC: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 65.62%,Val Loss:   5.8,  Val Acc: 51.76%, Val F1: 33.77% Time: 225.15093398094177 *
top-down:CONN: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 31.25%,Val Loss:   5.8,  Val Acc: 29.53%, Val F1:  7.84% Time: 225.15093398094177 *
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 61.80%, Val F1: 52.17% Time: 316.80769634246826 
top-down:SEC: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 68.75%,Val Loss:   5.7,  Val Acc: 49.61%, Val F1: 31.08% Time: 316.80769634246826 
top-down:CONN: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 29.70%, Val F1:  7.49% Time: 316.80769634246826 
 
 
Train time usage: 363.17211055755615
Test time usage: 2.391766309738159
TOP: Test Loss:   5.4,  Test Acc: 64.39%, Test F1: 53.94%
SEC: Test Loss:   5.4,  Test Acc: 49.09%, Test F1: 32.45%
CONN: Test Loss:   5.4,  Test Acc: 26.56%, Test F1:  8.25%
consistency_top_sec: 47.55%,  consistency_sec_conn: 20.79%, consistency_top_sec_conn: 20.12%
              precision    recall  f1-score   support

    Temporal     0.6364    0.3088    0.4158        68
 Contingency     0.5962    0.4652    0.5226       273
  Comparison     0.5398    0.4207    0.4729       145
   Expansion     0.6765    0.8318    0.7461       553

    accuracy                         0.6439      1039
   macro avg     0.6122    0.5066    0.5394      1039
weighted avg     0.6337    0.6439    0.6277      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5556    0.3704    0.4444        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5556    0.4851    0.5179       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5217    0.4688    0.4938       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4133    0.6200    0.4960       200
    Expansion.Instantiation     0.6887    0.6186    0.6518       118
      Expansion.Restatement     0.4188    0.4623    0.4395       212
      Expansion.Alternative     0.5000    0.5556    0.5263         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4909      1039
                  macro avg     0.3322    0.3255    0.3245      1039
               weighted avg     0.4840    0.4909    0.4813      1039

Epoch [8/30]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   5.8,  Val Acc: 62.15%, Val F1: 52.32% Time: 46.19137978553772 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 49.70%, Val F1: 31.38% Time: 46.19137978553772 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   5.8,  Val Acc: 28.58%, Val F1:  7.49% Time: 46.19137978553772 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   5.9,  Val Acc: 64.12%, Val F1: 54.91% Time: 137.97347974777222 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   5.9,  Val Acc: 49.70%, Val F1: 31.54% Time: 137.97347974777222 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.9,  Val Acc: 28.50%, Val F1:  7.66% Time: 137.97347974777222 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 63.00%, Val F1: 53.20% Time: 229.56712222099304 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.1,  Val Acc: 48.84%, Val F1: 30.86% Time: 229.56712222099304 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.1,  Val Acc: 27.98%, Val F1:  7.47% Time: 229.56712222099304 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   6.1,  Val Acc: 62.58%, Val F1: 51.22% Time: 321.1486542224884 
top-down:SEC: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   6.1,  Val Acc: 48.33%, Val F1: 30.33% Time: 321.1486542224884 
top-down:CONN: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   6.1,  Val Acc: 28.15%, Val F1:  7.56% Time: 321.1486542224884 
 
 
Train time usage: 361.588187456131
Test time usage: 2.390744686126709
TOP: Test Loss:   5.7,  Test Acc: 63.14%, Test F1: 50.54%
SEC: Test Loss:   5.7,  Test Acc: 48.51%, Test F1: 30.22%
CONN: Test Loss:   5.7,  Test Acc: 23.77%, Test F1:  8.07%
consistency_top_sec: 46.01%,  consistency_sec_conn: 19.06%, consistency_top_sec_conn: 18.00%
              precision    recall  f1-score   support

    Temporal     0.6667    0.2059    0.3146        68
 Contingency     0.6352    0.3713    0.4687       272
  Comparison     0.4899    0.5069    0.4983       144
   Expansion     0.6592    0.8432    0.7399       555

    accuracy                         0.6314      1039
   macro avg     0.6127    0.4818    0.5054      1039
weighted avg     0.6299    0.6314    0.6076      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.2222    0.3243        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6073    0.4328    0.5054       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4898    0.5625    0.5236       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4172    0.6300    0.5020       200
    Expansion.Instantiation     0.6545    0.6102    0.6316       118
      Expansion.Restatement     0.4000    0.4811    0.4368       212
      Expansion.Alternative     0.3636    0.4444    0.4000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4851      1039
                  macro avg     0.3211    0.3076    0.3022      1039
               weighted avg     0.4876    0.4851    0.4727      1039

Epoch [9/30]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.1,  Val Acc: 62.75%, Val F1: 52.58% Time: 52.56661033630371 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.1,  Val Acc: 48.84%, Val F1: 30.71% Time: 52.56661033630371 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.1,  Val Acc: 27.55%, Val F1:  7.31% Time: 52.56661033630371 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 63.09%, Val F1: 53.54% Time: 144.0518090724945 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   6.1,  Val Acc: 48.24%, Val F1: 31.11% Time: 144.0518090724945 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   6.1,  Val Acc: 27.73%, Val F1:  7.42% Time: 144.0518090724945 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 63.09%, Val F1: 52.48% Time: 235.52258467674255 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.1,  Val Acc: 49.70%, Val F1: 30.84% Time: 235.52258467674255 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 43.75%,Val Loss:   6.1,  Val Acc: 28.58%, Val F1:  7.71% Time: 235.52258467674255 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 62.83%, Val F1: 52.87% Time: 327.0801410675049 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 50.04%, Val F1: 31.78% Time: 327.0801410675049 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 43.75%,Val Loss:   6.1,  Val Acc: 28.15%, Val F1:  7.77% Time: 327.0801410675049 
 
 
Train time usage: 361.0091509819031
Test time usage: 2.394455671310425
TOP: Test Loss:   5.9,  Test Acc: 62.75%, Test F1: 52.61%
SEC: Test Loss:   5.9,  Test Acc: 48.99%, Test F1: 32.86%
CONN: Test Loss:   5.9,  Test Acc: 25.12%, Test F1:  8.84%
consistency_top_sec: 46.97%,  consistency_sec_conn: 20.79%, consistency_top_sec_conn: 19.73%
              precision    recall  f1-score   support

    Temporal     0.5882    0.2941    0.3922        68
 Contingency     0.5852    0.3787    0.4598       272
  Comparison     0.5101    0.5278    0.5188       144
   Expansion     0.6662    0.8162    0.7336       555

    accuracy                         0.6275      1039
   macro avg     0.5874    0.5042    0.5261      1039
weighted avg     0.6182    0.6275    0.6098      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3636    0.4211        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5758    0.4270    0.4903       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4797    0.5547    0.5145       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4275    0.5600    0.4848       200
    Expansion.Instantiation     0.6818    0.6410    0.6608       117
      Expansion.Restatement     0.4151    0.5164    0.4603       213
      Expansion.Alternative     0.4667    0.7778    0.5833         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4899      1039
                  macro avg     0.3224    0.3491    0.3286      1039
               weighted avg     0.4817    0.4899    0.4788      1039

Epoch [10/30]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 63.09%, Val F1: 52.77% Time: 58.72105431556702 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   6.2,  Val Acc: 48.67%, Val F1: 30.87% Time: 58.72105431556702 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 34.38%,Val Loss:   6.2,  Val Acc: 28.15%, Val F1:  7.41% Time: 58.72105431556702 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.3,  Val Acc: 63.43%, Val F1: 53.60% Time: 150.2717056274414 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   6.3,  Val Acc: 48.15%, Val F1: 31.26% Time: 150.2717056274414 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.3,  Val Acc: 28.41%, Val F1:  7.93% Time: 150.2717056274414 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 63.26%, Val F1: 53.16% Time: 242.12833666801453 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.2,  Val Acc: 49.79%, Val F1: 33.18% Time: 242.12833666801453 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   6.2,  Val Acc: 29.27%, Val F1:  7.97% Time: 242.12833666801453 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   6.5,  Val Acc: 62.32%, Val F1: 51.52% Time: 333.7962052822113 
top-down:SEC: Iter:   3900,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   6.5,  Val Acc: 48.84%, Val F1: 31.23% Time: 333.7962052822113 
top-down:CONN: Iter:   3900,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   6.5,  Val Acc: 26.70%, Val F1:  7.35% Time: 333.7962052822113 
 
 
Train time usage: 361.2857584953308
Test time usage: 2.3965208530426025
TOP: Test Loss:   5.9,  Test Acc: 61.79%, Test F1: 51.60%
SEC: Test Loss:   5.9,  Test Acc: 49.09%, Test F1: 32.38%
CONN: Test Loss:   5.9,  Test Acc: 24.54%, Test F1:  8.47%
consistency_top_sec: 46.97%,  consistency_sec_conn: 19.63%, consistency_top_sec_conn: 19.06%
              precision    recall  f1-score   support

    Temporal     0.7143    0.2206    0.3371        68
 Contingency     0.5363    0.4854    0.5096       274
  Comparison     0.5145    0.4931    0.5035       144
   Expansion     0.6693    0.7649    0.7139       553

    accuracy                         0.6179      1039
   macro avg     0.6086    0.4910    0.5160      1039
weighted avg     0.6157    0.6179    0.6062      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7000    0.2593    0.3784        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5333    0.5393    0.5363       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4859    0.5391    0.5111       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4333    0.5200    0.4727       200
    Expansion.Instantiation     0.6893    0.5966    0.6396       119
      Expansion.Restatement     0.4089    0.4764    0.4401       212
      Expansion.Alternative     0.4667    0.7778    0.5833         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4909      1039
                  macro avg     0.3380    0.3371    0.3238      1039
               weighted avg     0.4831    0.4909    0.4796      1039

Epoch [11/30]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 93.75%,Val Loss:   6.6,  Val Acc: 62.75%, Val F1: 52.79% Time: 65.00498580932617 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 81.25%,Val Loss:   6.6,  Val Acc: 47.30%, Val F1: 31.13% Time: 65.00498580932617 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 37.50%,Val Loss:   6.6,  Val Acc: 27.12%, Val F1:  8.17% Time: 65.00498580932617 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 61.72%, Val F1: 51.22% Time: 156.59901452064514 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   6.7,  Val Acc: 47.98%, Val F1: 31.71% Time: 156.59901452064514 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 43.75%,Val Loss:   6.7,  Val Acc: 29.01%, Val F1:  8.68% Time: 156.59901452064514 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 62.15%, Val F1: 51.56% Time: 248.3398494720459 
top-down:SEC: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   6.6,  Val Acc: 48.07%, Val F1: 31.29% Time: 248.3398494720459 
top-down:CONN: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   6.6,  Val Acc: 27.38%, Val F1:  8.20% Time: 248.3398494720459 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.5,  Val Acc: 61.80%, Val F1: 51.36% Time: 340.0323567390442 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.5,  Val Acc: 49.27%, Val F1: 32.56% Time: 340.0323567390442 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 25.00%,Val Loss:   6.5,  Val Acc: 28.15%, Val F1:  7.45% Time: 340.0323567390442 
 
 
Train time usage: 361.4463267326355
Test time usage: 2.401073932647705
TOP: Test Loss:   6.2,  Test Acc: 62.95%, Test F1: 52.76%
SEC: Test Loss:   6.2,  Test Acc: 48.12%, Test F1: 31.41%
CONN: Test Loss:   6.2,  Test Acc: 24.25%, Test F1:  8.10%
consistency_top_sec: 46.78%,  consistency_sec_conn: 20.12%, consistency_top_sec_conn: 19.83%
              precision    recall  f1-score   support

    Temporal     0.6667    0.2353    0.3478        68
 Contingency     0.5579    0.4745    0.5128       274
  Comparison     0.5170    0.5278    0.5223       144
   Expansion     0.6803    0.7812    0.7273       553

    accuracy                         0.6295      1039
   macro avg     0.6055    0.5047    0.5276      1039
weighted avg     0.6245    0.6295    0.6175      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6316    0.2222    0.3288        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5306    0.4869    0.5078       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4765    0.5547    0.5126       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4440    0.5150    0.4769       200
    Expansion.Instantiation     0.6111    0.6471    0.6286       119
      Expansion.Restatement     0.4132    0.4717    0.4405       212
      Expansion.Alternative     0.4375    0.7778    0.5600         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4812      1039
                  macro avg     0.3222    0.3341    0.3141      1039
               weighted avg     0.4714    0.4812    0.4693      1039

Epoch [12/30]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 61.20%, Val F1: 51.19% Time: 71.26395583152771 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 47.98%, Val F1: 31.31% Time: 71.26395583152771 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   6.6,  Val Acc: 27.81%, Val F1:  8.13% Time: 71.26395583152771 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.8,  Val Acc: 59.91%, Val F1: 51.02% Time: 162.72154116630554 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   6.8,  Val Acc: 48.07%, Val F1: 31.11% Time: 162.72154116630554 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   6.8,  Val Acc: 28.33%, Val F1:  7.77% Time: 162.72154116630554 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   6.8,  Val Acc: 62.23%, Val F1: 51.92% Time: 254.3461446762085 
top-down:SEC: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 48.93%, Val F1: 30.75% Time: 254.3461446762085 
top-down:CONN: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   6.8,  Val Acc: 27.98%, Val F1:  7.93% Time: 254.3461446762085 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.6,  Val Acc: 62.83%, Val F1: 52.60% Time: 346.01295042037964 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 49.70%, Val F1: 32.75% Time: 346.01295042037964 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   6.6,  Val Acc: 29.18%, Val F1:  8.23% Time: 346.01295042037964 
 
 
Train time usage: 361.2096929550171
Test time usage: 2.3926329612731934
TOP: Test Loss:   6.2,  Test Acc: 62.95%, Test F1: 53.37%
SEC: Test Loss:   6.2,  Test Acc: 49.47%, Test F1: 31.94%
CONN: Test Loss:   6.2,  Test Acc: 25.41%, Test F1:  8.63%
consistency_top_sec: 47.55%,  consistency_sec_conn: 20.98%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.5938    0.2754    0.3762        69
 Contingency     0.5556    0.5109    0.5323       274
  Comparison     0.5267    0.4792    0.5018       144
   Expansion     0.6827    0.7717    0.7245       552

    accuracy                         0.6295      1039
   macro avg     0.5897    0.5093    0.5337      1039
weighted avg     0.6216    0.6295    0.6198      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.3273    0.4235        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5356    0.5356    0.5356       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4925    0.5156    0.5038       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4725    0.5150    0.4928       200
    Expansion.Instantiation     0.6827    0.6017    0.6396       118
      Expansion.Restatement     0.3964    0.5142    0.4476       212
      Expansion.Alternative     0.5000    0.4444    0.4706         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4947      1039
                  macro avg     0.3345    0.3140    0.3194      1039
               weighted avg     0.4838    0.4947    0.4850      1039

Epoch [13/30]
top-down:TOP: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 63.18%, Val F1: 54.05% Time: 77.30502772331238 
top-down:SEC: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 49.53%, Val F1: 32.41% Time: 77.30502772331238 
top-down:CONN: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 59.38%,Val Loss:   6.7,  Val Acc: 27.73%, Val F1:  7.70% Time: 77.30502772331238 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   6.8,  Val Acc: 63.95%, Val F1: 52.15% Time: 168.89788150787354 
top-down:SEC: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.8,  Val Acc: 49.44%, Val F1: 31.82% Time: 168.89788150787354 
top-down:CONN: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 37.50%,Val Loss:   6.8,  Val Acc: 27.90%, Val F1:  7.68% Time: 168.89788150787354 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 61.80%, Val F1: 51.69% Time: 260.35504269599915 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.9,  Val Acc: 49.79%, Val F1: 33.51% Time: 260.35504269599915 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.9,  Val Acc: 27.47%, Val F1:  7.68% Time: 260.35504269599915 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 62.75%, Val F1: 52.72% Time: 345.37887167930603 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   6.8,  Val Acc: 50.30%, Val F1: 32.62% Time: 345.37887167930603 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 43.75%,Val Loss:   6.8,  Val Acc: 28.58%, Val F1:  8.40% Time: 345.37887167930603 
 
 
Train time usage: 353.39737367630005
Test time usage: 1.7512462139129639
TOP: Test Loss:   6.5,  Test Acc: 62.08%, Test F1: 51.28%
SEC: Test Loss:   6.5,  Test Acc: 49.09%, Test F1: 32.08%
CONN: Test Loss:   6.5,  Test Acc: 25.12%, Test F1:  9.14%
consistency_top_sec: 46.97%,  consistency_sec_conn: 20.50%, consistency_top_sec_conn: 19.73%
              precision    recall  f1-score   support

    Temporal     0.5484    0.2500    0.3434        68
 Contingency     0.5631    0.4249    0.4843       273
  Comparison     0.5109    0.4861    0.4982       144
   Expansion     0.6647    0.7978    0.7252       554

    accuracy                         0.6208      1039
   macro avg     0.5718    0.4897    0.5128      1039
weighted avg     0.6091    0.6208    0.6055      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6923    0.3333    0.4500        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5591    0.4624    0.5062       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5000    0.5312    0.5152       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4458    0.5550    0.4944       200
    Expansion.Instantiation     0.6429    0.6050    0.6234       119
      Expansion.Restatement     0.4066    0.5211    0.4568       213
      Expansion.Alternative     0.3500    0.7778    0.4828         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4909      1039
                  macro avg     0.3270    0.3442    0.3208      1039
               weighted avg     0.4865    0.4909    0.4808      1039

Epoch [14/30]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 62.06%, Val F1: 51.34% Time: 72.11112403869629 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 47.90%, Val F1: 32.31% Time: 72.11112403869629 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   7.1,  Val Acc: 27.73%, Val F1:  8.97% Time: 72.11112403869629 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.0,  Val Acc: 62.66%, Val F1: 52.41% Time: 150.87046241760254 
top-down:SEC: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 47.98%, Val F1: 32.12% Time: 150.87046241760254 
top-down:CONN: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.0,  Val Acc: 28.41%, Val F1:  8.28% Time: 150.87046241760254 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   7.1,  Val Acc: 61.55%, Val F1: 52.86% Time: 229.97175669670105 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 87.50%,Val Loss:   7.1,  Val Acc: 48.07%, Val F1: 31.64% Time: 229.97175669670105 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 27.98%, Val F1:  8.74% Time: 229.97175669670105 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   7.1,  Val Acc: 63.69%, Val F1: 52.34% Time: 308.92579197883606 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 48.93%, Val F1: 31.83% Time: 308.92579197883606 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 46.88%,Val Loss:   7.1,  Val Acc: 27.98%, Val F1:  8.84% Time: 308.92579197883606 
 
 
Train time usage: 311.5469927787781
Test time usage: 1.7429234981536865
TOP: Test Loss:   6.8,  Test Acc: 63.52%, Test F1: 51.62%
SEC: Test Loss:   6.8,  Test Acc: 48.51%, Test F1: 32.12%
CONN: Test Loss:   6.8,  Test Acc: 25.22%, Test F1:  9.22%
consistency_top_sec: 46.87%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 20.21%
              precision    recall  f1-score   support

    Temporal     0.5429    0.2754    0.3654        69
 Contingency     0.6301    0.3382    0.4402       272
  Comparison     0.6214    0.4444    0.5182       144
   Expansion     0.6424    0.8755    0.7410       554

    accuracy                         0.6352      1039
   macro avg     0.6092    0.4834    0.5162      1039
weighted avg     0.6297    0.6352    0.6064      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5882    0.3636    0.4494        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6087    0.3684    0.4590       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5676    0.4922    0.5272       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4049    0.6600    0.5019       200
    Expansion.Instantiation     0.6606    0.6102    0.6344       118
      Expansion.Restatement     0.4079    0.5305    0.4612       213
      Expansion.Alternative     0.4000    0.6667    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4851      1039
                  macro avg     0.3307    0.3356    0.3212      1039
               weighted avg     0.4970    0.4851    0.4738      1039

Epoch [15/30]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 61.29%, Val F1: 50.99% Time: 77.20665526390076 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   7.1,  Val Acc: 50.90%, Val F1: 33.76% Time: 77.20665526390076 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 27.12%, Val F1:  7.61% Time: 77.20665526390076 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.2,  Val Acc: 63.18%, Val F1: 53.18% Time: 156.2054681777954 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 48.84%, Val F1: 31.84% Time: 156.2054681777954 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   7.2,  Val Acc: 27.04%, Val F1:  7.97% Time: 156.2054681777954 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 62.06%, Val F1: 51.32% Time: 235.09798169136047 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.2,  Val Acc: 49.10%, Val F1: 32.75% Time: 235.09798169136047 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 56.25%,Val Loss:   7.2,  Val Acc: 28.24%, Val F1:  8.67% Time: 235.09798169136047 
 
 
Train time usage: 309.24651551246643
Test time usage: 1.7487618923187256
TOP: Test Loss:   6.8,  Test Acc: 62.85%, Test F1: 52.30%
SEC: Test Loss:   6.8,  Test Acc: 49.09%, Test F1: 33.21%
CONN: Test Loss:   6.8,  Test Acc: 25.22%, Test F1:  9.36%
consistency_top_sec: 47.45%,  consistency_sec_conn: 20.79%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.5484    0.2500    0.3434        68
 Contingency     0.5485    0.5365    0.5424       274
  Comparison     0.5487    0.4306    0.4825       144
   Expansion     0.6810    0.7722    0.7237       553

    accuracy                         0.6285      1039
   macro avg     0.5816    0.4973    0.5230      1039
weighted avg     0.6191    0.6285    0.6176      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.2778    0.3797        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5286    0.5543    0.5411       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5596    0.4766    0.5148       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4486    0.5450    0.4921       200
    Expansion.Instantiation     0.7083    0.5714    0.6326       119
      Expansion.Restatement     0.3864    0.4811    0.4286       212
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.5000    0.0833    0.1429        12

                   accuracy                         0.4909      1039
                  macro avg     0.3782    0.3324    0.3321      1039
               weighted avg     0.4918    0.4909    0.4830      1039

Epoch [16/30]
top-down:TOP: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 60.60%, Val F1: 49.86% Time: 5.73544716835022 
top-down:SEC: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 48.84%, Val F1: 31.67% Time: 5.73544716835022 
top-down:CONN: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   7.3,  Val Acc: 27.30%, Val F1:  7.90% Time: 5.73544716835022 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 61.80%, Val F1: 51.55% Time: 84.65187954902649 
top-down:SEC: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 48.24%, Val F1: 31.32% Time: 84.65187954902649 
top-down:CONN: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   7.3,  Val Acc: 27.64%, Val F1:  8.55% Time: 84.65187954902649 
 
 
top-down:TOP: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 62.15%, Val F1: 50.84% Time: 163.37020134925842 
top-down:SEC: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 87.50%,Val Loss:   7.3,  Val Acc: 48.58%, Val F1: 31.47% Time: 163.37020134925842 
top-down:CONN: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.3,  Val Acc: 26.44%, Val F1:  7.71% Time: 163.37020134925842 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 61.46%, Val F1: 51.75% Time: 241.98683261871338 
top-down:SEC: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 48.33%, Val F1: 31.55% Time: 241.98683261871338 
top-down:CONN: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   7.2,  Val Acc: 26.95%, Val F1:  8.34% Time: 241.98683261871338 
 
 
Train time usage: 310.78459334373474
Test time usage: 1.7240703105926514
TOP: Test Loss:   6.9,  Test Acc: 62.75%, Test F1: 52.82%
SEC: Test Loss:   6.9,  Test Acc: 48.89%, Test F1: 31.37%
CONN: Test Loss:   6.9,  Test Acc: 24.35%, Test F1:  9.06%
consistency_top_sec: 47.45%,  consistency_sec_conn: 19.92%, consistency_top_sec_conn: 19.54%
              precision    recall  f1-score   support

    Temporal     0.5135    0.2794    0.3619        68
 Contingency     0.5658    0.4725    0.5150       273
  Comparison     0.5299    0.4931    0.5108       144
   Expansion     0.6766    0.7816    0.7253       554

    accuracy                         0.6275      1039
   macro avg     0.5714    0.5066    0.5282      1039
weighted avg     0.6165    0.6275    0.6165      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5758    0.3519    0.4368        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5473    0.4981    0.5216       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5037    0.5312    0.5171       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4523    0.5450    0.4943       200
    Expansion.Instantiation     0.6733    0.5714    0.6182       119
      Expansion.Restatement     0.4030    0.5000    0.4463       212
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4889      1039
                  macro avg     0.3172    0.3230    0.3137      1039
               weighted avg     0.4819    0.4889    0.4811      1039

Epoch [17/30]
top-down:TOP: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   7.2,  Val Acc: 62.32%, Val F1: 51.45% Time: 11.095900297164917 
top-down:SEC: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 49.44%, Val F1: 32.56% Time: 11.095900297164917 
top-down:CONN: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   7.2,  Val Acc: 27.47%, Val F1:  7.88% Time: 11.095900297164917 
 
 
top-down:TOP: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 61.63%, Val F1: 51.33% Time: 89.87077212333679 
top-down:SEC: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 49.96%, Val F1: 32.33% Time: 89.87077212333679 
top-down:CONN: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 56.25%,Val Loss:   7.3,  Val Acc: 27.38%, Val F1:  8.94% Time: 89.87077212333679 
 
 
top-down:TOP: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 60.34%, Val F1: 50.83% Time: 168.6409478187561 
top-down:SEC: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.4,  Val Acc: 49.01%, Val F1: 32.18% Time: 168.6409478187561 
top-down:CONN: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   7.4,  Val Acc: 27.98%, Val F1:  8.42% Time: 168.6409478187561 
 
 
top-down:TOP: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 62.58%, Val F1: 52.27% Time: 247.46573758125305 
top-down:SEC: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.3,  Val Acc: 49.70%, Val F1: 33.06% Time: 247.46573758125305 
top-down:CONN: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   7.3,  Val Acc: 28.33%, Val F1:  8.31% Time: 247.46573758125305 
 
 
Train time usage: 310.8376317024231
Test time usage: 1.7433371543884277
TOP: Test Loss:   6.9,  Test Acc: 62.56%, Test F1: 53.40%
SEC: Test Loss:   6.9,  Test Acc: 48.60%, Test F1: 32.15%
CONN: Test Loss:   6.9,  Test Acc: 25.22%, Test F1:  9.29%
consistency_top_sec: 47.16%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 20.50%
              precision    recall  f1-score   support

    Temporal     0.6061    0.2899    0.3922        69
 Contingency     0.5321    0.5146    0.5232       274
  Comparison     0.5360    0.4653    0.4981       144
   Expansion     0.6851    0.7645    0.7226       552

    accuracy                         0.6256      1039
   macro avg     0.5898    0.5086    0.5340      1039
weighted avg     0.6188    0.6256    0.6170      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6061    0.3636    0.4545        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5181    0.5336    0.5257       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5082    0.4844    0.4960       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4583    0.5500    0.5000       200
    Expansion.Instantiation     0.6571    0.5897    0.6216       117
      Expansion.Restatement     0.3983    0.4434    0.4196       212
      Expansion.Alternative     0.3889    0.7778    0.5185         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4860      1039
                  macro avg     0.3214    0.3402    0.3215      1039
               weighted avg     0.4752    0.4860    0.4771      1039

Epoch [18/30]
top-down:TOP: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 87.50%,Val Loss:   7.3,  Val Acc: 62.32%, Val F1: 50.77% Time: 16.504286527633667 
top-down:SEC: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.3,  Val Acc: 47.81%, Val F1: 32.07% Time: 16.504286527633667 
top-down:CONN: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 59.38%,Val Loss:   7.3,  Val Acc: 27.55%, Val F1:  8.79% Time: 16.504286527633667 
 
 
top-down:TOP: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.4,  Val Acc: 61.97%, Val F1: 51.17% Time: 95.38330125808716 
top-down:SEC: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 48.76%, Val F1: 32.19% Time: 95.38330125808716 
top-down:CONN: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   7.4,  Val Acc: 27.47%, Val F1:  8.97% Time: 95.38330125808716 
 
 
top-down:TOP: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 100.00%,Val Loss:   7.4,  Val Acc: 62.23%, Val F1: 52.41% Time: 174.1195080280304 
top-down:SEC: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 49.10%, Val F1: 33.31% Time: 174.1195080280304 
top-down:CONN: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 62.50%,Val Loss:   7.4,  Val Acc: 28.41%, Val F1:  8.94% Time: 174.1195080280304 
 
 
top-down:TOP: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 62.49%, Val F1: 50.69% Time: 252.94611287117004 
top-down:SEC: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 49.79%, Val F1: 32.15% Time: 252.94611287117004 
top-down:CONN: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   7.4,  Val Acc: 27.90%, Val F1:  8.75% Time: 252.94611287117004 
 
 
Train time usage: 310.7798457145691
Test time usage: 1.729271650314331
TOP: Test Loss:   7.1,  Test Acc: 61.98%, Test F1: 52.19%
SEC: Test Loss:   7.1,  Test Acc: 48.80%, Test F1: 31.59%
CONN: Test Loss:   7.1,  Test Acc: 23.10%, Test F1:  8.71%
consistency_top_sec: 46.78%,  consistency_sec_conn: 19.35%, consistency_top_sec_conn: 18.58%
              precision    recall  f1-score   support

    Temporal     0.5128    0.2941    0.3738        68
 Contingency     0.5609    0.4708    0.5119       274
  Comparison     0.5000    0.4653    0.4820       144
   Expansion     0.6730    0.7740    0.7199       553

    accuracy                         0.6198      1039
   macro avg     0.5617    0.5010    0.5219      1039
weighted avg     0.6089    0.6198    0.6094      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6061    0.3636    0.4545        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5536    0.4831    0.5160       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4922    0.4922    0.4922       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4606    0.5850    0.5154       200
    Expansion.Instantiation     0.6379    0.6271    0.6325       118
      Expansion.Restatement     0.3976    0.4670    0.4295       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4880      1039
                  macro avg     0.3187    0.3249    0.3159      1039
               weighted avg     0.4803    0.4880    0.4797      1039

Epoch [19/30]
top-down:TOP: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.5,  Val Acc: 62.15%, Val F1: 52.31% Time: 21.7978253364563 
top-down:SEC: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 49.27%, Val F1: 32.56% Time: 21.7978253364563 
top-down:CONN: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   7.5,  Val Acc: 26.70%, Val F1:  8.29% Time: 21.7978253364563 
 
 
top-down:TOP: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 90.62%,Val Loss:   7.5,  Val Acc: 62.32%, Val F1: 51.15% Time: 100.73768639564514 
top-down:SEC: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 81.25%,Val Loss:   7.5,  Val Acc: 48.07%, Val F1: 31.41% Time: 100.73768639564514 
top-down:CONN: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 27.12%, Val F1:  8.84% Time: 100.73768639564514 
 
 
top-down:TOP: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 62.83%, Val F1: 52.43% Time: 179.5549693107605 
top-down:SEC: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 48.15%, Val F1: 30.67% Time: 179.5549693107605 
top-down:CONN: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   7.6,  Val Acc: 28.41%, Val F1:  8.70% Time: 179.5549693107605 
 
 
top-down:TOP: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 62.15%, Val F1: 51.59% Time: 258.4853706359863 
top-down:SEC: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 48.76%, Val F1: 32.65% Time: 258.4853706359863 
top-down:CONN: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   7.6,  Val Acc: 27.81%, Val F1:  8.72% Time: 258.4853706359863 
 
 
Train time usage: 311.16526770591736
Test time usage: 1.768782377243042
TOP: Test Loss:   7.2,  Test Acc: 62.27%, Test F1: 52.76%
SEC: Test Loss:   7.2,  Test Acc: 48.70%, Test F1: 31.58%
CONN: Test Loss:   7.2,  Test Acc: 25.02%, Test F1:  9.37%
consistency_top_sec: 46.87%,  consistency_sec_conn: 21.17%, consistency_top_sec_conn: 20.40%
              precision    recall  f1-score   support

    Temporal     0.6333    0.2794    0.3878        68
 Contingency     0.5320    0.4854    0.5076       274
  Comparison     0.5780    0.4345    0.4961       145
   Expansion     0.6646    0.7826    0.7188       552

    accuracy                         0.6227      1039
   macro avg     0.6020    0.4955    0.5276      1039
weighted avg     0.6155    0.6227    0.6104      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5862    0.3148    0.4096        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5233    0.5056    0.5143       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5357    0.4688    0.5000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4389    0.5750    0.4978       200
    Expansion.Instantiation     0.6729    0.6050    0.6372       119
      Expansion.Restatement     0.4032    0.4811    0.4387       212
      Expansion.Alternative     0.4167    0.5556    0.4762         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4870      1039
                  macro avg     0.3252    0.3187    0.3158      1039
               weighted avg     0.4784    0.4870    0.4775      1039

Epoch [20/30]
top-down:TOP: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 62.06%, Val F1: 51.91% Time: 27.316831588745117 
top-down:SEC: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 48.50%, Val F1: 33.33% Time: 27.316831588745117 
top-down:CONN: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.6,  Val Acc: 26.44%, Val F1:  8.46% Time: 27.316831588745117 
 
 
top-down:TOP: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 63.00%, Val F1: 52.26% Time: 106.22955107688904 
top-down:SEC: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   7.6,  Val Acc: 48.67%, Val F1: 31.98% Time: 106.22955107688904 
top-down:CONN: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   7.6,  Val Acc: 27.55%, Val F1:  9.12% Time: 106.22955107688904 
 
 
top-down:TOP: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 63.00%, Val F1: 51.46% Time: 185.0508451461792 
top-down:SEC: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   7.6,  Val Acc: 48.93%, Val F1: 31.81% Time: 185.0508451461792 
top-down:CONN: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   7.6,  Val Acc: 28.41%, Val F1:  8.85% Time: 185.0508451461792 
 
 
top-down:TOP: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 61.46%, Val F1: 50.48% Time: 263.777281999588 
top-down:SEC: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   7.7,  Val Acc: 48.76%, Val F1: 31.40% Time: 263.777281999588 
top-down:CONN: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   7.7,  Val Acc: 27.04%, Val F1:  8.73% Time: 263.777281999588 
 
 
Train time usage: 310.96224570274353
Test time usage: 1.7681028842926025
TOP: Test Loss:   7.2,  Test Acc: 63.62%, Test F1: 52.74%
SEC: Test Loss:   7.2,  Test Acc: 49.37%, Test F1: 31.26%
CONN: Test Loss:   7.2,  Test Acc: 23.68%, Test F1:  8.64%
consistency_top_sec: 47.45%,  consistency_sec_conn: 19.73%, consistency_top_sec_conn: 19.25%
              precision    recall  f1-score   support

    Temporal     0.6154    0.2353    0.3404        68
 Contingency     0.5990    0.4432    0.5095       273
  Comparison     0.5133    0.5347    0.5238       144
   Expansion     0.6762    0.8069    0.7358       554

    accuracy                         0.6362      1039
   macro avg     0.6010    0.5050    0.5274      1039
weighted avg     0.6294    0.6362    0.6211      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6250    0.2778    0.3846        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5806    0.4701    0.5196       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4857    0.5312    0.5075       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4471    0.5700    0.5011       200
    Expansion.Instantiation     0.6907    0.5678    0.6233       118
      Expansion.Restatement     0.4149    0.5519    0.4737       212
      Expansion.Alternative     0.3158    0.6667    0.4286         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4937      1039
                  macro avg     0.3236    0.3305    0.3126      1039
               weighted avg     0.4940    0.4937    0.4841      1039

Epoch [21/30]
top-down:TOP: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 62.66%, Val F1: 52.31% Time: 32.61687135696411 
top-down:SEC: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 47.90%, Val F1: 31.47% Time: 32.61687135696411 
top-down:CONN: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 27.98%, Val F1:  9.10% Time: 32.61687135696411 
 
 
top-down:TOP: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 61.72%, Val F1: 51.36% Time: 111.56000781059265 
top-down:SEC: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   7.6,  Val Acc: 47.81%, Val F1: 30.15% Time: 111.56000781059265 
top-down:CONN: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   7.6,  Val Acc: 27.47%, Val F1:  8.59% Time: 111.56000781059265 
 
 
top-down:TOP: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 62.06%, Val F1: 51.27% Time: 190.56775379180908 
top-down:SEC: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 49.18%, Val F1: 33.70% Time: 190.56775379180908 
top-down:CONN: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 27.30%, Val F1:  8.67% Time: 190.56775379180908 
 
 
top-down:TOP: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 61.97%, Val F1: 51.08% Time: 269.53499603271484 
top-down:SEC: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   7.6,  Val Acc: 47.81%, Val F1: 32.21% Time: 269.53499603271484 
top-down:CONN: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   7.6,  Val Acc: 27.04%, Val F1:  8.77% Time: 269.53499603271484 
 
 
Train time usage: 311.531662940979
Test time usage: 1.7341341972351074
TOP: Test Loss:   7.3,  Test Acc: 62.56%, Test F1: 52.19%
SEC: Test Loss:   7.3,  Test Acc: 49.37%, Test F1: 30.47%
CONN: Test Loss:   7.3,  Test Acc: 25.02%, Test F1:  9.07%
consistency_top_sec: 47.26%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 20.40%
              precision    recall  f1-score   support

    Temporal     0.6957    0.2353    0.3516        68
 Contingency     0.5546    0.4818    0.5156       274
  Comparison     0.5271    0.4722    0.4982       144
   Expansion     0.6687    0.7848    0.7221       553

    accuracy                         0.6256      1039
   macro avg     0.6115    0.4935    0.5219      1039
weighted avg     0.6208    0.6256    0.6124      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7619    0.2963    0.4267        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5579    0.5056    0.5305       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5169    0.4766    0.4959       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4453    0.5900    0.5075       200
    Expansion.Instantiation     0.6863    0.5882    0.6335       119
      Expansion.Restatement     0.4089    0.5189    0.4574       212
      Expansion.Alternative     0.2727    0.3333    0.3000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4937      1039
                  macro avg     0.3318    0.3008    0.3047      1039
               weighted avg     0.4968    0.4937    0.4858      1039

Epoch [22/30]
top-down:TOP: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.72%, Val F1: 51.54% Time: 38.15731954574585 
top-down:SEC: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 47.12%, Val F1: 30.78% Time: 38.15731954574585 
top-down:CONN: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 62.50%,Val Loss:   7.7,  Val Acc: 27.47%, Val F1:  8.69% Time: 38.15731954574585 
 
 
top-down:TOP: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 61.29%, Val F1: 51.07% Time: 117.15176057815552 
top-down:SEC: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 47.38%, Val F1: 32.54% Time: 117.15176057815552 
top-down:CONN: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   7.8,  Val Acc: 25.49%, Val F1:  8.46% Time: 117.15176057815552 
 
 
top-down:TOP: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 62.49%, Val F1: 51.21% Time: 195.96060848236084 
top-down:SEC: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 48.07%, Val F1: 31.16% Time: 195.96060848236084 
top-down:CONN: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   7.8,  Val Acc: 26.52%, Val F1:  8.55% Time: 195.96060848236084 
 
 
top-down:TOP: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 62.06%, Val F1: 51.55% Time: 274.6779522895813 
top-down:SEC: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   7.8,  Val Acc: 49.18%, Val F1: 31.78% Time: 274.6779522895813 
top-down:CONN: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.8,  Val Acc: 27.55%, Val F1:  9.06% Time: 274.6779522895813 
 
 
Train time usage: 311.16319036483765
Test time usage: 1.7334277629852295
TOP: Test Loss:   7.4,  Test Acc: 61.98%, Test F1: 50.63%
SEC: Test Loss:   7.4,  Test Acc: 47.93%, Test F1: 30.94%
CONN: Test Loss:   7.4,  Test Acc: 23.77%, Test F1:  9.29%
consistency_top_sec: 46.01%,  consistency_sec_conn: 19.54%, consistency_top_sec_conn: 18.96%
              precision    recall  f1-score   support

    Temporal     0.5152    0.2500    0.3366        68
 Contingency     0.5640    0.4359    0.4917       273
  Comparison     0.4961    0.4444    0.4689       144
   Expansion     0.6667    0.8014    0.7279       554

    accuracy                         0.6198      1039
   macro avg     0.5605    0.4829    0.5063      1039
weighted avg     0.6061    0.6198    0.6043      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6667    0.2963    0.4103        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5541    0.4607    0.5031       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4915    0.4531    0.4715       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4346    0.5650    0.4913       200
    Expansion.Instantiation     0.6364    0.5882    0.6114       119
      Expansion.Restatement     0.4065    0.5330    0.4612       212
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4793      1039
                  macro avg     0.3249    0.3138    0.3094      1039
               weighted avg     0.4804    0.4793    0.4713      1039

Epoch [23/30]
top-down:TOP: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 63.09%, Val F1: 52.18% Time: 43.519787549972534 
top-down:SEC: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 48.33%, Val F1: 31.63% Time: 43.519787549972534 
top-down:CONN: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   7.8,  Val Acc: 26.44%, Val F1:  8.67% Time: 43.519787549972534 
 
 
top-down:TOP: Iter:   8800,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 62.49%, Val F1: 51.69% Time: 122.65507984161377 
top-down:SEC: Iter:   8800,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 48.07%, Val F1: 31.46% Time: 122.65507984161377 
top-down:CONN: Iter:   8800,  Train Loss: 2.4e+01,  Train Acc: 65.62%,Val Loss:   7.8,  Val Acc: 27.47%, Val F1:  8.70% Time: 122.65507984161377 
 
 
top-down:TOP: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 61.89%, Val F1: 50.96% Time: 201.74530172348022 
top-down:SEC: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.8,  Val Acc: 47.64%, Val F1: 31.56% Time: 201.74530172348022 
top-down:CONN: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   7.8,  Val Acc: 26.70%, Val F1:  8.35% Time: 201.74530172348022 
 
 
top-down:TOP: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 62.83%, Val F1: 52.74% Time: 280.86389994621277 
top-down:SEC: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 48.15%, Val F1: 32.43% Time: 280.86389994621277 
top-down:CONN: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   7.8,  Val Acc: 26.61%, Val F1:  8.21% Time: 280.86389994621277 
 
 
Train time usage: 312.00086975097656
Test time usage: 1.7249345779418945
TOP: Test Loss:   7.4,  Test Acc: 63.04%, Test F1: 52.70%
SEC: Test Loss:   7.4,  Test Acc: 48.80%, Test F1: 30.93%
CONN: Test Loss:   7.4,  Test Acc: 24.74%, Test F1:  9.16%
consistency_top_sec: 46.87%,  consistency_sec_conn: 20.31%, consistency_top_sec_conn: 19.92%
              precision    recall  f1-score   support

    Temporal     0.5758    0.2794    0.3762        68
 Contingency     0.5813    0.4322    0.4958       273
  Comparison     0.5690    0.4583    0.5077       144
   Expansion     0.6579    0.8159    0.7284       554

    accuracy                         0.6304      1039
   macro avg     0.5960    0.4965    0.5270      1039
weighted avg     0.6201    0.6304    0.6137      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5625    0.3333    0.4186        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5616    0.4607    0.5062       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5273    0.4531    0.4874       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4470    0.5900    0.5086       200
    Expansion.Instantiation     0.6939    0.5714    0.6267       119
      Expansion.Restatement     0.4091    0.5519    0.4699       212
      Expansion.Alternative     0.2941    0.5556    0.3846         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4880      1039
                  macro avg     0.3178    0.3196    0.3093      1039
               weighted avg     0.4901    0.4880    0.4808      1039

Epoch [24/30]
top-down:TOP: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 62.32%, Val F1: 51.20% Time: 48.76137018203735 
top-down:SEC: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 48.33%, Val F1: 31.97% Time: 48.76137018203735 
top-down:CONN: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 25.84%, Val F1:  8.32% Time: 48.76137018203735 
 
 
top-down:TOP: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 62.06%, Val F1: 52.27% Time: 127.8216404914856 
top-down:SEC: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 47.98%, Val F1: 31.95% Time: 127.8216404914856 
top-down:CONN: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 27.21%, Val F1:  8.88% Time: 127.8216404914856 
 
 
top-down:TOP: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.46%, Val F1: 50.41% Time: 206.88716554641724 
top-down:SEC: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 47.21%, Val F1: 31.49% Time: 206.88716554641724 
top-down:CONN: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 26.70%, Val F1:  8.97% Time: 206.88716554641724 
 
 
top-down:TOP: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.49%, Val F1: 52.00% Time: 286.03606510162354 
top-down:SEC: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 47.81%, Val F1: 31.83% Time: 286.03606510162354 
top-down:CONN: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 62.50%,Val Loss:   8.0,  Val Acc: 26.61%, Val F1:  8.47% Time: 286.03606510162354 
 
 
Train time usage: 312.06362771987915
Test time usage: 1.7367937564849854
TOP: Test Loss:   7.5,  Test Acc: 62.37%, Test F1: 52.48%
SEC: Test Loss:   7.5,  Test Acc: 48.70%, Test F1: 31.55%
CONN: Test Loss:   7.5,  Test Acc: 25.31%, Test F1:  9.86%
consistency_top_sec: 46.68%,  consistency_sec_conn: 20.79%, consistency_top_sec_conn: 20.12%
              precision    recall  f1-score   support

    Temporal     0.5405    0.2941    0.3810        68
 Contingency     0.5749    0.4359    0.4958       273
  Comparison     0.5000    0.4931    0.4965       144
   Expansion     0.6708    0.7906    0.7258       554

    accuracy                         0.6237      1039
   macro avg     0.5715    0.5034    0.5248      1039
weighted avg     0.6134    0.6237    0.6110      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5714    0.3636    0.4444        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5628    0.4532    0.5021       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5120    0.5000    0.5059       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4427    0.5800    0.5022       200
    Expansion.Instantiation     0.6699    0.5847    0.6244       118
      Expansion.Restatement     0.4051    0.5236    0.4568       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4870      1039
                  macro avg     0.3201    0.3237    0.3155      1039
               weighted avg     0.4850    0.4870    0.4794      1039

Epoch [25/30]
top-down:TOP: Iter:   9500,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 63.35%, Val F1: 52.58% Time: 54.25485014915466 
top-down:SEC: Iter:   9500,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 47.98%, Val F1: 30.63% Time: 54.25485014915466 
top-down:CONN: Iter:   9500,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   7.9,  Val Acc: 27.64%, Val F1:  9.23% Time: 54.25485014915466 
 
 
top-down:TOP: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 62.15%, Val F1: 51.43% Time: 133.34698295593262 
top-down:SEC: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 47.73%, Val F1: 31.20% Time: 133.34698295593262 
top-down:CONN: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   8.0,  Val Acc: 26.78%, Val F1:  8.79% Time: 133.34698295593262 
 
 
top-down:TOP: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 63.00%, Val F1: 52.69% Time: 212.3671202659607 
top-down:SEC: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 47.55%, Val F1: 30.35% Time: 212.3671202659607 
top-down:CONN: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 53.12%,Val Loss:   8.0,  Val Acc: 27.04%, Val F1:  8.91% Time: 212.3671202659607 
 
 
top-down:TOP: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 62.83%, Val F1: 52.11% Time: 291.24760460853577 
top-down:SEC: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 47.81%, Val F1: 31.09% Time: 291.24760460853577 
top-down:CONN: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 71.88%,Val Loss:   7.9,  Val Acc: 27.38%, Val F1:  8.75% Time: 291.24760460853577 
 
 
Train time usage: 311.43451046943665
Test time usage: 1.7559657096862793
TOP: Test Loss:   7.6,  Test Acc: 62.85%, Test F1: 53.45%
SEC: Test Loss:   7.6,  Test Acc: 48.32%, Test F1: 31.43%
CONN: Test Loss:   7.6,  Test Acc: 24.06%, Test F1:  9.22%
consistency_top_sec: 46.49%,  consistency_sec_conn: 19.63%, consistency_top_sec_conn: 19.25%
              precision    recall  f1-score   support

    Temporal     0.5349    0.3333    0.4107        69
 Contingency     0.6032    0.4176    0.4935       273
  Comparison     0.5303    0.4861    0.5072       144
   Expansion     0.6607    0.8065    0.7264       553

    accuracy                         0.6285      1039
   macro avg     0.5823    0.5109    0.5345      1039
weighted avg     0.6192    0.6285    0.6139      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5556    0.3636    0.4396        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5950    0.4457    0.5096       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5120    0.5000    0.5059       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4245    0.5900    0.4937       200
    Expansion.Instantiation     0.6372    0.6102    0.6234       118
      Expansion.Restatement     0.4039    0.4858    0.4411       212
      Expansion.Alternative     0.3333    0.6667    0.4444         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4832      1039
                  macro avg     0.3147    0.3329    0.3143      1039
               weighted avg     0.4848    0.4832    0.4763      1039

Epoch [26/30]
top-down:TOP: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.40%, Val F1: 52.77% Time: 59.5982301235199 
top-down:SEC: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 47.30%, Val F1: 31.92% Time: 59.5982301235199 
top-down:CONN: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   8.0,  Val Acc: 26.78%, Val F1:  9.01% Time: 59.5982301235199 
 
 
top-down:TOP: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 61.72%, Val F1: 51.69% Time: 138.37669253349304 
top-down:SEC: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 47.81%, Val F1: 31.14% Time: 138.37669253349304 
top-down:CONN: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   8.0,  Val Acc: 26.87%, Val F1:  8.72% Time: 138.37669253349304 
 
 
top-down:TOP: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.23%, Val F1: 52.37% Time: 217.1709108352661 
top-down:SEC: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 48.07%, Val F1: 30.66% Time: 217.1709108352661 
top-down:CONN: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   8.0,  Val Acc: 27.81%, Val F1:  9.14% Time: 217.1709108352661 
 
 
top-down:TOP: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.58%, Val F1: 52.65% Time: 296.14891171455383 
top-down:SEC: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 31.13% Time: 296.14891171455383 
top-down:CONN: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   8.0,  Val Acc: 26.52%, Val F1:  8.58% Time: 296.14891171455383 
 
 
Train time usage: 311.2370038032532
Test time usage: 1.7321958541870117
TOP: Test Loss:   7.5,  Test Acc: 62.18%, Test F1: 52.64%
SEC: Test Loss:   7.5,  Test Acc: 48.70%, Test F1: 31.36%
CONN: Test Loss:   7.5,  Test Acc: 23.77%, Test F1:  9.06%
consistency_top_sec: 46.68%,  consistency_sec_conn: 19.54%, consistency_top_sec_conn: 19.15%
              precision    recall  f1-score   support

    Temporal     0.6552    0.2794    0.3918        68
 Contingency     0.5498    0.4652    0.5040       273
  Comparison     0.5075    0.4722    0.4892       144
   Expansion     0.6698    0.7798    0.7206       554

    accuracy                         0.6218      1039
   macro avg     0.5955    0.4992    0.5264      1039
weighted avg     0.6148    0.6218    0.6101      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6429    0.3333    0.4390        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5407    0.4981    0.5185       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4919    0.4766    0.4841       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4480    0.5600    0.4978       200
    Expansion.Instantiation     0.6699    0.5798    0.6216       119
      Expansion.Restatement     0.4091    0.5094    0.4538       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4870      1039
                  macro avg     0.3236    0.3193    0.3136      1039
               weighted avg     0.4825    0.4870    0.4791      1039

Epoch [27/30]
top-down:TOP: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 62.32%, Val F1: 52.07% Time: 65.19215679168701 
top-down:SEC: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 48.41%, Val F1: 31.22% Time: 65.19215679168701 
top-down:CONN: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   8.0,  Val Acc: 27.21%, Val F1:  9.09% Time: 65.19215679168701 
 
 
top-down:TOP: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 63.35%, Val F1: 53.06% Time: 144.19931435585022 
top-down:SEC: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 30.84% Time: 144.19931435585022 
top-down:CONN: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   8.0,  Val Acc: 26.70%, Val F1:  9.04% Time: 144.19931435585022 
 
 
top-down:TOP: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 63.09%, Val F1: 52.74% Time: 223.05784511566162 
top-down:SEC: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 31.18% Time: 223.05784511566162 
top-down:CONN: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   8.0,  Val Acc: 27.47%, Val F1:  9.17% Time: 223.05784511566162 
 
 
top-down:TOP: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 62.83%, Val F1: 52.70% Time: 301.91915464401245 
top-down:SEC: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 30.93% Time: 301.91915464401245 
top-down:CONN: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 75.00%,Val Loss:   8.0,  Val Acc: 27.12%, Val F1:  9.28% Time: 301.91915464401245 
 
 
Train time usage: 311.45324635505676
Test time usage: 1.761251449584961
TOP: Test Loss:   7.6,  Test Acc: 62.46%, Test F1: 52.94%
SEC: Test Loss:   7.6,  Test Acc: 48.70%, Test F1: 31.19%
CONN: Test Loss:   7.6,  Test Acc: 23.58%, Test F1:  9.09%
consistency_top_sec: 46.58%,  consistency_sec_conn: 19.25%, consistency_top_sec_conn: 18.77%
              precision    recall  f1-score   support

    Temporal     0.5882    0.2941    0.3922        68
 Contingency     0.5602    0.4432    0.4949       273
  Comparison     0.5303    0.4861    0.5072       144
   Expansion     0.6667    0.7906    0.7234       554

    accuracy                         0.6246      1039
   macro avg     0.5863    0.5035    0.5294      1039
weighted avg     0.6147    0.6246    0.6117      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.3333    0.4286        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5701    0.4719    0.5164       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4923    0.5000    0.4961       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4444    0.5800    0.5033       200
    Expansion.Instantiation     0.6635    0.5798    0.6188       119
      Expansion.Restatement     0.4045    0.5094    0.4509       212
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4870      1039
                  macro avg     0.3189    0.3209    0.3119      1039
               weighted avg     0.4853    0.4870    0.4795      1039

Epoch [28/30]
top-down:TOP: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 62.58%, Val F1: 51.53% Time: 70.50697922706604 
top-down:SEC: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 48.07%, Val F1: 30.82% Time: 70.50697922706604 
top-down:CONN: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 26.44%, Val F1:  9.00% Time: 70.50697922706604 
 
 
top-down:TOP: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 63.09%, Val F1: 52.34% Time: 149.35941338539124 
top-down:SEC: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 31.20% Time: 149.35941338539124 
top-down:CONN: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   8.0,  Val Acc: 27.47%, Val F1:  9.29% Time: 149.35941338539124 
 
 
top-down:TOP: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.92%, Val F1: 53.10% Time: 228.10607838630676 
top-down:SEC: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 31.08% Time: 228.10607838630676 
top-down:CONN: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   8.0,  Val Acc: 26.95%, Val F1:  8.95% Time: 228.10607838630676 
 
 
top-down:TOP: Iter:  11000,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 62.83%, Val F1: 52.40% Time: 307.0262961387634 
top-down:SEC: Iter:  11000,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 48.58%, Val F1: 32.08% Time: 307.0262961387634 
top-down:CONN: Iter:  11000,  Train Loss: 2.3e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 27.04%, Val F1:  9.00% Time: 307.0262961387634 
 
 
Train time usage: 311.21048855781555
Test time usage: 1.7535510063171387
TOP: Test Loss:   7.6,  Test Acc: 62.46%, Test F1: 53.04%
SEC: Test Loss:   7.6,  Test Acc: 49.18%, Test F1: 31.48%
CONN: Test Loss:   7.6,  Test Acc: 24.54%, Test F1:  9.87%
consistency_top_sec: 46.68%,  consistency_sec_conn: 20.02%, consistency_top_sec_conn: 19.54%
              precision    recall  f1-score   support

    Temporal     0.5385    0.3088    0.3925        68
 Contingency     0.5805    0.4359    0.4979       273
  Comparison     0.5180    0.5000    0.5088       144
   Expansion     0.6662    0.7888    0.7223       554

    accuracy                         0.6246      1039
   macro avg     0.5758    0.5084    0.5304      1039
weighted avg     0.6148    0.6246    0.6122      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5588    0.3455    0.4270        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5753    0.4719    0.5185       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4884    0.4922    0.4903       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4545    0.5750    0.5077       200
    Expansion.Instantiation     0.6970    0.5847    0.6359       118
      Expansion.Restatement     0.4116    0.5377    0.4663       212
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4918      1039
                  macro avg     0.3199    0.3239    0.3148      1039
               weighted avg     0.4911    0.4918    0.4850      1039

Epoch [29/30]
top-down:TOP: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 63.09%, Val F1: 53.12% Time: 75.58633947372437 
top-down:SEC: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 48.33%, Val F1: 30.95% Time: 75.58633947372437 
top-down:CONN: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 87.50%,Val Loss:   8.0,  Val Acc: 27.21%, Val F1:  9.20% Time: 75.58633947372437 
 
 
top-down:TOP: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.83%, Val F1: 52.59% Time: 154.27903580665588 
top-down:SEC: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 48.15%, Val F1: 31.14% Time: 154.27903580665588 
top-down:CONN: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 27.38%, Val F1:  9.15% Time: 154.27903580665588 
 
 
top-down:TOP: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 63.09%, Val F1: 52.69% Time: 233.51213836669922 
top-down:SEC: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 31.32% Time: 233.51213836669922 
top-down:CONN: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   8.0,  Val Acc: 26.87%, Val F1:  9.14% Time: 233.51213836669922 
 
 
Train time usage: 309.351304769516
Test time usage: 1.7510569095611572
TOP: Test Loss:   7.6,  Test Acc: 63.23%, Test F1: 53.63%
SEC: Test Loss:   7.6,  Test Acc: 48.80%, Test F1: 31.34%
CONN: Test Loss:   7.6,  Test Acc: 23.58%, Test F1:  8.99%
consistency_top_sec: 46.87%,  consistency_sec_conn: 19.44%, consistency_top_sec_conn: 18.77%
              precision    recall  f1-score   support

    Temporal     0.5833    0.3043    0.4000        69
 Contingency     0.5874    0.4432    0.5052       273
  Comparison     0.5259    0.4931    0.5090       144
   Expansion     0.6707    0.8029    0.7309       553

    accuracy                         0.6323      1039
   macro avg     0.5918    0.5109    0.5363      1039
weighted avg     0.6229    0.6323    0.6188      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5758    0.3455    0.4318        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5755    0.4569    0.5094       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5039    0.5078    0.5058       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4531    0.5800    0.5088       200
    Expansion.Instantiation     0.6604    0.5932    0.6250       118
      Expansion.Restatement     0.3971    0.5189    0.4499       212
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4880      1039
                  macro avg     0.3181    0.3234    0.3134      1039
               weighted avg     0.4866    0.4880    0.4804      1039

Epoch [30/30]
top-down:TOP: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 62.75%, Val F1: 52.66% Time: 4.172726631164551 
top-down:SEC: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   8.0,  Val Acc: 47.90%, Val F1: 31.19% Time: 4.172726631164551 
top-down:CONN: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   8.0,  Val Acc: 27.21%, Val F1:  9.35% Time: 4.172726631164551 
 
 
top-down:TOP: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.58%, Val F1: 52.31% Time: 82.99122142791748 
top-down:SEC: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 48.07%, Val F1: 31.12% Time: 82.99122142791748 
top-down:CONN: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   8.0,  Val Acc: 27.30%, Val F1:  9.35% Time: 82.99122142791748 
 
 
top-down:TOP: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.83%, Val F1: 52.62% Time: 161.91295528411865 
top-down:SEC: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 48.15%, Val F1: 31.16% Time: 161.91295528411865 
top-down:CONN: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 27.38%, Val F1:  9.34% Time: 161.91295528411865 
 
 
top-down:TOP: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 62.92%, Val F1: 52.59% Time: 240.90902829170227 
top-down:SEC: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 47.81%, Val F1: 31.07% Time: 240.90902829170227 
top-down:CONN: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   8.0,  Val Acc: 27.38%, Val F1:  9.33% Time: 240.90902829170227 
 
 
Train time usage: 311.26017808914185
Test time usage: 1.7315824031829834
TOP: Test Loss:   7.7,  Test Acc: 62.95%, Test F1: 53.24%
SEC: Test Loss:   7.7,  Test Acc: 48.80%, Test F1: 31.17%
CONN: Test Loss:   7.7,  Test Acc: 23.97%, Test F1:  9.41%
consistency_top_sec: 46.68%,  consistency_sec_conn: 19.73%, consistency_top_sec_conn: 19.06%
              precision    recall  f1-score   support

    Temporal     0.6250    0.2941    0.4000        68
 Contingency     0.5787    0.4562    0.5102       274
  Comparison     0.5113    0.4722    0.4910       144
   Expansion     0.6702    0.7975    0.7283       553

    accuracy                         0.6295      1039
   macro avg     0.5963    0.5050    0.5324      1039
weighted avg     0.6211    0.6295    0.6164      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6207    0.3273    0.4286        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5662    0.4644    0.5103       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5039    0.5000    0.5020       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4528    0.5750    0.5066       200
    Expansion.Instantiation     0.6667    0.5932    0.6278       118
      Expansion.Restatement     0.4007    0.5236    0.4540       212
      Expansion.Alternative     0.3125    0.5556    0.4000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4880      1039
                  macro avg     0.3203    0.3217    0.3117      1039
               weighted avg     0.4878    0.4880    0.4806      1039

dev_best_acc_top: 62.83%,  dev_best_f1_top: 53.54%, 
dev_best_acc_sec: 51.76%,  dev_best_f1_sec: 33.77%, 
dev_best_acc_conn: 29.53%,  dev_best_f1_conn:  7.84%
