nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_it_train_luna_test/data/', 'log_file': 'data/pdtb_it_train_luna_test/log/', 'save_file': 'data/pdtb_it_train_luna_test/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'February28-10:48:36', 'log': 'data/pdtb_it_train_luna_test/log/February28-10:48:36.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]62it [00:00, 615.01it/s]231it [00:00, 1242.08it/s]397it [00:00, 1428.77it/s]561it [00:00, 1510.17it/s]758it [00:00, 1674.43it/s]941it [00:00, 1726.23it/s]1114it [00:00, 1700.28it/s]1311it [00:00, 1783.25it/s]1490it [00:00, 1733.91it/s]1664it [00:01, 1638.94it/s]1830it [00:01, 1622.00it/s]2012it [00:01, 1678.87it/s]2181it [00:01, 1624.02it/s]2345it [00:01, 1600.49it/s]2506it [00:01, 1559.03it/s]2663it [00:01, 1541.39it/s]2829it [00:01, 1574.40it/s]2987it [00:01, 1528.86it/s]3166it [00:01, 1602.55it/s]3327it [00:02, 1586.36it/s]3487it [00:02, 1571.63it/s]3645it [00:02, 1568.22it/s]3803it [00:02, 1543.43it/s]3958it [00:02, 1454.09it/s]4105it [00:02, 1429.19it/s]4249it [00:02, 1426.48it/s]4393it [00:02, 1385.65it/s]4562it [00:02, 1470.30it/s]4718it [00:03, 1493.53it/s]4868it [00:03, 1482.39it/s]5028it [00:03, 1516.14it/s]5185it [00:03, 1026.54it/s]5316it [00:03, 1088.74it/s]5487it [00:03, 1236.15it/s]5657it [00:03, 1352.53it/s]5838it [00:03, 1474.01it/s]6010it [00:04, 1538.90it/s]6172it [00:04, 1489.49it/s]6327it [00:04, 1463.65it/s]6490it [00:04, 1507.99it/s]6647it [00:04, 1523.14it/s]6802it [00:04, 1521.56it/s]6957it [00:04, 1527.28it/s]7111it [00:04, 1521.66it/s]7264it [00:04, 1517.97it/s]7429it [00:04, 1554.13it/s]7593it [00:05, 1579.31it/s]7758it [00:05, 1598.80it/s]7934it [00:05, 1642.80it/s]8099it [00:05, 1554.22it/s]8266it [00:05, 1585.94it/s]8426it [00:05, 1509.67it/s]8592it [00:05, 1550.20it/s]8763it [00:05, 1594.39it/s]8924it [00:05, 1598.82it/s]9093it [00:05, 1623.94it/s]9256it [00:06, 1603.40it/s]9417it [00:06, 1602.19it/s]9589it [00:06, 1636.58it/s]9753it [00:06, 1605.35it/s]9914it [00:06, 1564.28it/s]10072it [00:06, 1566.00it/s]10229it [00:06, 1490.74it/s]10398it [00:06, 1545.81it/s]10559it [00:06, 1564.16it/s]10729it [00:07, 1603.59it/s]10909it [00:07, 1661.25it/s]11077it [00:07, 1664.82it/s]11244it [00:07, 1663.11it/s]11411it [00:07, 1660.03it/s]11578it [00:07, 1584.83it/s]11750it [00:07, 1621.97it/s]11914it [00:07, 1626.26it/s]12078it [00:07, 1494.83it/s]12239it [00:07, 1524.59it/s]12394it [00:08, 1485.36it/s]12544it [00:08, 1467.99it/s]12547it [00:08, 1529.78it/s]
0it [00:00, ?it/s]169it [00:00, 1675.24it/s]351it [00:00, 1754.04it/s]535it [00:00, 1787.77it/s]714it [00:00, 1737.75it/s]888it [00:00, 1726.74it/s]1063it [00:00, 1731.21it/s]1165it [00:00, 1732.32it/s]
0it [00:00, ?it/s]289it [00:00, 2889.40it/s]292it [00:00, 2887.53it/s]
Time usage: 18.313039779663086
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.8,  Val Acc: 55.88%, Val F1: 17.92% Time: 85.41769886016846 *
top-down:SEC: Iter:    100,  Train Loss: 3e+01,  Train Acc: 18.75%,Val Loss:   6.8,  Val Acc: 24.21%, Val F1:  3.54% Time: 85.41769886016846 *
top-down:CONN: Iter:    100,  Train Loss: 3e+01,  Train Acc:  3.12%,Val Loss:   6.8,  Val Acc: 12.88%, Val F1:  0.34% Time: 85.41769886016846 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   6.4,  Val Acc: 55.28%, Val F1: 19.92% Time: 163.96053791046143 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.4,  Val Acc: 28.84%, Val F1:  7.25% Time: 163.96053791046143 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.4,  Val Acc: 14.25%, Val F1:  0.70% Time: 163.96053791046143 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 78.12%,Val Loss:   6.3,  Val Acc: 55.79%, Val F1: 17.91% Time: 242.89315128326416 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 34.38%,Val Loss:   6.3,  Val Acc: 33.65%, Val F1: 10.78% Time: 242.89315128326416 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 15.62%,Val Loss:   6.3,  Val Acc: 16.05%, Val F1:  1.00% Time: 242.89315128326416 *
 
 
Train time usage: 313.7542324066162
Test time usage: 0.5701577663421631
TOP: Test Loss:   7.0,  Test Acc: 37.67%, Test F1: 18.59%
SEC: Test Loss:   7.0,  Test Acc: 27.40%, Test F1:  9.70%
CONN: Test Loss:   7.0,  Test Acc:  5.48%, Test F1:  1.73%
consistency_top_sec:  3.18%,  consistency_sec_conn:  0.38%, consistency_top_sec_conn:  0.38%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        53
 Contingency     0.2826    0.1529    0.1985        85
  Comparison     0.0000    0.0000    0.0000        43
   Expansion     0.3959    0.8739    0.5449       111

    accuracy                         0.3767       292
   macro avg     0.1696    0.2567    0.1859       292
weighted avg     0.2328    0.3767    0.2649       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.3191    0.7059    0.4396        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.1176    0.0339    0.0526        59
      Expansion.Conjunction     0.2195    0.4000    0.2835        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.2740       292
                  macro avg     0.0820    0.1425    0.0970       292
               weighted avg     0.1505    0.2740    0.1823       292

Epoch [2/15]
top-down:TOP: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   6.2,  Val Acc: 56.74%, Val F1: 38.73% Time: 8.709672927856445 *
top-down:SEC: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   6.2,  Val Acc: 36.74%, Val F1: 15.86% Time: 8.709672927856445 *
top-down:CONN: Iter:    400,  Train Loss: 2.9e+01,  Train Acc:  6.25%,Val Loss:   6.2,  Val Acc: 16.57%, Val F1:  1.97% Time: 8.709672927856445 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   5.8,  Val Acc: 56.31%, Val F1: 36.61% Time: 87.96730279922485 *
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   5.8,  Val Acc: 42.06%, Val F1: 19.71% Time: 87.96730279922485 *
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   5.8,  Val Acc: 22.49%, Val F1:  3.18% Time: 87.96730279922485 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 56.82%, Val F1: 41.15% Time: 165.97197818756104 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 43.69%, Val F1: 21.73% Time: 165.97197818756104 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 23.00%, Val F1:  3.30% Time: 165.97197818756104 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 59.14%, Val F1: 45.09% Time: 245.25010085105896 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   5.6,  Val Acc: 42.58%, Val F1: 20.48% Time: 245.25010085105896 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 23.52%, Val F1:  3.64% Time: 245.25010085105896 *
 
 
Train time usage: 310.9629466533661
Test time usage: 0.5349578857421875
TOP: Test Loss:   6.1,  Test Acc: 44.18%, Test F1: 34.90%
SEC: Test Loss:   6.1,  Test Acc: 35.96%, Test F1: 17.40%
CONN: Test Loss:   6.1,  Test Acc: 17.47%, Test F1:  3.30%
consistency_top_sec:  6.06%,  consistency_sec_conn:  1.73%, consistency_top_sec_conn:  1.73%
              precision    recall  f1-score   support

    Temporal     0.5556    0.1887    0.2817        53
 Contingency     0.4815    0.1529    0.2321        85
  Comparison     0.4762    0.2326    0.3125        43
   Expansion     0.4248    0.8649    0.5697       111

    accuracy                         0.4418       292
   macro avg     0.4845    0.3598    0.3490       292
weighted avg     0.4726    0.4418    0.3813       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4000    0.1778    0.2462        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4091    0.6353    0.4977        85
Contingency.Pragmatic cause     0.0455    0.0909    0.0606        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3729    0.3729    0.3729        59
      Expansion.Conjunction     0.3448    0.4444    0.3883        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3608    0.3596    0.3602       292
                  macro avg     0.1965    0.2152    0.1957       292
               weighted avg     0.3109    0.3596    0.3203       292

Epoch [3/15]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 58.63%, Val F1: 49.21% Time: 14.033586740493774 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 45.67%, Val F1: 26.68% Time: 14.033586740493774 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 25.00%,Val Loss:   5.6,  Val Acc: 22.75%, Val F1:  4.33% Time: 14.033586740493774 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 59.91%, Val F1: 49.61% Time: 92.28416180610657 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 47.21%, Val F1: 26.77% Time: 92.28416180610657 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 21.88%,Val Loss:   5.5,  Val Acc: 26.61%, Val F1:  5.15% Time: 92.28416180610657 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   5.4,  Val Acc: 63.09%, Val F1: 46.55% Time: 169.42984890937805 *
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   5.4,  Val Acc: 48.58%, Val F1: 27.73% Time: 169.42984890937805 *
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   5.4,  Val Acc: 26.44%, Val F1:  4.96% Time: 169.42984890937805 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   5.4,  Val Acc: 62.75%, Val F1: 48.68% Time: 244.94385027885437 
top-down:SEC: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 46.95%, Val F1: 27.67% Time: 244.94385027885437 
top-down:CONN: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 37.50%,Val Loss:   5.4,  Val Acc: 25.58%, Val F1:  4.79% Time: 244.94385027885437 
 
 
Train time usage: 304.32573676109314
Test time usage: 0.5535519123077393
TOP: Test Loss:   6.4,  Test Acc: 46.23%, Test F1: 39.14%
SEC: Test Loss:   6.4,  Test Acc: 34.93%, Test F1: 19.26%
CONN: Test Loss:   6.4,  Test Acc: 19.18%, Test F1:  3.58%
consistency_top_sec:  7.03%,  consistency_sec_conn:  2.12%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.5238    0.2075    0.2973        53
 Contingency     0.5000    0.1765    0.2609        85
  Comparison     0.4186    0.4186    0.4186        43
   Expansion     0.4596    0.8198    0.5890       111

    accuracy                         0.4623       292
   macro avg     0.4755    0.4056    0.3914       292
weighted avg     0.4770    0.4623    0.4154       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4074    0.2444    0.3056        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5714    0.4706    0.5161        85
Contingency.Pragmatic cause     0.0645    0.3636    0.1096        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3429    0.4068    0.3721        59
      Expansion.Conjunction     0.3710    0.5111    0.4299        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3505    0.3493    0.3499       292
                  macro avg     0.2196    0.2496    0.2167       292
               weighted avg     0.3580    0.3493    0.3429       292

Epoch [4/15]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.4,  Val Acc: 62.58%, Val F1: 48.06% Time: 17.35896635055542 
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 47.73%, Val F1: 26.51% Time: 17.35896635055542 
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 26.35%, Val F1:  5.98% Time: 17.35896635055542 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 62.15%, Val F1: 48.35% Time: 94.60539221763611 *
top-down:SEC: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 53.12%,Val Loss:   5.5,  Val Acc: 47.64%, Val F1: 28.29% Time: 94.60539221763611 *
top-down:CONN: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 37.50%,Val Loss:   5.5,  Val Acc: 26.18%, Val F1:  5.46% Time: 94.60539221763611 *
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   5.5,  Val Acc: 61.37%, Val F1: 50.82% Time: 171.75865483283997 *
top-down:SEC: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 46.95%, Val F1: 28.96% Time: 171.75865483283997 *
top-down:CONN: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 27.98%, Val F1:  6.25% Time: 171.75865483283997 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 60.43%, Val F1: 48.34% Time: 247.27379488945007 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   5.5,  Val Acc: 46.70%, Val F1: 30.30% Time: 247.27379488945007 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 37.50%,Val Loss:   5.5,  Val Acc: 26.61%, Val F1:  6.14% Time: 247.27379488945007 
 
 
Train time usage: 301.4012567996979
Test time usage: 0.5570809841156006
TOP: Test Loss:   6.9,  Test Acc: 46.92%, Test F1: 38.56%
SEC: Test Loss:   6.9,  Test Acc: 33.22%, Test F1: 17.88%
CONN: Test Loss:   6.9,  Test Acc: 11.30%, Test F1:  2.03%
consistency_top_sec:  7.80%,  consistency_sec_conn:  1.15%, consistency_top_sec_conn:  1.15%
              precision    recall  f1-score   support

    Temporal     0.3846    0.0943    0.1515        53
 Contingency     0.5682    0.2941    0.3876        85
  Comparison     0.3800    0.4419    0.4086        43
   Expansion     0.4757    0.7928    0.5946       111

    accuracy                         0.4692       292
   macro avg     0.4521    0.4058    0.3856       292
weighted avg     0.4720    0.4692    0.4265       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4375    0.1556    0.2295        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4868    0.4353    0.4596        85
Contingency.Pragmatic cause     0.0727    0.3636    0.1212        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.4333    0.4407    0.4370        59
      Expansion.Conjunction     0.2805    0.5111    0.3622        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3333    0.3322    0.3328       292
                  macro avg     0.2139    0.2383    0.2012       292
               weighted avg     0.3427    0.3322    0.3178       292

Epoch [5/15]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   5.5,  Val Acc: 61.37%, Val F1: 51.16% Time: 24.240864038467407 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 47.90%, Val F1: 29.67% Time: 24.240864038467407 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.5,  Val Acc: 27.04%, Val F1:  5.62% Time: 24.240864038467407 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   5.6,  Val Acc: 60.77%, Val F1: 49.81% Time: 99.38138365745544 
top-down:SEC: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.6,  Val Acc: 46.61%, Val F1: 29.56% Time: 99.38138365745544 
top-down:CONN: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 25.58%, Val F1:  5.60% Time: 99.38138365745544 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   5.7,  Val Acc: 59.48%, Val F1: 49.19% Time: 174.45146942138672 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   5.7,  Val Acc: 45.92%, Val F1: 28.76% Time: 174.45146942138672 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   5.7,  Val Acc: 25.92%, Val F1:  5.61% Time: 174.45146942138672 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 60.77%, Val F1: 51.45% Time: 249.98419499397278 
top-down:SEC: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   5.6,  Val Acc: 47.30%, Val F1: 29.70% Time: 249.98419499397278 
top-down:CONN: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 27.38%, Val F1:  6.02% Time: 249.98419499397278 
 
 
Train time usage: 298.88329243659973
Test time usage: 0.5412313938140869
TOP: Test Loss:   7.0,  Test Acc: 45.55%, Test F1: 38.51%
SEC: Test Loss:   7.0,  Test Acc: 32.88%, Test F1: 18.79%
CONN: Test Loss:   7.0,  Test Acc: 15.41%, Test F1:  2.05%
consistency_top_sec:  8.08%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.83%
              precision    recall  f1-score   support

    Temporal     0.5000    0.1509    0.2319        53
 Contingency     0.5385    0.2471    0.3387        85
  Comparison     0.3043    0.4884    0.3750        43
   Expansion     0.4940    0.7477    0.5950       111

    accuracy                         0.4555       292
   macro avg     0.4592    0.4085    0.3851       292
weighted avg     0.4801    0.4555    0.4221       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4231    0.2444    0.3099        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5179    0.3412    0.4113        85
Contingency.Pragmatic cause     0.0435    0.2727    0.0750        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.4219    0.4576    0.4390        59
      Expansion.Conjunction     0.3768    0.5778    0.4561        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3299    0.3288    0.3293       292
                  macro avg     0.2229    0.2367    0.2114       292
               weighted avg     0.3609    0.3288    0.3293       292

Epoch [6/15]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   5.7,  Val Acc: 61.12%, Val F1: 49.41% Time: 27.657408952713013 
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 48.24%, Val F1: 29.58% Time: 27.657408952713013 
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 26.87%, Val F1:  6.11% Time: 27.657408952713013 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 60.17%, Val F1: 50.27% Time: 103.18673086166382 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.8,  Val Acc: 47.38%, Val F1: 29.96% Time: 103.18673086166382 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   5.8,  Val Acc: 26.70%, Val F1:  6.18% Time: 103.18673086166382 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 61.20%, Val F1: 50.79% Time: 178.68476128578186 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 45.84%, Val F1: 28.31% Time: 178.68476128578186 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.8,  Val Acc: 26.95%, Val F1:  6.31% Time: 178.68476128578186 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 62.49%, Val F1: 51.04% Time: 255.67539858818054 *
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 65.62%,Val Loss:   5.8,  Val Acc: 47.38%, Val F1: 29.43% Time: 255.67539858818054 *
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 40.62%,Val Loss:   5.8,  Val Acc: 26.87%, Val F1:  6.22% Time: 255.67539858818054 *
 
 
Train time usage: 299.5025532245636
Test time usage: 0.5753588676452637
TOP: Test Loss:   7.5,  Test Acc: 45.21%, Test F1: 38.65%
SEC: Test Loss:   7.5,  Test Acc: 30.14%, Test F1: 15.63%
CONN: Test Loss:   7.5,  Test Acc: 13.01%, Test F1:  1.54%
consistency_top_sec:  7.70%,  consistency_sec_conn:  1.15%, consistency_top_sec_conn:  1.15%
              precision    recall  f1-score   support

    Temporal     0.4444    0.1509    0.2254        53
 Contingency     0.5116    0.2588    0.3438        85
  Comparison     0.3333    0.4884    0.3962        43
   Expansion     0.4821    0.7297    0.5806       111

    accuracy                         0.4521       292
   macro avg     0.4429    0.4070    0.3865       292
weighted avg     0.4620    0.4521    0.4200       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4167    0.2222    0.2899        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4902    0.2941    0.3676        85
Contingency.Pragmatic cause     0.0588    0.3636    0.1013        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3860    0.3729    0.3793        59
      Expansion.Conjunction     0.3293    0.6000    0.4252        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3034    0.3014    0.3024       292
                  macro avg     0.2101    0.2316    0.1954       292
               weighted avg     0.3379    0.3014    0.2977       292

Epoch [7/15]
top-down:TOP: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   5.9,  Val Acc: 60.60%, Val F1: 50.45% Time: 32.75275468826294 
top-down:SEC: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 44.72%, Val F1: 27.50% Time: 32.75275468826294 
top-down:CONN: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 37.50%,Val Loss:   5.9,  Val Acc: 26.87%, Val F1:  6.44% Time: 32.75275468826294 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.9,  Val Acc: 62.15%, Val F1: 51.26% Time: 107.89099550247192 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 46.18%, Val F1: 29.14% Time: 107.89099550247192 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   5.9,  Val Acc: 25.41%, Val F1:  6.18% Time: 107.89099550247192 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   6.0,  Val Acc: 61.12%, Val F1: 48.65% Time: 183.08077025413513 
top-down:SEC: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 56.25%,Val Loss:   6.0,  Val Acc: 46.44%, Val F1: 28.08% Time: 183.08077025413513 
top-down:CONN: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 34.38%,Val Loss:   6.0,  Val Acc: 27.55%, Val F1:  6.65% Time: 183.08077025413513 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   5.9,  Val Acc: 61.03%, Val F1: 49.42% Time: 259.0031020641327 
top-down:SEC: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   5.9,  Val Acc: 45.92%, Val F1: 28.39% Time: 259.0031020641327 
top-down:CONN: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 34.38%,Val Loss:   5.9,  Val Acc: 26.44%, Val F1:  6.61% Time: 259.0031020641327 
 
 
Train time usage: 298.11367082595825
Test time usage: 0.5458498001098633
TOP: Test Loss:   7.7,  Test Acc: 45.89%, Test F1: 41.87%
SEC: Test Loss:   7.7,  Test Acc: 32.88%, Test F1: 18.58%
CONN: Test Loss:   7.7,  Test Acc: 12.33%, Test F1:  1.37%
consistency_top_sec:  8.57%,  consistency_sec_conn:  1.15%, consistency_top_sec_conn:  1.15%
              precision    recall  f1-score   support

    Temporal     0.4828    0.2642    0.3415        53
 Contingency     0.4902    0.2941    0.3676        85
  Comparison     0.3571    0.4651    0.4040        43
   Expansion     0.4808    0.6757    0.5618       111

    accuracy                         0.4589       292
   macro avg     0.4527    0.4248    0.4187       292
weighted avg     0.4657    0.4589    0.4421       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4545    0.3333    0.3846        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5094    0.3176    0.3913        85
Contingency.Pragmatic cause     0.0345    0.1818    0.0580        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.4032    0.4237    0.4132        59
      Expansion.Conjunction     0.3293    0.6000    0.4252        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3299    0.3288    0.3293       292
                  macro avg     0.2164    0.2321    0.2090       292
               weighted avg     0.3519    0.3288    0.3244       292

Epoch [8/15]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 60.94%, Val F1: 50.04% Time: 38.480655908584595 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   6.1,  Val Acc: 46.52%, Val F1: 28.15% Time: 38.480655908584595 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.1,  Val Acc: 26.87%, Val F1:  6.63% Time: 38.480655908584595 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.2,  Val Acc: 59.48%, Val F1: 48.88% Time: 115.31175017356873 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.2,  Val Acc: 44.81%, Val F1: 27.05% Time: 115.31175017356873 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   6.2,  Val Acc: 25.75%, Val F1:  6.60% Time: 115.31175017356873 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 60.69%, Val F1: 49.52% Time: 191.51392483711243 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.3,  Val Acc: 44.81%, Val F1: 27.41% Time: 191.51392483711243 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   6.3,  Val Acc: 25.49%, Val F1:  6.95% Time: 191.51392483711243 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.3,  Val Acc: 61.89%, Val F1: 50.29% Time: 268.9777283668518 
top-down:SEC: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.3,  Val Acc: 45.84%, Val F1: 28.18% Time: 268.9777283668518 
top-down:CONN: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   6.3,  Val Acc: 27.12%, Val F1:  6.99% Time: 268.9777283668518 
 
 
Train time usage: 303.3104281425476
Test time usage: 0.5334184169769287
TOP: Test Loss:   7.8,  Test Acc: 44.52%, Test F1: 36.98%
SEC: Test Loss:   7.8,  Test Acc: 30.14%, Test F1: 16.72%
CONN: Test Loss:   7.8,  Test Acc: 20.21%, Test F1:  1.98%
consistency_top_sec:  7.99%,  consistency_sec_conn:  2.12%, consistency_top_sec_conn:  2.12%
              precision    recall  f1-score   support

    Temporal     0.4444    0.1509    0.2254        53
 Contingency     0.5862    0.2000    0.2982        85
  Comparison     0.3600    0.4186    0.3871        43
   Expansion     0.4462    0.7838    0.5686       111

    accuracy                         0.4452       292
   macro avg     0.4592    0.3883    0.3698       292
weighted avg     0.4739    0.4452    0.4009       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4091    0.2000    0.2687        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5263    0.2353    0.3252        85
Contingency.Pragmatic cause     0.0566    0.2727    0.0938        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3370    0.5254    0.4106        59
      Expansion.Conjunction     0.3205    0.5556    0.4065        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3024    0.3014    0.3019       292
                  macro avg     0.2062    0.2236    0.1881       292
               weighted avg     0.3359    0.3014    0.2852       292

Epoch [9/15]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.3,  Val Acc: 60.77%, Val F1: 50.68% Time: 43.32850527763367 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 46.61%, Val F1: 29.13% Time: 43.32850527763367 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   6.3,  Val Acc: 26.09%, Val F1:  6.12% Time: 43.32850527763367 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   6.3,  Val Acc: 60.94%, Val F1: 50.92% Time: 119.84884858131409 
top-down:SEC: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   6.3,  Val Acc: 44.64%, Val F1: 27.56% Time: 119.84884858131409 
top-down:CONN: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   6.3,  Val Acc: 26.18%, Val F1:  6.37% Time: 119.84884858131409 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 84.38%,Val Loss:   6.3,  Val Acc: 61.72%, Val F1: 51.57% Time: 196.86974453926086 
top-down:SEC: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   6.3,  Val Acc: 47.47%, Val F1: 28.99% Time: 196.86974453926086 
top-down:CONN: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 50.00%,Val Loss:   6.3,  Val Acc: 26.44%, Val F1:  6.46% Time: 196.86974453926086 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 87.50%,Val Loss:   6.4,  Val Acc: 61.29%, Val F1: 51.07% Time: 273.98487281799316 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 45.49%, Val F1: 27.16% Time: 273.98487281799316 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   6.4,  Val Acc: 26.09%, Val F1:  6.99% Time: 273.98487281799316 
 
 
Train time usage: 302.7435610294342
Test time usage: 0.5678567886352539
TOP: Test Loss:   8.5,  Test Acc: 47.26%, Test F1: 43.51%
SEC: Test Loss:   8.5,  Test Acc: 33.90%, Test F1: 19.80%
CONN: Test Loss:   8.5,  Test Acc: 10.62%, Test F1:  1.13%
consistency_top_sec:  8.85%,  consistency_sec_conn:  0.96%, consistency_top_sec_conn:  0.96%
              precision    recall  f1-score   support

    Temporal     0.5161    0.3019    0.3810        53
 Contingency     0.5000    0.3059    0.3796        85
  Comparison     0.3333    0.4884    0.3962        43
   Expansion     0.5137    0.6757    0.5837       111

    accuracy                         0.4726       292
   macro avg     0.4658    0.4430    0.4351       292
weighted avg     0.4836    0.4726    0.4599       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4667    0.3111    0.3733        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5094    0.3176    0.3913        85
Contingency.Pragmatic cause     0.0588    0.3636    0.1013        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3971    0.4576    0.4252        59
      Expansion.Conjunction     0.4154    0.6000    0.4909        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3402    0.3390    0.3396       292
                  macro avg     0.2309    0.2563    0.2228       292
               weighted avg     0.3667    0.3390    0.3368       292

Epoch [10/15]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.5,  Val Acc: 59.23%, Val F1: 49.52% Time: 48.99660682678223 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 71.88%,Val Loss:   6.5,  Val Acc: 45.49%, Val F1: 27.59% Time: 48.99660682678223 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   6.5,  Val Acc: 25.49%, Val F1:  6.48% Time: 48.99660682678223 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 61.37%, Val F1: 49.74% Time: 126.43642234802246 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   6.6,  Val Acc: 46.27%, Val F1: 27.95% Time: 126.43642234802246 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.6,  Val Acc: 24.72%, Val F1:  6.43% Time: 126.43642234802246 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.6,  Val Acc: 60.86%, Val F1: 49.23% Time: 203.01233196258545 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.6,  Val Acc: 46.09%, Val F1: 28.36% Time: 203.01233196258545 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   6.6,  Val Acc: 27.04%, Val F1:  7.24% Time: 203.01233196258545 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.6,  Val Acc: 62.49%, Val F1: 50.46% Time: 280.0133864879608 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 45.58%, Val F1: 28.21% Time: 280.0133864879608 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   6.6,  Val Acc: 26.78%, Val F1:  6.76% Time: 280.0133864879608 
 
 
Train time usage: 303.8854081630707
Test time usage: 0.5518763065338135
TOP: Test Loss:   8.1,  Test Acc: 45.21%, Test F1: 39.41%
SEC: Test Loss:   8.1,  Test Acc: 32.19%, Test F1: 18.44%
CONN: Test Loss:   8.1,  Test Acc: 17.47%, Test F1:  1.75%
consistency_top_sec:  8.37%,  consistency_sec_conn:  1.92%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.4583    0.2075    0.2857        53
 Contingency     0.5000    0.2706    0.3511        85
  Comparison     0.3158    0.4186    0.3600        43
   Expansion     0.4848    0.7207    0.5797       111

    accuracy                         0.4521       292
   macro avg     0.4397    0.4044    0.3941       292
weighted avg     0.4596    0.4521    0.4275       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4615    0.2667    0.3380        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4894    0.2706    0.3485        85
Contingency.Pragmatic cause     0.0476    0.2727    0.0811        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3571    0.5085    0.4196        59
      Expansion.Conjunction     0.4000    0.5778    0.4727        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3230    0.3219    0.3225       292
                  macro avg     0.2195    0.2370    0.2075       292
               weighted avg     0.3492    0.3219    0.3142       292

Epoch [11/15]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 60.69%, Val F1: 49.57% Time: 54.702624559402466 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   6.7,  Val Acc: 46.27%, Val F1: 28.19% Time: 54.702624559402466 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 40.62%,Val Loss:   6.7,  Val Acc: 26.44%, Val F1:  7.06% Time: 54.702624559402466 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   6.8,  Val Acc: 60.09%, Val F1: 49.26% Time: 131.69726848602295 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.8,  Val Acc: 44.89%, Val F1: 27.61% Time: 131.69726848602295 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   6.8,  Val Acc: 24.98%, Val F1:  6.46% Time: 131.69726848602295 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 60.94%, Val F1: 50.59% Time: 208.92681741714478 
top-down:SEC: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   6.8,  Val Acc: 44.46%, Val F1: 26.66% Time: 208.92681741714478 
top-down:CONN: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   6.8,  Val Acc: 25.41%, Val F1:  6.79% Time: 208.92681741714478 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 60.60%, Val F1: 49.10% Time: 289.5704896450043 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.8,  Val Acc: 44.21%, Val F1: 26.56% Time: 289.5704896450043 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   6.8,  Val Acc: 25.24%, Val F1:  6.66% Time: 289.5704896450043 
 
 
Train time usage: 308.8975877761841
Test time usage: 0.547832727432251
TOP: Test Loss:   8.5,  Test Acc: 47.95%, Test F1: 42.98%
SEC: Test Loss:   8.5,  Test Acc: 33.56%, Test F1: 19.12%
CONN: Test Loss:   8.5,  Test Acc: 14.04%, Test F1:  1.37%
consistency_top_sec:  8.95%,  consistency_sec_conn:  1.35%, consistency_top_sec_conn:  1.35%
              precision    recall  f1-score   support

    Temporal     0.4483    0.2453    0.3171        53
 Contingency     0.5556    0.2941    0.3846        85
  Comparison     0.3750    0.4884    0.4242        43
   Expansion     0.5000    0.7297    0.5934       111

    accuracy                         0.4795       292
   macro avg     0.4697    0.4394    0.4298       292
weighted avg     0.4884    0.4795    0.4576       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4667    0.3111    0.3733        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5094    0.3176    0.3913        85
Contingency.Pragmatic cause     0.0508    0.2727    0.0857        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3735    0.5254    0.4366        59
      Expansion.Conjunction     0.3770    0.5111    0.4340        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3368    0.3356    0.3362       292
                  macro avg     0.2222    0.2423    0.2151       292
               weighted avg     0.3557    0.3356    0.3298       292

Epoch [12/15]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.8,  Val Acc: 60.26%, Val F1: 50.41% Time: 62.49182987213135 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 45.24%, Val F1: 27.29% Time: 62.49182987213135 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   6.8,  Val Acc: 25.75%, Val F1:  6.76% Time: 62.49182987213135 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 60.77%, Val F1: 50.49% Time: 143.4363396167755 
top-down:SEC: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   6.8,  Val Acc: 45.15%, Val F1: 27.33% Time: 143.4363396167755 
top-down:CONN: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   6.8,  Val Acc: 26.61%, Val F1:  6.92% Time: 143.4363396167755 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   6.9,  Val Acc: 61.12%, Val F1: 49.53% Time: 223.45064163208008 
top-down:SEC: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   6.9,  Val Acc: 44.29%, Val F1: 27.25% Time: 223.45064163208008 
top-down:CONN: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   6.9,  Val Acc: 25.75%, Val F1:  6.89% Time: 223.45064163208008 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.9,  Val Acc: 60.17%, Val F1: 50.25% Time: 304.4922456741333 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   6.9,  Val Acc: 45.15%, Val F1: 27.69% Time: 304.4922456741333 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.9,  Val Acc: 24.89%, Val F1:  6.86% Time: 304.4922456741333 
 
 
Train time usage: 318.23344016075134
Test time usage: 0.5406317710876465
TOP: Test Loss:   8.7,  Test Acc: 46.58%, Test F1: 39.84%
SEC: Test Loss:   8.7,  Test Acc: 32.19%, Test F1: 18.06%
CONN: Test Loss:   8.7,  Test Acc: 15.41%, Test F1:  1.57%
consistency_top_sec:  8.57%,  consistency_sec_conn:  1.64%, consistency_top_sec_conn:  1.64%
              precision    recall  f1-score   support

    Temporal     0.4167    0.1887    0.2597        53
 Contingency     0.5000    0.2588    0.3411        85
  Comparison     0.3673    0.4186    0.3913        43
   Expansion     0.4914    0.7748    0.6014       111

    accuracy                         0.4658       292
   macro avg     0.4439    0.4102    0.3984       292
weighted avg     0.4621    0.4658    0.4327       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4643    0.2889    0.3562        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5111    0.2706    0.3538        85
Contingency.Pragmatic cause     0.0182    0.0909    0.0303        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3580    0.4915    0.4143        59
      Expansion.Conjunction     0.3784    0.6222    0.4706        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3230    0.3219    0.3225       292
                  macro avg     0.2162    0.2205    0.2031       292
               weighted avg     0.3517    0.3219    0.3153       292

Epoch [13/15]
top-down:TOP: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.9,  Val Acc: 60.60%, Val F1: 49.03% Time: 68.41755223274231 
top-down:SEC: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.9,  Val Acc: 44.64%, Val F1: 27.27% Time: 68.41755223274231 
top-down:CONN: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   6.9,  Val Acc: 25.41%, Val F1:  7.02% Time: 68.41755223274231 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.0,  Val Acc: 60.34%, Val F1: 49.46% Time: 149.90519762039185 
top-down:SEC: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   7.0,  Val Acc: 44.64%, Val F1: 27.64% Time: 149.90519762039185 
top-down:CONN: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   7.0,  Val Acc: 25.41%, Val F1:  6.87% Time: 149.90519762039185 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 60.09%, Val F1: 49.25% Time: 230.41271042823792 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   7.0,  Val Acc: 45.32%, Val F1: 27.65% Time: 230.41271042823792 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   7.0,  Val Acc: 24.64%, Val F1:  6.57% Time: 230.41271042823792 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.0,  Val Acc: 60.34%, Val F1: 50.01% Time: 310.66414403915405 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   7.0,  Val Acc: 44.64%, Val F1: 27.16% Time: 310.66414403915405 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   7.0,  Val Acc: 25.15%, Val F1:  6.83% Time: 310.66414403915405 
 
 
Train time usage: 319.0522127151489
Test time usage: 0.5435707569122314
TOP: Test Loss:   8.6,  Test Acc: 47.26%, Test F1: 41.92%
SEC: Test Loss:   8.6,  Test Acc: 31.85%, Test F1: 17.99%
CONN: Test Loss:   8.6,  Test Acc: 18.15%, Test F1:  1.92%
consistency_top_sec:  8.76%,  consistency_sec_conn:  2.02%, consistency_top_sec_conn:  2.02%
              precision    recall  f1-score   support

    Temporal     0.5200    0.2453    0.3333        53
 Contingency     0.5000    0.2824    0.3609        85
  Comparison     0.3393    0.4419    0.3838        43
   Expansion     0.5031    0.7387    0.5985       111

    accuracy                         0.4726       292
   macro avg     0.4656    0.4271    0.4192       292
weighted avg     0.4811    0.4726    0.4496       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4815    0.2889    0.3611        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4706    0.2824    0.3529        85
Contingency.Pragmatic cause     0.0172    0.0909    0.0290        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3483    0.5254    0.4189        59
      Expansion.Conjunction     0.4000    0.5333    0.4571        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3196    0.3185    0.3190       292
                  macro avg     0.2147    0.2151    0.2024       292
               weighted avg     0.3439    0.3185    0.3146       292

Epoch [14/15]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   7.0,  Val Acc: 61.03%, Val F1: 50.54% Time: 73.15404319763184 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 45.32%, Val F1: 27.99% Time: 73.15404319763184 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   7.0,  Val Acc: 25.06%, Val F1:  6.58% Time: 73.15404319763184 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 60.26%, Val F1: 49.68% Time: 155.67263054847717 
top-down:SEC: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 44.64%, Val F1: 27.15% Time: 155.67263054847717 
top-down:CONN: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.1,  Val Acc: 25.06%, Val F1:  6.72% Time: 155.67263054847717 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 100.00%,Val Loss:   7.1,  Val Acc: 60.94%, Val F1: 50.37% Time: 236.38293600082397 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 45.32%, Val F1: 27.40% Time: 236.38293600082397 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   7.1,  Val Acc: 25.49%, Val F1:  6.79% Time: 236.38293600082397 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 60.69%, Val F1: 50.31% Time: 317.20327711105347 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 45.15%, Val F1: 27.52% Time: 317.20327711105347 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 37.50%,Val Loss:   7.1,  Val Acc: 25.06%, Val F1:  6.68% Time: 317.20327711105347 
 
 
Train time usage: 320.0208406448364
Test time usage: 0.5912570953369141
TOP: Test Loss:   8.9,  Test Acc: 48.63%, Test F1: 43.83%
SEC: Test Loss:   8.9,  Test Acc: 33.22%, Test F1: 18.73%
CONN: Test Loss:   8.9,  Test Acc: 14.73%, Test F1:  1.35%
consistency_top_sec:  9.05%,  consistency_sec_conn:  1.73%, consistency_top_sec_conn:  1.73%
              precision    recall  f1-score   support

    Temporal     0.5000    0.2642    0.3457        53
 Contingency     0.5306    0.3059    0.3881        85
  Comparison     0.3846    0.4651    0.4211        43
   Expansion     0.5031    0.7387    0.5985       111

    accuracy                         0.4863       292
   macro avg     0.4796    0.4435    0.4383       292
weighted avg     0.4931    0.4863    0.4652       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4815    0.2889    0.3611        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5294    0.3176    0.3971        85
Contingency.Pragmatic cause     0.0364    0.1818    0.0606        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3529    0.5085    0.4167        59
      Expansion.Conjunction     0.3788    0.5556    0.4505        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3333    0.3322    0.3328       292
                  macro avg     0.2224    0.2315    0.2107       292
               weighted avg     0.3594    0.3322    0.3271       292

Epoch [15/15]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 60.77%, Val F1: 50.49% Time: 79.22230744361877 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 45.41%, Val F1: 27.52% Time: 79.22230744361877 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   7.1,  Val Acc: 25.41%, Val F1:  6.64% Time: 79.22230744361877 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.1,  Val Acc: 60.26%, Val F1: 49.69% Time: 160.06309151649475 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 44.38%, Val F1: 26.72% Time: 160.06309151649475 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   7.1,  Val Acc: 24.98%, Val F1:  6.67% Time: 160.06309151649475 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 60.17%, Val F1: 49.89% Time: 240.90982699394226 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 44.98%, Val F1: 27.17% Time: 240.90982699394226 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 24.98%, Val F1:  6.74% Time: 240.90982699394226 
 
 
Train time usage: 317.1303451061249
Test time usage: 0.5640041828155518
TOP: Test Loss:   8.9,  Test Acc: 46.92%, Test F1: 42.35%
SEC: Test Loss:   8.9,  Test Acc: 31.85%, Test F1: 18.18%
CONN: Test Loss:   8.9,  Test Acc: 16.44%, Test F1:  1.57%
consistency_top_sec:  8.57%,  consistency_sec_conn:  2.02%, consistency_top_sec_conn:  2.02%
              precision    recall  f1-score   support

    Temporal     0.4828    0.2642    0.3415        53
 Contingency     0.5106    0.2824    0.3636        85
  Comparison     0.3636    0.4651    0.4082        43
   Expansion     0.4907    0.7117    0.5809       111

    accuracy                         0.4692       292
   macro avg     0.4619    0.4308    0.4235       292
weighted avg     0.4763    0.4692    0.4488       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4828    0.3111    0.3784        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4792    0.2706    0.3459        85
Contingency.Pragmatic cause     0.0345    0.1818    0.0580        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3488    0.5085    0.4138        59
      Expansion.Conjunction     0.3750    0.5333    0.4404        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3196    0.3185    0.3190       292
                  macro avg     0.2150    0.2257    0.2045       292
               weighted avg     0.3435    0.3185    0.3126       292

dev_best_acc_top: 62.49%,  dev_best_f1_top: 51.04%, 
dev_best_acc_sec: 47.38%,  dev_best_f1_sec: 29.43%, 
dev_best_acc_conn: 26.87%,  dev_best_f1_conn:  6.22%
