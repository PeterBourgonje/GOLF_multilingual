nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_tr_train_tdb_test/data/', 'log_file': 'data/pdtb_tr_train_tdb_test/log/', 'save_file': 'data/pdtb_tr_train_tdb_test/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'February28-14:57:15', 'log': 'data/pdtb_tr_train_tdb_test/log/February28-14:57:15.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]10it [00:00, 95.87it/s]156it [00:00, 883.06it/s]330it [00:00, 1260.23it/s]457it [00:00, 1217.62it/s]626it [00:00, 1380.28it/s]810it [00:00, 1532.23it/s]992it [00:00, 1624.69it/s]1186it [00:00, 1722.57it/s]1390it [00:00, 1820.32it/s]1573it [00:01, 1745.21it/s]1749it [00:01, 1733.13it/s]1923it [00:01, 1627.11it/s]2088it [00:01, 1628.11it/s]2252it [00:01, 1567.08it/s]2410it [00:01, 1485.74it/s]2560it [00:01, 1437.13it/s]2705it [00:01, 1435.55it/s]2850it [00:01, 1421.29it/s]2993it [00:02, 1401.99it/s]3134it [00:02, 1370.39it/s]3276it [00:02, 1383.34it/s]3432it [00:02, 1433.65it/s]3581it [00:02, 1449.96it/s]3727it [00:02, 1434.71it/s]3871it [00:02, 1311.96it/s]4007it [00:02, 1324.81it/s]4160it [00:02, 1382.24it/s]4300it [00:02, 1363.96it/s]4464it [00:03, 1441.98it/s]4610it [00:03, 1355.10it/s]4765it [00:03, 1409.06it/s]4908it [00:03, 1394.59it/s]5089it [00:03, 1513.79it/s]5242it [00:03, 1115.49it/s]5384it [00:03, 1186.10it/s]5554it [00:03, 1313.28it/s]5697it [00:04, 1337.01it/s]5884it [00:04, 1479.92it/s]6044it [00:04, 1511.32it/s]6206it [00:04, 1541.55it/s]6371it [00:04, 1572.17it/s]6532it [00:04, 1538.06it/s]6688it [00:04, 1535.42it/s]6852it [00:04, 1563.43it/s]7042it [00:04, 1661.63it/s]7231it [00:04, 1727.22it/s]7416it [00:05, 1761.70it/s]7603it [00:05, 1792.62it/s]7790it [00:05, 1814.24it/s]7972it [00:05, 1672.34it/s]8142it [00:05, 1554.76it/s]8302it [00:05, 1564.37it/s]8461it [00:05, 1524.19it/s]8618it [00:05, 1535.60it/s]8785it [00:05, 1573.01it/s]8944it [00:06, 1508.11it/s]9107it [00:06, 1541.92it/s]9277it [00:06, 1562.39it/s]9460it [00:06, 1636.97it/s]9650it [00:06, 1710.92it/s]9847it [00:06, 1784.64it/s]10027it [00:06, 1747.94it/s]10203it [00:06, 1628.23it/s]10370it [00:06, 1637.49it/s]10538it [00:06, 1645.78it/s]10704it [00:07, 1521.38it/s]10873it [00:07, 1565.73it/s]11043it [00:07, 1602.26it/s]11205it [00:07, 1550.18it/s]11373it [00:07, 1585.84it/s]11561it [00:07, 1668.51it/s]11753it [00:07, 1741.24it/s]11943it [00:07, 1787.65it/s]12123it [00:07, 1720.31it/s]12297it [00:08, 1708.86it/s]12469it [00:08, 1639.37it/s]12547it [00:08, 1527.62it/s]
0it [00:00, ?it/s]171it [00:00, 1707.53it/s]361it [00:00, 1819.71it/s]548it [00:00, 1839.96it/s]732it [00:00, 1660.10it/s]901it [00:00, 1579.63it/s]1061it [00:00, 1582.35it/s]1165it [00:00, 1629.61it/s]
0it [00:00, ?it/s]222it [00:00, 2216.70it/s]268it [00:00, 2129.17it/s]
Time usage: 19.753899812698364
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.8,  Val Acc: 55.88%, Val F1: 17.92% Time: 89.72921133041382 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 18.75%,Val Loss:   6.8,  Val Acc: 24.21%, Val F1:  3.54% Time: 89.72921133041382 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  3.12%,Val Loss:   6.8,  Val Acc: 12.88%, Val F1:  0.34% Time: 89.72921133041382 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.4,  Val Acc: 55.88%, Val F1: 17.93% Time: 171.35236430168152 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.4,  Val Acc: 29.53%, Val F1:  7.32% Time: 171.35236430168152 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.4,  Val Acc: 14.59%, Val F1:  0.67% Time: 171.35236430168152 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 75.00%,Val Loss:   6.3,  Val Acc: 55.79%, Val F1: 17.91% Time: 252.85447072982788 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 34.38%,Val Loss:   6.3,  Val Acc: 32.88%, Val F1: 11.06% Time: 252.85447072982788 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 18.75%,Val Loss:   6.3,  Val Acc: 16.57%, Val F1:  1.15% Time: 252.85447072982788 *
 
 
Train time usage: 326.8243091106415
Test time usage: 0.534804105758667
TOP: Test Loss:   6.6,  Test Acc: 42.54%, Test F1: 29.62%
SEC: Test Loss:   6.6,  Test Acc: 21.64%, Test F1: 14.54%
CONN: Test Loss:   6.6,  Test Acc: 13.06%, Test F1:  3.85%
consistency_top_sec:  3.56%,  consistency_sec_conn:  1.73%, consistency_top_sec_conn:  1.54%
              precision    recall  f1-score   support

    Temporal     0.5333    0.1356    0.2162        59
 Contingency     0.2766    0.3171    0.2955        41
  Comparison     1.0000    0.0566    0.1071        53
   Expansion     0.4433    0.7826    0.5660       115

    accuracy                         0.4254       268
   macro avg     0.5633    0.3230    0.2962       268
weighted avg     0.5477    0.4254    0.3569       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.2542    0.3371        59
         Temporal.Synchrony     0.2407    0.6341    0.3490        41
          Contingency.Cause     1.0000    0.0476    0.0909        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.6154    0.1495    0.2406       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                  micro avg     0.3515    0.2164    0.2679       268
                  macro avg     0.3927    0.1809    0.1696       268
               weighted avg     0.4710    0.2164    0.2308       268

Epoch [2/15]
top-down:TOP: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   6.1,  Val Acc: 54.94%, Val F1: 39.48% Time: 9.123856782913208 *
top-down:SEC: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   6.1,  Val Acc: 36.65%, Val F1: 17.28% Time: 9.123856782913208 *
top-down:CONN: Iter:    400,  Train Loss: 2.9e+01,  Train Acc:  6.25%,Val Loss:   6.1,  Val Acc: 18.63%, Val F1:  2.09% Time: 9.123856782913208 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 55.19%, Val F1: 38.08% Time: 90.92943072319031 *
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 43.75%,Val Loss:   5.9,  Val Acc: 40.77%, Val F1: 18.62% Time: 90.92943072319031 *
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   5.9,  Val Acc: 22.23%, Val F1:  2.97% Time: 90.92943072319031 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 58.03%, Val F1: 41.68% Time: 172.75758028030396 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   5.7,  Val Acc: 42.92%, Val F1: 21.07% Time: 172.75758028030396 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   5.7,  Val Acc: 23.35%, Val F1:  3.05% Time: 172.75758028030396 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 60.94%, Val F1: 45.47% Time: 254.5649447441101 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 41.97%, Val F1: 21.25% Time: 254.5649447441101 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 31.25%,Val Loss:   5.6,  Val Acc: 22.75%, Val F1:  3.00% Time: 254.5649447441101 *
 
 
Train time usage: 322.7909502983093
Test time usage: 0.5260584354400635
TOP: Test Loss:   5.5,  Test Acc: 51.12%, Test F1: 45.04%
SEC: Test Loss:   5.5,  Test Acc: 39.55%, Test F1: 25.88%
CONN: Test Loss:   5.5,  Test Acc: 25.00%, Test F1:  4.44%
consistency_top_sec:  8.85%,  consistency_sec_conn:  3.18%, consistency_top_sec_conn:  3.18%
              precision    recall  f1-score   support

    Temporal     0.4805    0.6271    0.5441        59
 Contingency     0.4000    0.3902    0.3951        41
  Comparison     0.6000    0.1698    0.2647        53
   Expansion     0.5515    0.6522    0.5976       115

    accuracy                         0.5112       268
   macro avg     0.5080    0.4598    0.4504       268
weighted avg     0.5223    0.5112    0.4890       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4730    0.5932    0.5263        59
         Temporal.Synchrony     0.3011    0.6829    0.4179        41
          Contingency.Cause     0.2000    0.1905    0.1951        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.6129    0.3551    0.4497       107
      Comparison.Concession     1.0000    0.1250    0.2222         8

                  micro avg     0.4240    0.3955    0.4093       268
                  macro avg     0.4312    0.3245    0.3019       268
               weighted avg     0.4404    0.3955    0.3813       268

Epoch [3/15]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 59.66%, Val F1: 47.30% Time: 14.47246766090393 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 31.25%,Val Loss:   5.6,  Val Acc: 43.52%, Val F1: 24.61% Time: 14.47246766090393 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 21.88%,Val Loss:   5.6,  Val Acc: 23.18%, Val F1:  4.09% Time: 14.47246766090393 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 60.17%, Val F1: 47.47% Time: 96.36670923233032 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.5,  Val Acc: 44.72%, Val F1: 24.71% Time: 96.36670923233032 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc:  3.12%,Val Loss:   5.5,  Val Acc: 24.72%, Val F1:  4.24% Time: 96.36670923233032 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 62.40%, Val F1: 46.31% Time: 178.16396522521973 *
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 46.35%, Val F1: 24.17% Time: 178.16396522521973 *
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 24.12%, Val F1:  4.15% Time: 178.16396522521973 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 61.37%, Val F1: 46.72% Time: 259.84349274635315 *
top-down:SEC: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   5.5,  Val Acc: 45.49%, Val F1: 25.56% Time: 259.84349274635315 *
top-down:CONN: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   5.5,  Val Acc: 24.21%, Val F1:  4.31% Time: 259.84349274635315 *
 
 
Train time usage: 322.3549921512604
Test time usage: 0.5191376209259033
TOP: Test Loss:   5.8,  Test Acc: 51.12%, Test F1: 46.50%
SEC: Test Loss:   5.8,  Test Acc: 37.31%, Test F1: 25.16%
CONN: Test Loss:   5.8,  Test Acc: 26.49%, Test F1:  3.49%
consistency_top_sec:  8.85%,  consistency_sec_conn:  3.37%, consistency_top_sec_conn:  3.37%
              precision    recall  f1-score   support

    Temporal     0.4571    0.5424    0.4961        59
 Contingency     0.2941    0.2439    0.2667        41
  Comparison     0.5750    0.4340    0.4946        53
   Expansion     0.5806    0.6261    0.6025       115

    accuracy                         0.5112       268
   macro avg     0.4767    0.4616    0.4650       268
weighted avg     0.5085    0.5112    0.5064       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4658    0.5763    0.5152        59
         Temporal.Synchrony     0.2540    0.3902    0.3077        41
          Contingency.Cause     0.1860    0.3810    0.2500        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5942    0.3832    0.4659       107
      Comparison.Concession     1.0000    0.1250    0.2222         8

                  micro avg     0.4016    0.3731    0.3868       268
                  macro avg     0.4167    0.3093    0.2935       268
               weighted avg     0.4231    0.3731    0.3727       268

Epoch [4/15]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 60.43%, Val F1: 46.84% Time: 18.27871584892273 
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 45.92%, Val F1: 23.71% Time: 18.27871584892273 
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 43.75%,Val Loss:   5.5,  Val Acc: 24.03%, Val F1:  5.12% Time: 18.27871584892273 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 61.37%, Val F1: 47.43% Time: 99.80629968643188 *
top-down:SEC: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 50.00%,Val Loss:   5.5,  Val Acc: 45.49%, Val F1: 24.83% Time: 99.80629968643188 *
top-down:CONN: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 24.21%, Val F1:  5.22% Time: 99.80629968643188 *
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 60.17%, Val F1: 49.17% Time: 181.6078760623932 *
top-down:SEC: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 45.32%, Val F1: 27.39% Time: 181.6078760623932 *
top-down:CONN: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 25.41%, Val F1:  5.40% Time: 181.6078760623932 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 60.00%, Val F1: 48.50% Time: 261.29518866539 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 56.25%,Val Loss:   5.6,  Val Acc: 44.55%, Val F1: 26.17% Time: 261.29518866539 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 24.81%, Val F1:  5.76% Time: 261.29518866539 
 
 
Train time usage: 318.6456639766693
Test time usage: 0.5355873107910156
TOP: Test Loss:   6.3,  Test Acc: 49.25%, Test F1: 43.90%
SEC: Test Loss:   6.3,  Test Acc: 30.22%, Test F1: 18.30%
CONN: Test Loss:   6.3,  Test Acc: 30.22%, Test F1:  2.90%
consistency_top_sec:  7.22%,  consistency_sec_conn:  2.79%, consistency_top_sec_conn:  2.69%
              precision    recall  f1-score   support

    Temporal     0.5000    0.2881    0.3656        59
 Contingency     0.2889    0.3171    0.3023        41
  Comparison     0.5946    0.4151    0.4889        53
   Expansion     0.5263    0.6957    0.5993       115

    accuracy                         0.4925       268
   macro avg     0.4774    0.4290    0.4390       268
weighted avg     0.4977    0.4925    0.4806       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4865    0.3051    0.3750        59
         Temporal.Synchrony     0.2576    0.4146    0.3178        41
          Contingency.Cause     0.2381    0.4762    0.3175        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5645    0.3271    0.4142       107
      Comparison.Concession     1.0000    0.1250    0.2222         8

                  micro avg     0.3894    0.3022    0.3403       268
                  macro avg     0.4244    0.2747    0.2744       268
               weighted avg     0.4204    0.3022    0.3280       268

Epoch [5/15]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.6,  Val Acc: 61.89%, Val F1: 50.24% Time: 25.4345645904541 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   5.6,  Val Acc: 44.98%, Val F1: 27.90% Time: 25.4345645904541 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   5.6,  Val Acc: 24.98%, Val F1:  5.97% Time: 25.4345645904541 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   5.7,  Val Acc: 60.34%, Val F1: 49.69% Time: 105.08826327323914 
top-down:SEC: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 44.38%, Val F1: 27.65% Time: 105.08826327323914 
top-down:CONN: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 22.92%, Val F1:  5.18% Time: 105.08826327323914 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.7,  Val Acc: 60.17%, Val F1: 50.11% Time: 184.73347282409668 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.7,  Val Acc: 45.49%, Val F1: 27.70% Time: 184.73347282409668 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 23.69%, Val F1:  5.80% Time: 184.73347282409668 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 59.74%, Val F1: 50.42% Time: 264.81934118270874 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.6,  Val Acc: 45.06%, Val F1: 26.14% Time: 264.81934118270874 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 37.50%,Val Loss:   5.6,  Val Acc: 25.58%, Val F1:  5.63% Time: 264.81934118270874 
 
 
Train time usage: 316.76943707466125
Test time usage: 0.5114986896514893
TOP: Test Loss:   6.4,  Test Acc: 51.87%, Test F1: 48.29%
SEC: Test Loss:   6.4,  Test Acc: 36.19%, Test F1: 22.85%
CONN: Test Loss:   6.4,  Test Acc: 24.25%, Test F1:  2.79%
consistency_top_sec:  8.66%,  consistency_sec_conn:  2.98%, consistency_top_sec_conn:  2.79%
              precision    recall  f1-score   support

    Temporal     0.5172    0.5085    0.5128        59
 Contingency     0.3095    0.3171    0.3133        41
  Comparison     0.5854    0.4528    0.5106        53
   Expansion     0.5669    0.6261    0.5950       115

    accuracy                         0.5187       268
   macro avg     0.4948    0.4761    0.4829       268
weighted avg     0.5203    0.5187    0.5171       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5085    0.5085    0.5085        59
         Temporal.Synchrony     0.2982    0.4146    0.3469        41
          Contingency.Cause     0.2195    0.4286    0.2903        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5970    0.3738    0.4598       107
      Comparison.Concession     1.0000    0.1250    0.2222         8

                  micro avg     0.4311    0.3619    0.3935       268
                  macro avg     0.4372    0.3084    0.3046       268
               weighted avg     0.4430    0.3619    0.3780       268

Epoch [6/15]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 61.89%, Val F1: 50.30% Time: 30.970430374145508 *
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 47.21%, Val F1: 28.34% Time: 30.970430374145508 *
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 26.01%, Val F1:  6.27% Time: 30.970430374145508 *
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   5.7,  Val Acc: 62.06%, Val F1: 50.31% Time: 110.53034591674805 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.7,  Val Acc: 46.78%, Val F1: 27.75% Time: 110.53034591674805 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 25.67%, Val F1:  6.52% Time: 110.53034591674805 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.7,  Val Acc: 62.32%, Val F1: 50.62% Time: 190.46944212913513 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.7,  Val Acc: 46.27%, Val F1: 27.15% Time: 190.46944212913513 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 37.50%,Val Loss:   5.7,  Val Acc: 26.01%, Val F1:  6.47% Time: 190.46944212913513 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 87.50%,Val Loss:   5.8,  Val Acc: 61.89%, Val F1: 50.87% Time: 270.42718625068665 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 68.75%,Val Loss:   5.8,  Val Acc: 45.92%, Val F1: 27.25% Time: 270.42718625068665 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 37.50%,Val Loss:   5.8,  Val Acc: 24.55%, Val F1:  5.75% Time: 270.42718625068665 
 
 
Train time usage: 316.716983795166
Test time usage: 0.5413351058959961
TOP: Test Loss:   6.9,  Test Acc: 48.88%, Test F1: 44.52%
SEC: Test Loss:   6.9,  Test Acc: 33.58%, Test F1: 21.38%
CONN: Test Loss:   6.9,  Test Acc: 21.27%, Test F1:  2.19%
consistency_top_sec:  8.08%,  consistency_sec_conn:  2.02%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.5102    0.4237    0.4630        59
 Contingency     0.2857    0.3415    0.3111        41
  Comparison     0.4872    0.3585    0.4130        53
   Expansion     0.5573    0.6348    0.5935       115

    accuracy                         0.4888       268
   macro avg     0.4601    0.4396    0.4452       268
weighted avg     0.4915    0.4888    0.4859       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5306    0.4407    0.4815        59
         Temporal.Synchrony     0.2931    0.4146    0.3434        41
          Contingency.Cause     0.2093    0.4286    0.2812        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5968    0.3458    0.4379       107
      Comparison.Concession     0.2500    0.1250    0.1667         8

                  micro avg     0.4167    0.3358    0.3719       268
                  macro avg     0.3133    0.2924    0.2851       268
               weighted avg     0.4238    0.3358    0.3604       268

Epoch [7/15]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 61.97%, Val F1: 49.57% Time: 34.62941241264343 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   5.9,  Val Acc: 44.46%, Val F1: 26.89% Time: 34.62941241264343 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   5.9,  Val Acc: 24.21%, Val F1:  6.17% Time: 34.62941241264343 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   5.8,  Val Acc: 61.63%, Val F1: 50.18% Time: 114.44805955886841 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.8,  Val Acc: 46.27%, Val F1: 27.41% Time: 114.44805955886841 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   5.8,  Val Acc: 24.98%, Val F1:  6.21% Time: 114.44805955886841 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 61.97%, Val F1: 50.47% Time: 194.71710896492004 
top-down:SEC: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 62.50%,Val Loss:   5.9,  Val Acc: 45.49%, Val F1: 27.38% Time: 194.71710896492004 
top-down:CONN: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 28.12%,Val Loss:   5.9,  Val Acc: 24.72%, Val F1:  5.96% Time: 194.71710896492004 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   5.9,  Val Acc: 60.26%, Val F1: 48.94% Time: 274.2696692943573 
top-down:SEC: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 65.62%,Val Loss:   5.9,  Val Acc: 45.67%, Val F1: 27.47% Time: 274.2696692943573 
top-down:CONN: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 25.00%,Val Loss:   5.9,  Val Acc: 24.72%, Val F1:  6.57% Time: 274.2696692943573 
 
 
Train time usage: 315.3060746192932
Test time usage: 0.5022735595703125
TOP: Test Loss:   7.6,  Test Acc: 47.01%, Test F1: 43.08%
SEC: Test Loss:   7.6,  Test Acc: 30.97%, Test F1: 20.40%
CONN: Test Loss:   7.6,  Test Acc: 20.52%, Test F1:  2.27%
consistency_top_sec:  7.51%,  consistency_sec_conn:  2.02%, consistency_top_sec_conn:  2.02%
              precision    recall  f1-score   support

    Temporal     0.5000    0.4068    0.4486        59
 Contingency     0.2388    0.3902    0.2963        41
  Comparison     0.5714    0.3019    0.3951        53
   Expansion     0.5600    0.6087    0.5833       115

    accuracy                         0.4701       268
   macro avg     0.4676    0.4269    0.4308       268
weighted avg     0.4999    0.4701    0.4725       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4068    0.4486        59
         Temporal.Synchrony     0.2500    0.4634    0.3248        41
          Contingency.Cause     0.2667    0.3810    0.3137        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5439    0.2897    0.3780       107
      Comparison.Concession     0.2500    0.1250    0.1667         8

                  micro avg     0.3860    0.3097    0.3437       268
                  macro avg     0.3018    0.2776    0.2720       268
               weighted avg     0.3938    0.3097    0.3289       268

Epoch [8/15]
top-down:TOP: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   5.9,  Val Acc: 60.86%, Val F1: 50.06% Time: 39.92616868019104 
top-down:SEC: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 46.01%, Val F1: 27.45% Time: 39.92616868019104 
top-down:CONN: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   5.9,  Val Acc: 25.32%, Val F1:  6.35% Time: 39.92616868019104 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 60.09%, Val F1: 48.83% Time: 119.61028099060059 
top-down:SEC: Iter:   2900,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   6.1,  Val Acc: 43.35%, Val F1: 27.06% Time: 119.61028099060059 
top-down:CONN: Iter:   2900,  Train Loss: 2.7e+01,  Train Acc: 31.25%,Val Loss:   6.1,  Val Acc: 24.55%, Val F1:  6.48% Time: 119.61028099060059 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.1,  Val Acc: 60.43%, Val F1: 50.37% Time: 199.25959825515747 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 44.89%, Val F1: 26.99% Time: 199.25959825515747 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.1,  Val Acc: 25.49%, Val F1:  6.80% Time: 199.25959825515747 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   6.0,  Val Acc: 61.97%, Val F1: 51.56% Time: 280.6974096298218 *
top-down:SEC: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   6.0,  Val Acc: 46.95%, Val F1: 27.72% Time: 280.6974096298218 *
top-down:CONN: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   6.0,  Val Acc: 26.44%, Val F1:  6.73% Time: 280.6974096298218 *
 
 
Train time usage: 315.99293780326843
Test time usage: 0.5338330268859863
TOP: Test Loss:   7.4,  Test Acc: 45.15%, Test F1: 39.40%
SEC: Test Loss:   7.4,  Test Acc: 32.46%, Test F1: 21.31%
CONN: Test Loss:   7.4,  Test Acc: 25.37%, Test F1:  2.53%
consistency_top_sec:  7.80%,  consistency_sec_conn:  2.41%, consistency_top_sec_conn:  2.31%
              precision    recall  f1-score   support

    Temporal     0.5000    0.5085    0.5042        59
 Contingency     0.1800    0.2195    0.1978        41
  Comparison     0.5000    0.2264    0.3117        53
   Expansion     0.5224    0.6087    0.5622       115

    accuracy                         0.4515       268
   macro avg     0.4256    0.3908    0.3940       268
weighted avg     0.4607    0.4515    0.4442       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5283    0.4746    0.5000        59
         Temporal.Synchrony     0.2258    0.3415    0.2718        41
          Contingency.Cause     0.2308    0.2857    0.2553        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5068    0.3458    0.4111       107
      Comparison.Concession     0.2857    0.2500    0.2667         8

                  micro avg     0.3937    0.3246    0.3558       268
                  macro avg     0.2962    0.2829    0.2842       268
               weighted avg     0.3798    0.3246    0.3438       268

Epoch [9/15]
top-down:TOP: Iter:   3200,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   6.2,  Val Acc: 60.69%, Val F1: 50.05% Time: 45.50994372367859 
top-down:SEC: Iter:   3200,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   6.2,  Val Acc: 45.58%, Val F1: 27.96% Time: 45.50994372367859 
top-down:CONN: Iter:   3200,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 24.12%, Val F1:  6.05% Time: 45.50994372367859 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   6.2,  Val Acc: 60.34%, Val F1: 50.20% Time: 125.11214208602905 
top-down:SEC: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 45.41%, Val F1: 27.04% Time: 125.11214208602905 
top-down:CONN: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 23.86%, Val F1:  6.09% Time: 125.11214208602905 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   6.3,  Val Acc: 60.94%, Val F1: 50.34% Time: 204.7552671432495 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 62.50%,Val Loss:   6.3,  Val Acc: 43.95%, Val F1: 28.34% Time: 204.7552671432495 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   6.3,  Val Acc: 25.67%, Val F1:  6.96% Time: 204.7552671432495 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.4e+01,  Train Acc: 81.25%,Val Loss:   6.3,  Val Acc: 61.46%, Val F1: 50.78% Time: 284.5516550540924 
top-down:SEC: Iter:   3500,  Train Loss: 3.4e+01,  Train Acc: 71.88%,Val Loss:   6.3,  Val Acc: 45.41%, Val F1: 27.04% Time: 284.5516550540924 
top-down:CONN: Iter:   3500,  Train Loss: 3.4e+01,  Train Acc: 40.62%,Val Loss:   6.3,  Val Acc: 24.38%, Val F1:  6.55% Time: 284.5516550540924 
 
 
Train time usage: 314.861887216568
Test time usage: 0.5495045185089111
TOP: Test Loss:   8.0,  Test Acc: 45.52%, Test F1: 42.10%
SEC: Test Loss:   8.0,  Test Acc: 33.21%, Test F1: 19.86%
CONN: Test Loss:   8.0,  Test Acc: 19.03%, Test F1:  1.88%
consistency_top_sec:  7.80%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.73%
              precision    recall  f1-score   support

    Temporal     0.5192    0.4576    0.4865        59
 Contingency     0.2075    0.2683    0.2340        41
  Comparison     0.4762    0.3774    0.4211        53
   Expansion     0.5289    0.5565    0.5424       115

    accuracy                         0.4552       268
   macro avg     0.4330    0.4150    0.4210       268
weighted avg     0.4672    0.4552    0.4589       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4407    0.4685        59
         Temporal.Synchrony     0.2364    0.3171    0.2708        41
          Contingency.Cause     0.2564    0.4762    0.3333        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5429    0.3551    0.4294       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4009    0.3321    0.3633       268
                  macro avg     0.3115    0.3065    0.2980       268
               weighted avg     0.3930    0.3321    0.3506       268

Epoch [10/15]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   6.3,  Val Acc: 61.12%, Val F1: 50.22% Time: 50.921329498291016 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.3,  Val Acc: 44.98%, Val F1: 26.42% Time: 50.921329498291016 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   6.3,  Val Acc: 24.64%, Val F1:  6.38% Time: 50.921329498291016 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.5,  Val Acc: 59.83%, Val F1: 48.45% Time: 130.56688714027405 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   6.5,  Val Acc: 42.75%, Val F1: 26.75% Time: 130.56688714027405 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.5,  Val Acc: 23.86%, Val F1:  6.52% Time: 130.56688714027405 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   6.4,  Val Acc: 61.29%, Val F1: 49.03% Time: 210.18292427062988 
top-down:SEC: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   6.4,  Val Acc: 45.06%, Val F1: 26.67% Time: 210.18292427062988 
top-down:CONN: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 40.62%,Val Loss:   6.4,  Val Acc: 24.55%, Val F1:  6.55% Time: 210.18292427062988 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   6.4,  Val Acc: 62.23%, Val F1: 50.55% Time: 289.9332253932953 
top-down:SEC: Iter:   3900,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   6.4,  Val Acc: 45.84%, Val F1: 27.17% Time: 289.9332253932953 
top-down:CONN: Iter:   3900,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   6.4,  Val Acc: 24.98%, Val F1:  6.65% Time: 289.9332253932953 
 
 
Train time usage: 314.52743101119995
Test time usage: 0.5461246967315674
TOP: Test Loss:   7.7,  Test Acc: 45.90%, Test F1: 41.06%
SEC: Test Loss:   7.7,  Test Acc: 30.97%, Test F1: 21.19%
CONN: Test Loss:   7.7,  Test Acc: 26.12%, Test F1:  2.18%
consistency_top_sec:  7.70%,  consistency_sec_conn:  2.02%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.5000    0.4407    0.4685        59
 Contingency     0.2115    0.2683    0.2366        41
  Comparison     0.4571    0.3019    0.3636        53
   Expansion     0.5426    0.6087    0.5738       115

    accuracy                         0.4590       268
   macro avg     0.4278    0.4049    0.4106       268
weighted avg     0.4657    0.4590    0.4574       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4898    0.4068    0.4444        59
         Temporal.Synchrony     0.2241    0.3171    0.2626        41
          Contingency.Cause     0.2500    0.4286    0.3158        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4730    0.3271    0.3867       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.3722    0.3097    0.3381       268
                  macro avg     0.2950    0.2883    0.2826       268
               weighted avg     0.3605    0.3097    0.3257       268

Epoch [11/15]
top-down:TOP: Iter:   4000,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   6.5,  Val Acc: 61.29%, Val F1: 50.38% Time: 56.17678761482239 
top-down:SEC: Iter:   4000,  Train Loss: 3.3e+01,  Train Acc: 65.62%,Val Loss:   6.5,  Val Acc: 45.75%, Val F1: 27.15% Time: 56.17678761482239 
top-down:CONN: Iter:   4000,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   6.5,  Val Acc: 24.38%, Val F1:  6.48% Time: 56.17678761482239 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 61.20%, Val F1: 48.78% Time: 136.1504237651825 
top-down:SEC: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   6.6,  Val Acc: 43.52%, Val F1: 26.21% Time: 136.1504237651825 
top-down:CONN: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   6.6,  Val Acc: 23.69%, Val F1:  6.43% Time: 136.1504237651825 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   6.6,  Val Acc: 60.69%, Val F1: 49.92% Time: 215.91568398475647 
top-down:SEC: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   6.6,  Val Acc: 43.95%, Val F1: 27.14% Time: 215.91568398475647 
top-down:CONN: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.6,  Val Acc: 23.86%, Val F1:  6.62% Time: 215.91568398475647 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 61.12%, Val F1: 50.68% Time: 295.6567165851593 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   6.6,  Val Acc: 45.32%, Val F1: 29.58% Time: 295.6567165851593 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   6.6,  Val Acc: 24.12%, Val F1:  6.46% Time: 295.6567165851593 
 
 
Train time usage: 314.7136723995209
Test time usage: 0.5446064472198486
TOP: Test Loss:   8.6,  Test Acc: 45.52%, Test F1: 41.34%
SEC: Test Loss:   8.6,  Test Acc: 33.21%, Test F1: 22.97%
CONN: Test Loss:   8.6,  Test Acc: 19.78%, Test F1:  1.83%
consistency_top_sec:  7.99%,  consistency_sec_conn:  1.35%, consistency_top_sec_conn:  1.25%
              precision    recall  f1-score   support

    Temporal     0.4925    0.5593    0.5238        59
 Contingency     0.2414    0.3415    0.2828        41
  Comparison     0.4615    0.2264    0.3038        53
   Expansion     0.5385    0.5478    0.5431       115

    accuracy                         0.4552       268
   macro avg     0.4335    0.4188    0.4134       268
weighted avg     0.4677    0.4552    0.4517       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5161    0.5424    0.5289        59
         Temporal.Synchrony     0.2742    0.4146    0.3301        41
          Contingency.Cause     0.2800    0.3333    0.3043        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5000    0.2897    0.3669       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.4120    0.3321    0.3678       268
                  macro avg     0.3284    0.3050    0.3063       268
               weighted avg     0.3891    0.3321    0.3464       268

Epoch [12/15]
top-down:TOP: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.6,  Val Acc: 60.86%, Val F1: 50.57% Time: 61.50841426849365 
top-down:SEC: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 45.06%, Val F1: 26.79% Time: 61.50841426849365 
top-down:CONN: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   6.6,  Val Acc: 23.95%, Val F1:  6.58% Time: 61.50841426849365 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 61.97%, Val F1: 50.92% Time: 140.98247361183167 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 45.84%, Val F1: 27.22% Time: 140.98247361183167 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   6.6,  Val Acc: 24.38%, Val F1:  6.63% Time: 140.98247361183167 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 60.77%, Val F1: 49.14% Time: 220.5275137424469 
top-down:SEC: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   6.7,  Val Acc: 44.21%, Val F1: 25.98% Time: 220.5275137424469 
top-down:CONN: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 34.38%,Val Loss:   6.7,  Val Acc: 23.61%, Val F1:  6.29% Time: 220.5275137424469 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.6,  Val Acc: 61.12%, Val F1: 49.22% Time: 299.96681666374207 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.6,  Val Acc: 46.44%, Val F1: 27.72% Time: 299.96681666374207 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 24.72%, Val F1:  6.57% Time: 299.96681666374207 
 
 
Train time usage: 313.9269711971283
Test time usage: 0.5354070663452148
TOP: Test Loss:   8.5,  Test Acc: 45.15%, Test F1: 39.10%
SEC: Test Loss:   8.5,  Test Acc: 32.09%, Test F1: 21.81%
CONN: Test Loss:   8.5,  Test Acc: 21.64%, Test F1:  2.09%
consistency_top_sec:  7.80%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.73%
              precision    recall  f1-score   support

    Temporal     0.4918    0.5085    0.5000        59
 Contingency     0.2264    0.2927    0.2553        41
  Comparison     0.4500    0.1698    0.2466        53
   Expansion     0.5224    0.6087    0.5622       115

    accuracy                         0.4515       268
   macro avg     0.4227    0.3949    0.3910       268
weighted avg     0.4561    0.4515    0.4392       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4746    0.4746    0.4746        59
         Temporal.Synchrony     0.2407    0.3171    0.2737        41
          Contingency.Cause     0.2800    0.3333    0.3043        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5143    0.3364    0.4068       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4019    0.3209    0.3568       268
                  macro avg     0.3072    0.2852    0.2909       268
               weighted avg     0.3785    0.3209    0.3411       268

Epoch [13/15]
top-down:TOP: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 61.37%, Val F1: 49.37% Time: 67.25659584999084 
top-down:SEC: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   6.7,  Val Acc: 45.75%, Val F1: 27.95% Time: 67.25659584999084 
top-down:CONN: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   6.7,  Val Acc: 24.64%, Val F1:  6.64% Time: 67.25659584999084 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   6.8,  Val Acc: 61.29%, Val F1: 49.70% Time: 146.90945291519165 
top-down:SEC: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 45.58%, Val F1: 27.83% Time: 146.90945291519165 
top-down:CONN: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   6.8,  Val Acc: 23.52%, Val F1:  6.37% Time: 146.90945291519165 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 60.34%, Val F1: 49.80% Time: 226.45973539352417 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 46.09%, Val F1: 27.72% Time: 226.45973539352417 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 24.46%, Val F1:  6.72% Time: 226.45973539352417 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   6.8,  Val Acc: 62.15%, Val F1: 51.34% Time: 305.9098451137543 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 45.84%, Val F1: 27.26% Time: 305.9098451137543 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 43.75%,Val Loss:   6.8,  Val Acc: 23.86%, Val F1:  6.34% Time: 305.9098451137543 
 
 
Train time usage: 313.97902250289917
Test time usage: 0.5309808254241943
TOP: Test Loss:   8.5,  Test Acc: 44.78%, Test F1: 39.37%
SEC: Test Loss:   8.5,  Test Acc: 31.72%, Test F1: 19.03%
CONN: Test Loss:   8.5,  Test Acc: 21.27%, Test F1:  1.95%
consistency_top_sec:  7.70%,  consistency_sec_conn:  1.92%, consistency_top_sec_conn:  1.73%
              precision    recall  f1-score   support

    Temporal     0.4844    0.5254    0.5041        59
 Contingency     0.2222    0.2927    0.2526        41
  Comparison     0.4545    0.1887    0.2667        53
   Expansion     0.5234    0.5826    0.5514       115

    accuracy                         0.4478       268
   macro avg     0.4211    0.3973    0.3937       268
weighted avg     0.4551    0.4478    0.4390       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4746    0.4746    0.4746        59
         Temporal.Synchrony     0.2069    0.2927    0.2424        41
          Contingency.Cause     0.2727    0.2857    0.2791        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5000    0.3458    0.4088       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.3899    0.3172    0.3498       268
                  macro avg     0.3090    0.2748    0.2854       268
               weighted avg     0.3691    0.3172    0.3358       268

Epoch [14/15]
top-down:TOP: Iter:   5200,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 61.63%, Val F1: 49.93% Time: 72.8476333618164 
top-down:SEC: Iter:   5200,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   6.8,  Val Acc: 45.49%, Val F1: 28.13% Time: 72.8476333618164 
top-down:CONN: Iter:   5200,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   6.8,  Val Acc: 24.29%, Val F1:  6.77% Time: 72.8476333618164 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 61.20%, Val F1: 49.16% Time: 152.60482239723206 
top-down:SEC: Iter:   5300,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   6.8,  Val Acc: 44.98%, Val F1: 26.96% Time: 152.60482239723206 
top-down:CONN: Iter:   5300,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   6.8,  Val Acc: 23.35%, Val F1:  6.42% Time: 152.60482239723206 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 96.88%,Val Loss:   6.8,  Val Acc: 61.20%, Val F1: 49.41% Time: 232.45940589904785 
top-down:SEC: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   6.8,  Val Acc: 45.24%, Val F1: 27.02% Time: 232.45940589904785 
top-down:CONN: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 50.00%,Val Loss:   6.8,  Val Acc: 23.09%, Val F1:  6.30% Time: 232.45940589904785 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   6.8,  Val Acc: 61.63%, Val F1: 50.13% Time: 312.13016843795776 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 45.75%, Val F1: 27.44% Time: 312.13016843795776 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 28.12%,Val Loss:   6.8,  Val Acc: 23.69%, Val F1:  6.47% Time: 312.13016843795776 
 
 
Train time usage: 314.88312005996704
Test time usage: 0.5246341228485107
TOP: Test Loss:   8.7,  Test Acc: 44.78%, Test F1: 39.00%
SEC: Test Loss:   8.7,  Test Acc: 31.34%, Test F1: 18.45%
CONN: Test Loss:   8.7,  Test Acc: 22.39%, Test F1:  1.93%
consistency_top_sec:  7.70%,  consistency_sec_conn:  1.92%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.4839    0.5085    0.4959        59
 Contingency     0.2037    0.2683    0.2316        41
  Comparison     0.5000    0.1887    0.2740        53
   Expansion     0.5227    0.6000    0.5587       115

    accuracy                         0.4478       268
   macro avg     0.4276    0.3914    0.3900       268
weighted avg     0.4609    0.4478    0.4385       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4737    0.4576    0.4655        59
         Temporal.Synchrony     0.2419    0.3659    0.2913        41
          Contingency.Cause     0.2222    0.1905    0.2051        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4675    0.3364    0.3913       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.3836    0.3134    0.3450       268
                  macro avg     0.3009    0.2667    0.2768       268
               weighted avg     0.3573    0.3134    0.3285       268

Epoch [15/15]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 61.80%, Val F1: 50.39% Time: 78.27605438232422 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 46.27%, Val F1: 27.70% Time: 78.27605438232422 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   6.8,  Val Acc: 23.78%, Val F1:  6.42% Time: 78.27605438232422 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 61.12%, Val F1: 49.88% Time: 157.96678113937378 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   6.9,  Val Acc: 45.49%, Val F1: 27.20% Time: 157.96678113937378 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   6.9,  Val Acc: 23.26%, Val F1:  6.31% Time: 157.96678113937378 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 81.25%,Val Loss:   6.9,  Val Acc: 61.29%, Val F1: 50.53% Time: 237.85571479797363 
top-down:SEC: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   6.9,  Val Acc: 45.67%, Val F1: 27.39% Time: 237.85571479797363 
top-down:CONN: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 56.25%,Val Loss:   6.9,  Val Acc: 23.52%, Val F1:  6.39% Time: 237.85571479797363 
 
 
Train time usage: 313.03109765052795
Test time usage: 0.502751350402832
TOP: Test Loss:   8.7,  Test Acc: 43.66%, Test F1: 38.19%
SEC: Test Loss:   8.7,  Test Acc: 31.72%, Test F1: 19.13%
CONN: Test Loss:   8.7,  Test Acc: 22.01%, Test F1:  1.90%
consistency_top_sec:  7.41%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.64%
              precision    recall  f1-score   support

    Temporal     0.4754    0.4915    0.4833        59
 Contingency     0.2000    0.2683    0.2292        41
  Comparison     0.4762    0.1887    0.2703        53
   Expansion     0.5115    0.5826    0.5447       115

    accuracy                         0.4366       268
   macro avg     0.4158    0.3828    0.3819       268
weighted avg     0.4489    0.4366    0.4287       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4727    0.4407    0.4561        59
         Temporal.Synchrony     0.2419    0.3659    0.2913        41
          Contingency.Cause     0.2609    0.2857    0.2727        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4737    0.3364    0.3934       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.3846    0.3172    0.3476       268
                  macro avg     0.3082    0.2798    0.2869       268
               weighted avg     0.3626    0.3172    0.3326       268

dev_best_acc_top: 61.97%,  dev_best_f1_top: 51.56%, 
dev_best_acc_sec: 46.95%,  dev_best_f1_sec: 27.72%, 
dev_best_acc_conn: 26.44%,  dev_best_f1_conn:  6.73%
