nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_tr_train_tdb_test/data/', 'log_file': 'data/pdtb_tr_train_tdb_test/log/', 'save_file': 'data/pdtb_tr_train_tdb_test/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 30, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March08-08:54:32', 'log': 'data/pdtb_tr_train_tdb_test/log/March08-08:54:32.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]84it [00:00, 838.53it/s]283it [00:00, 1509.30it/s]468it [00:00, 1662.20it/s]669it [00:00, 1798.52it/s]872it [00:00, 1880.19it/s]1061it [00:00, 1772.90it/s]1240it [00:00, 1775.00it/s]1425it [00:00, 1797.09it/s]1606it [00:00, 1721.82it/s]1782it [00:01, 1730.70it/s]1968it [00:01, 1767.65it/s]2146it [00:01, 1763.17it/s]2325it [00:01, 1769.95it/s]2503it [00:01, 1734.58it/s]2685it [00:01, 1758.00it/s]2867it [00:01, 1774.15it/s]3045it [00:01, 1740.05it/s]3220it [00:01, 1696.11it/s]3413it [00:01, 1762.46it/s]3594it [00:02, 1775.46it/s]3781it [00:02, 1800.29it/s]3962it [00:02, 1749.47it/s]4146it [00:02, 1774.15it/s]4324it [00:02, 1704.80it/s]4510it [00:02, 1746.63it/s]4686it [00:02, 1716.73it/s]4862it [00:02, 1729.03it/s]5043it [00:02, 1751.83it/s]5219it [00:03, 1196.61it/s]5368it [00:03, 1261.02it/s]5551it [00:03, 1396.94it/s]5721it [00:03, 1474.53it/s]5918it [00:03, 1605.32it/s]6096it [00:03, 1651.64it/s]6277it [00:03, 1695.80it/s]6455it [00:03, 1719.15it/s]6631it [00:03, 1710.73it/s]6805it [00:04, 1656.01it/s]6984it [00:04, 1692.88it/s]7156it [00:04, 1672.21it/s]7334it [00:04, 1702.58it/s]7506it [00:04, 1666.33it/s]7675it [00:04, 1669.56it/s]7855it [00:04, 1705.79it/s]8027it [00:04, 1685.14it/s]8196it [00:04, 1670.80it/s]8370it [00:04, 1688.92it/s]8548it [00:05, 1715.12it/s]8732it [00:05, 1751.56it/s]8917it [00:05, 1778.30it/s]9095it [00:05, 1660.05it/s]9263it [00:05, 1647.10it/s]9429it [00:05, 1636.79it/s]9615it [00:05, 1700.58it/s]9792it [00:05, 1719.35it/s]9974it [00:05, 1748.80it/s]10150it [00:06, 1743.81it/s]10334it [00:06, 1769.99it/s]10512it [00:06, 1762.81it/s]10689it [00:06, 1693.18it/s]10875it [00:06, 1738.79it/s]11062it [00:06, 1774.81it/s]11245it [00:06, 1790.70it/s]11433it [00:06, 1813.93it/s]11615it [00:06, 1808.64it/s]11797it [00:06, 1810.82it/s]11979it [00:07, 1762.95it/s]12156it [00:07, 1664.04it/s]12330it [00:07, 1681.09it/s]12500it [00:07, 1651.64it/s]12547it [00:07, 1691.37it/s]
0it [00:00, ?it/s]127it [00:00, 1268.21it/s]292it [00:00, 1492.59it/s]467it [00:00, 1608.33it/s]628it [00:00, 1601.93it/s]795it [00:00, 1624.86it/s]958it [00:00, 1608.42it/s]1128it [00:00, 1636.66it/s]1165it [00:00, 1607.49it/s]
0it [00:00, ?it/s]241it [00:00, 2394.16it/s]268it [00:00, 2356.45it/s]
Time usage: 18.635140895843506
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/30]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 55.88%, Val F1: 17.92% Time: 96.03602123260498 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   7.5,  Val Acc: 24.21%, Val F1:  3.54% Time: 96.03602123260498 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  0.00%,Val Loss:   7.5,  Val Acc:  1.12%, Val F1:  0.14% Time: 96.03602123260498 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 55.88%, Val F1: 17.92% Time: 184.32119941711426 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   6.6,  Val Acc: 27.90%, Val F1:  6.18% Time: 184.32119941711426 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.6,  Val Acc: 12.88%, Val F1:  0.34% Time: 184.32119941711426 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 75.00%,Val Loss:   6.4,  Val Acc: 55.71%, Val F1: 18.55% Time: 273.02633786201477 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 28.12%,Val Loss:   6.4,  Val Acc: 30.47%, Val F1:  9.77% Time: 273.02633786201477 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 12.50%,Val Loss:   6.4,  Val Acc: 17.08%, Val F1:  1.09% Time: 273.02633786201477 *
 
 
Train time usage: 353.02693009376526
Test time usage: 0.5462944507598877
TOP: Test Loss:   6.5,  Test Acc: 41.04%, Test F1: 20.16%
SEC: Test Loss:   6.5,  Test Acc: 11.19%, Test F1:  7.15%
CONN: Test Loss:   6.5,  Test Acc: 15.67%, Test F1:  9.03%
consistency_top_sec:  1.92%,  consistency_sec_conn:  1.15%, consistency_top_sec_conn:  1.15%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        59
 Contingency     0.2857    0.1951    0.2319        41
  Comparison     0.0000    0.0000    0.0000        53
   Expansion     0.4250    0.8870    0.5746       115

    accuracy                         0.4104       268
   macro avg     0.1777    0.2705    0.2016       268
weighted avg     0.2261    0.4104    0.2821       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        59
         Temporal.Synchrony     0.2338    0.4390    0.3051        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.7500    0.1121    0.1951       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                  micro avg     0.3226    0.1119    0.1662       268
                  macro avg     0.1640    0.0919    0.0834       268
               weighted avg     0.3352    0.1119    0.1246       268

Epoch [2/30]
top-down:TOP: Iter:    400,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   6.3,  Val Acc: 54.76%, Val F1: 27.87% Time: 9.839522123336792 *
top-down:SEC: Iter:    400,  Train Loss: 3e+01,  Train Acc: 25.00%,Val Loss:   6.3,  Val Acc: 33.48%, Val F1: 10.33% Time: 9.839522123336792 *
top-down:CONN: Iter:    400,  Train Loss: 3e+01,  Train Acc:  9.38%,Val Loss:   6.3,  Val Acc: 17.00%, Val F1:  1.12% Time: 9.839522123336792 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.3,  Val Acc: 50.82%, Val F1: 27.62% Time: 96.63882207870483 
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.3,  Val Acc: 33.65%, Val F1: 11.17% Time: 96.63882207870483 
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.3,  Val Acc: 17.94%, Val F1:  1.40% Time: 96.63882207870483 
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 65.62%,Val Loss:   6.1,  Val Acc: 52.62%, Val F1: 35.20% Time: 185.17923188209534 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 46.88%,Val Loss:   6.1,  Val Acc: 37.85%, Val F1: 17.42% Time: 185.17923188209534 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 31.25%,Val Loss:   6.1,  Val Acc: 20.43%, Val F1:  2.40% Time: 185.17923188209534 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   5.8,  Val Acc: 57.68%, Val F1: 42.75% Time: 273.7632369995117 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   5.8,  Val Acc: 39.31%, Val F1: 16.71% Time: 273.7632369995117 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   5.8,  Val Acc: 21.46%, Val F1:  2.71% Time: 273.7632369995117 *
 
 
Train time usage: 347.5390100479126
Test time usage: 0.5413920879364014
TOP: Test Loss:   5.4,  Test Acc: 47.76%, Test F1: 38.63%
SEC: Test Loss:   5.4,  Test Acc: 36.19%, Test F1: 23.13%
CONN: Test Loss:   5.4,  Test Acc: 25.00%, Test F1:  5.00%
consistency_top_sec:  7.80%,  consistency_sec_conn:  3.08%, consistency_top_sec_conn:  3.08%
              precision    recall  f1-score   support

    Temporal     0.4487    0.5932    0.5109        59
 Contingency     0.3103    0.2195    0.2571        41
  Comparison     0.6000    0.1132    0.1905        53
   Expansion     0.5166    0.6783    0.5865       115

    accuracy                         0.4776       268
   macro avg     0.4689    0.4011    0.3863       268
weighted avg     0.4866    0.4776    0.4411       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4096    0.5763    0.4789        59
         Temporal.Synchrony     0.2727    0.5854    0.3721        41
          Contingency.Cause     0.1538    0.0952    0.1176        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5902    0.3364    0.4286       107
      Comparison.Concession     1.0000    0.1250    0.2222         8

                  micro avg     0.3943    0.3619    0.3774       268
                  macro avg     0.4044    0.2864    0.2699       268
               weighted avg     0.4094    0.3619    0.3493       268

Epoch [3/30]
top-down:TOP: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 59.31%, Val F1: 45.16% Time: 15.764597177505493 *
top-down:SEC: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 42.40%, Val F1: 21.95% Time: 15.764597177505493 *
top-down:CONN: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 21.88%,Val Loss:   5.6,  Val Acc: 21.20%, Val F1:  3.25% Time: 15.764597177505493 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.6,  Val Acc: 60.86%, Val F1: 46.32% Time: 104.50206065177917 *
top-down:SEC: Iter:    900,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 44.21%, Val F1: 25.88% Time: 104.50206065177917 *
top-down:CONN: Iter:    900,  Train Loss: 3e+01,  Train Acc:  9.38%,Val Loss:   5.6,  Val Acc: 23.35%, Val F1:  3.22% Time: 104.50206065177917 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.6,  Val Acc: 61.72%, Val F1: 44.49% Time: 191.35273098945618 
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 44.03%, Val F1: 22.43% Time: 191.35273098945618 
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   5.6,  Val Acc: 21.12%, Val F1:  3.32% Time: 191.35273098945618 
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 60.00%, Val F1: 44.27% Time: 278.5080211162567 
top-down:SEC: Iter:   1100,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   5.5,  Val Acc: 44.72%, Val F1: 24.48% Time: 278.5080211162567 
top-down:CONN: Iter:   1100,  Train Loss: 3.2e+01,  Train Acc: 28.12%,Val Loss:   5.5,  Val Acc: 22.32%, Val F1:  3.44% Time: 278.5080211162567 
 
 
Train time usage: 346.37773513793945
Test time usage: 0.5472466945648193
TOP: Test Loss:   6.0,  Test Acc: 47.76%, Test F1: 43.80%
SEC: Test Loss:   6.0,  Test Acc: 37.69%, Test F1: 22.60%
CONN: Test Loss:   6.0,  Test Acc: 23.88%, Test F1:  3.50%
consistency_top_sec:  8.28%,  consistency_sec_conn:  2.79%, consistency_top_sec_conn:  2.50%
              precision    recall  f1-score   support

    Temporal     0.4559    0.5254    0.4882        59
 Contingency     0.3243    0.2927    0.3077        41
  Comparison     0.4865    0.3396    0.4000        53
   Expansion     0.5317    0.5826    0.5560       115

    accuracy                         0.4776       268
   macro avg     0.4496    0.4351    0.4380       268
weighted avg     0.4744    0.4776    0.4722       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4267    0.5424    0.4776        59
         Temporal.Synchrony     0.2985    0.4878    0.3704        41
          Contingency.Cause     0.1875    0.4286    0.2609        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.6452    0.3738    0.4734       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                  micro avg     0.4008    0.3769    0.3885       268
                  macro avg     0.2596    0.3054    0.2637       268
               weighted avg     0.4119    0.3769    0.3712       268

Epoch [4/30]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 59.23%, Val F1: 45.66% Time: 20.087144136428833 
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 75.00%,Val Loss:   5.5,  Val Acc: 45.06%, Val F1: 23.30% Time: 20.087144136428833 
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 23.86%, Val F1:  4.38% Time: 20.087144136428833 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 61.46%, Val F1: 45.58% Time: 108.58704805374146 *
top-down:SEC: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 45.75%, Val F1: 25.42% Time: 108.58704805374146 *
top-down:CONN: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 24.29%, Val F1:  4.69% Time: 108.58704805374146 *
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 61.46%, Val F1: 48.38% Time: 197.01373958587646 *
top-down:SEC: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 46.35%, Val F1: 28.77% Time: 197.01373958587646 *
top-down:CONN: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 25.32%, Val F1:  5.23% Time: 197.01373958587646 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 65.62%,Val Loss:   5.6,  Val Acc: 59.06%, Val F1: 43.07% Time: 284.03087282180786 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 44.55%, Val F1: 24.55% Time: 284.03087282180786 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 28.12%,Val Loss:   5.6,  Val Acc: 23.86%, Val F1:  5.02% Time: 284.03087282180786 
 
 
Train time usage: 345.8529706001282
Test time usage: 0.5401773452758789
TOP: Test Loss:   6.3,  Test Acc: 49.63%, Test F1: 43.19%
SEC: Test Loss:   6.3,  Test Acc: 30.97%, Test F1: 23.26%
CONN: Test Loss:   6.3,  Test Acc: 25.37%, Test F1:  3.11%
consistency_top_sec:  6.74%,  consistency_sec_conn:  2.12%, consistency_top_sec_conn:  2.02%
              precision    recall  f1-score   support

    Temporal     0.6000    0.3051    0.4045        59
 Contingency     0.2941    0.2439    0.2667        41
  Comparison     0.5882    0.3774    0.4598        53
   Expansion     0.5000    0.7391    0.5965       115

    accuracy                         0.4963       268
   macro avg     0.4956    0.4164    0.4319       268
weighted avg     0.5080    0.4963    0.4767       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5641    0.3729    0.4490        59
         Temporal.Synchrony     0.2500    0.4390    0.3186        41
          Contingency.Cause     0.2162    0.3810    0.2759        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.6471    0.3084    0.4177       107
      Comparison.Concession     1.0000    0.2500    0.4000         8

                  micro avg     0.4129    0.3097    0.3539       268
                  macro avg     0.4462    0.2919    0.3102       268
               weighted avg     0.4676    0.3097    0.3479       268

Epoch [5/30]
top-down:TOP: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 61.80%, Val F1: 48.91% Time: 26.072455406188965 
top-down:SEC: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   5.6,  Val Acc: 45.58%, Val F1: 27.42% Time: 26.072455406188965 
top-down:CONN: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 25.00%,Val Loss:   5.6,  Val Acc: 24.72%, Val F1:  5.10% Time: 26.072455406188965 
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 59.06%, Val F1: 48.63% Time: 112.9308648109436 
top-down:SEC: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 44.29%, Val F1: 27.57% Time: 112.9308648109436 
top-down:CONN: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   5.7,  Val Acc: 23.09%, Val F1:  4.82% Time: 112.9308648109436 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.7,  Val Acc: 61.46%, Val F1: 50.01% Time: 201.41432785987854 *
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 46.78%, Val F1: 30.34% Time: 201.41432785987854 *
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   5.7,  Val Acc: 24.98%, Val F1:  5.54% Time: 201.41432785987854 *
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 59.06%, Val F1: 48.47% Time: 288.27188372612 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.6,  Val Acc: 45.67%, Val F1: 26.75% Time: 288.27188372612 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 25.00%,Val Loss:   5.6,  Val Acc: 26.01%, Val F1:  5.83% Time: 288.27188372612 
 
 
Train time usage: 344.1605393886566
Test time usage: 0.5453047752380371
TOP: Test Loss:   6.2,  Test Acc: 49.63%, Test F1: 45.36%
SEC: Test Loss:   6.2,  Test Acc: 37.69%, Test F1: 24.41%
CONN: Test Loss:   6.2,  Test Acc: 23.51%, Test F1:  2.93%
consistency_top_sec:  8.76%,  consistency_sec_conn:  3.08%, consistency_top_sec_conn:  2.98%
              precision    recall  f1-score   support

    Temporal     0.5263    0.5085    0.5172        59
 Contingency     0.2750    0.2683    0.2716        41
  Comparison     0.5556    0.3774    0.4494        53
   Expansion     0.5333    0.6261    0.5760       115

    accuracy                         0.4963       268
   macro avg     0.4726    0.4451    0.4536       268
weighted avg     0.4967    0.4963    0.4915       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5238    0.5593    0.5410        59
         Temporal.Synchrony     0.2712    0.3902    0.3200        41
          Contingency.Cause     0.2250    0.4286    0.2951        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5857    0.3832    0.4633       107
      Comparison.Concession     0.5000    0.2500    0.3333         8

                  micro avg     0.4280    0.3769    0.4008       268
                  macro avg     0.3510    0.3352    0.3254       268
               weighted avg     0.4232    0.3769    0.3861       268

Epoch [6/30]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 63.00%, Val F1: 50.60% Time: 33.496219635009766 *
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 48.33%, Val F1: 28.42% Time: 33.496219635009766 *
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   5.5,  Val Acc: 27.04%, Val F1:  6.34% Time: 33.496219635009766 *
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 62.83%, Val F1: 49.56% Time: 120.39975070953369 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 48.24%, Val F1: 30.02% Time: 120.39975070953369 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   5.6,  Val Acc: 25.49%, Val F1:  6.03% Time: 120.39975070953369 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 60.77%, Val F1: 49.43% Time: 207.51254177093506 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.7,  Val Acc: 47.30%, Val F1: 28.70% Time: 207.51254177093506 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   5.7,  Val Acc: 24.98%, Val F1:  5.83% Time: 207.51254177093506 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 61.03%, Val F1: 48.69% Time: 294.47142338752747 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 45.49%, Val F1: 26.82% Time: 294.47142338752747 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 34.38%,Val Loss:   5.7,  Val Acc: 25.32%, Val F1:  5.78% Time: 294.47142338752747 
 
 
Train time usage: 344.40028524398804
Test time usage: 0.5508897304534912
TOP: Test Loss:   7.1,  Test Acc: 47.01%, Test F1: 43.36%
SEC: Test Loss:   7.1,  Test Acc: 30.22%, Test F1: 20.29%
CONN: Test Loss:   7.1,  Test Acc: 19.03%, Test F1:  1.88%
consistency_top_sec:  7.03%,  consistency_sec_conn:  1.15%, consistency_top_sec_conn:  1.15%
              precision    recall  f1-score   support

    Temporal     0.5333    0.5424    0.5378        59
 Contingency     0.2222    0.2439    0.2326        41
  Comparison     0.5278    0.3585    0.4270        53
   Expansion     0.5118    0.5652    0.5372       115

    accuracy                         0.4701       268
   macro avg     0.4488    0.4275    0.4336       268
weighted avg     0.4754    0.4701    0.4689       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5397    0.5763    0.5574        59
         Temporal.Synchrony     0.2545    0.3415    0.2917        41
          Contingency.Cause     0.2000    0.3810    0.2623        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5106    0.2243    0.3117       107
      Comparison.Concession     0.5000    0.1250    0.2000         8

                  micro avg     0.3913    0.3022    0.3411       268
                  macro avg     0.3341    0.2747    0.2705       268
               weighted avg     0.3922    0.3022    0.3183       268

Epoch [7/30]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 59.74%, Val F1: 49.10% Time: 37.82266116142273 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   6.0,  Val Acc: 43.95%, Val F1: 27.93% Time: 37.82266116142273 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 31.25%,Val Loss:   6.0,  Val Acc: 23.52%, Val F1:  6.08% Time: 37.82266116142273 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 60.77%, Val F1: 48.51% Time: 124.7943263053894 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.8,  Val Acc: 44.81%, Val F1: 27.53% Time: 124.7943263053894 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   5.8,  Val Acc: 24.29%, Val F1:  5.96% Time: 124.7943263053894 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   5.9,  Val Acc: 60.17%, Val F1: 48.87% Time: 211.58044171333313 
top-down:SEC: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 59.38%,Val Loss:   5.9,  Val Acc: 45.49%, Val F1: 27.19% Time: 211.58044171333313 
top-down:CONN: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 28.12%,Val Loss:   5.9,  Val Acc: 25.32%, Val F1:  6.26% Time: 211.58044171333313 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   5.8,  Val Acc: 61.03%, Val F1: 48.34% Time: 298.28592324256897 
top-down:SEC: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 65.62%,Val Loss:   5.8,  Val Acc: 47.64%, Val F1: 29.34% Time: 298.28592324256897 
top-down:CONN: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 31.25%,Val Loss:   5.8,  Val Acc: 26.35%, Val F1:  6.56% Time: 298.28592324256897 
 
 
Train time usage: 342.39050555229187
Test time usage: 0.5450830459594727
TOP: Test Loss:   7.7,  Test Acc: 46.27%, Test F1: 42.64%
SEC: Test Loss:   7.7,  Test Acc: 33.21%, Test F1: 23.75%
CONN: Test Loss:   7.7,  Test Acc: 17.16%, Test F1:  1.83%
consistency_top_sec:  8.08%,  consistency_sec_conn:  1.92%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.5536    0.5254    0.5391        59
 Contingency     0.2714    0.4634    0.3423        41
  Comparison     0.4231    0.2075    0.2785        53
   Expansion     0.5431    0.5478    0.5455       115

    accuracy                         0.4627       268
   macro avg     0.4478    0.4361    0.4264       268
weighted avg     0.4801    0.4627    0.4602       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5263    0.5085    0.5172        59
         Temporal.Synchrony     0.2973    0.5366    0.3826        41
          Contingency.Cause     0.1429    0.1905    0.1633        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5660    0.2804    0.3750       107
      Comparison.Concession     0.6000    0.3750    0.4615         8

                  micro avg     0.4101    0.3321    0.3670       268
                  macro avg     0.3554    0.3152    0.3166       268
               weighted avg     0.4164    0.3321    0.3487       268

Epoch [8/30]
top-down:TOP: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 60.69%, Val F1: 50.07% Time: 43.740492820739746 
top-down:SEC: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 45.24%, Val F1: 28.07% Time: 43.740492820739746 
top-down:CONN: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   6.0,  Val Acc: 24.38%, Val F1:  6.43% Time: 43.740492820739746 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   6.2,  Val Acc: 59.57%, Val F1: 48.71% Time: 130.71621656417847 
top-down:SEC: Iter:   2900,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 44.29%, Val F1: 27.46% Time: 130.71621656417847 
top-down:CONN: Iter:   2900,  Train Loss: 2.7e+01,  Train Acc: 37.50%,Val Loss:   6.2,  Val Acc: 24.89%, Val F1:  6.49% Time: 130.71621656417847 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.2,  Val Acc: 59.66%, Val F1: 48.69% Time: 217.6249177455902 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.2,  Val Acc: 43.61%, Val F1: 26.60% Time: 217.6249177455902 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   6.2,  Val Acc: 23.95%, Val F1:  6.54% Time: 217.6249177455902 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   6.1,  Val Acc: 61.63%, Val F1: 47.94% Time: 304.8073878288269 
top-down:SEC: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   6.1,  Val Acc: 44.72%, Val F1: 26.46% Time: 304.8073878288269 
top-down:CONN: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   6.1,  Val Acc: 24.89%, Val F1:  6.35% Time: 304.8073878288269 
 
 
Train time usage: 343.1943738460541
Test time usage: 0.5443308353424072
TOP: Test Loss:   7.4,  Test Acc: 45.15%, Test F1: 42.07%
SEC: Test Loss:   7.4,  Test Acc: 34.70%, Test F1: 21.55%
CONN: Test Loss:   7.4,  Test Acc: 19.40%, Test F1:  1.91%
consistency_top_sec:  7.89%,  consistency_sec_conn:  1.92%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.5179    0.4915    0.5043        59
 Contingency     0.2264    0.2927    0.2553        41
  Comparison     0.4865    0.3396    0.4000        53
   Expansion     0.5082    0.5391    0.5232       115

    accuracy                         0.4515       268
   macro avg     0.4347    0.4157    0.4207       268
weighted avg     0.4629    0.4515    0.4537       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4576    0.4779        59
         Temporal.Synchrony     0.2676    0.4634    0.3393        41
          Contingency.Cause     0.2647    0.4286    0.3273        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5000    0.3271    0.3955       107
      Comparison.Concession     0.4286    0.3750    0.4000         8

                  micro avg     0.3941    0.3470    0.3690       268
                  macro avg     0.3268    0.3420    0.3233       268
               weighted avg     0.3842    0.3470    0.3526       268

Epoch [9/30]
top-down:TOP: Iter:   3200,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 60.09%, Val F1: 49.01% Time: 49.705562114715576 
top-down:SEC: Iter:   3200,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   6.2,  Val Acc: 44.98%, Val F1: 28.27% Time: 49.705562114715576 
top-down:CONN: Iter:   3200,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   6.2,  Val Acc: 25.06%, Val F1:  5.83% Time: 49.705562114715576 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   6.3,  Val Acc: 60.94%, Val F1: 50.15% Time: 136.66718745231628 
top-down:SEC: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   6.3,  Val Acc: 45.24%, Val F1: 27.81% Time: 136.66718745231628 
top-down:CONN: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   6.3,  Val Acc: 24.81%, Val F1:  6.27% Time: 136.66718745231628 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.4,  Val Acc: 61.20%, Val F1: 50.82% Time: 223.6077914237976 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   6.4,  Val Acc: 45.49%, Val F1: 27.88% Time: 223.6077914237976 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 43.75%,Val Loss:   6.4,  Val Acc: 24.98%, Val F1:  6.52% Time: 223.6077914237976 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.4e+01,  Train Acc: 90.62%,Val Loss:   6.4,  Val Acc: 60.86%, Val F1: 51.47% Time: 310.593829870224 
top-down:SEC: Iter:   3500,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   6.4,  Val Acc: 45.67%, Val F1: 28.72% Time: 310.593829870224 
top-down:CONN: Iter:   3500,  Train Loss: 3.4e+01,  Train Acc: 40.62%,Val Loss:   6.4,  Val Acc: 24.64%, Val F1:  6.67% Time: 310.593829870224 
 
 
Train time usage: 343.0084924697876
Test time usage: 0.5452747344970703
TOP: Test Loss:   8.3,  Test Acc: 47.01%, Test F1: 43.99%
SEC: Test Loss:   8.3,  Test Acc: 32.09%, Test F1: 21.36%
CONN: Test Loss:   8.3,  Test Acc: 14.18%, Test F1:  1.31%
consistency_top_sec:  7.70%,  consistency_sec_conn:  1.54%, consistency_top_sec_conn:  1.44%
              precision    recall  f1-score   support

    Temporal     0.4906    0.4407    0.4643        59
 Contingency     0.2449    0.2927    0.2667        41
  Comparison     0.4902    0.4717    0.4808        53
   Expansion     0.5478    0.5478    0.5478       115

    accuracy                         0.4701       268
   macro avg     0.4434    0.4382    0.4399       268
weighted avg     0.4775    0.4701    0.4732       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4909    0.4576    0.4737        59
         Temporal.Synchrony     0.2692    0.3415    0.3011        41
          Contingency.Cause     0.2553    0.5714    0.3529        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5263    0.2804    0.3659       107
      Comparison.Concession     0.5000    0.3750    0.4286         8

                  micro avg     0.3963    0.3209    0.3546       268
                  macro avg     0.3403    0.3376    0.3204       268
               weighted avg     0.3943    0.3209    0.3369       268

Epoch [10/30]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 60.60%, Val F1: 50.41% Time: 55.471609354019165 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 71.88%,Val Loss:   6.6,  Val Acc: 45.92%, Val F1: 28.38% Time: 55.471609354019165 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 34.38%,Val Loss:   6.6,  Val Acc: 24.89%, Val F1:  6.08% Time: 55.471609354019165 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   6.8,  Val Acc: 59.23%, Val F1: 47.04% Time: 142.4160554409027 
top-down:SEC: Iter:   3700,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   6.8,  Val Acc: 43.43%, Val F1: 27.57% Time: 142.4160554409027 
top-down:CONN: Iter:   3700,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   6.8,  Val Acc: 23.86%, Val F1:  7.08% Time: 142.4160554409027 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   6.6,  Val Acc: 60.86%, Val F1: 50.17% Time: 229.38357424736023 
top-down:SEC: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   6.6,  Val Acc: 44.98%, Val F1: 28.01% Time: 229.38357424736023 
top-down:CONN: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 40.62%,Val Loss:   6.6,  Val Acc: 25.06%, Val F1:  6.48% Time: 229.38357424736023 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 61.46%, Val F1: 50.35% Time: 316.38125371932983 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.7,  Val Acc: 44.81%, Val F1: 27.96% Time: 316.38125371932983 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   6.7,  Val Acc: 23.95%, Val F1:  6.19% Time: 316.38125371932983 
 
 
Train time usage: 342.8759641647339
Test time usage: 0.5520610809326172
TOP: Test Loss:   8.3,  Test Acc: 43.66%, Test F1: 40.99%
SEC: Test Loss:   8.3,  Test Acc: 30.60%, Test F1: 19.97%
CONN: Test Loss:   8.3,  Test Acc: 16.42%, Test F1:  1.48%
consistency_top_sec:  7.41%,  consistency_sec_conn:  1.64%, consistency_top_sec_conn:  1.44%
              precision    recall  f1-score   support

    Temporal     0.4694    0.3898    0.4259        59
 Contingency     0.1970    0.3171    0.2430        41
  Comparison     0.4884    0.3962    0.4375        53
   Expansion     0.5455    0.5217    0.5333       115

    accuracy                         0.4366       268
   macro avg     0.4250    0.4062    0.4099       268
weighted avg     0.4641    0.4366    0.4463       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4783    0.3729    0.4190        59
         Temporal.Synchrony     0.2059    0.3415    0.2569        41
          Contingency.Cause     0.2500    0.5238    0.3385        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5333    0.2991    0.3832       107
      Comparison.Concession     0.4286    0.3750    0.4000         8

                  micro avg     0.3644    0.3060    0.3327       268
                  macro avg     0.3160    0.3187    0.2996       268
               weighted avg     0.3821    0.3060    0.3230       268

Epoch [11/30]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 60.17%, Val F1: 48.11% Time: 61.52527832984924 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   6.9,  Val Acc: 43.35%, Val F1: 27.09% Time: 61.52527832984924 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   6.9,  Val Acc: 23.43%, Val F1:  6.66% Time: 61.52527832984924 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.9,  Val Acc: 61.29%, Val F1: 46.19% Time: 148.47883439064026 
top-down:SEC: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   6.9,  Val Acc: 44.81%, Val F1: 28.39% Time: 148.47883439064026 
top-down:CONN: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 43.75%,Val Loss:   6.9,  Val Acc: 24.03%, Val F1:  6.33% Time: 148.47883439064026 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   6.9,  Val Acc: 59.14%, Val F1: 48.52% Time: 235.3815152645111 
top-down:SEC: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   6.9,  Val Acc: 43.86%, Val F1: 27.44% Time: 235.3815152645111 
top-down:CONN: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.9,  Val Acc: 23.78%, Val F1:  6.70% Time: 235.3815152645111 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   6.8,  Val Acc: 61.03%, Val F1: 49.68% Time: 322.307804107666 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.8,  Val Acc: 46.01%, Val F1: 29.75% Time: 322.307804107666 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   6.8,  Val Acc: 26.09%, Val F1:  7.45% Time: 322.307804107666 
 
 
Train time usage: 342.7666702270508
Test time usage: 0.5455052852630615
TOP: Test Loss:   8.9,  Test Acc: 45.52%, Test F1: 41.31%
SEC: Test Loss:   8.9,  Test Acc: 31.72%, Test F1: 18.97%
CONN: Test Loss:   8.9,  Test Acc: 19.03%, Test F1:  1.68%
consistency_top_sec:  8.08%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.83%
              precision    recall  f1-score   support

    Temporal     0.5082    0.5254    0.5167        59
 Contingency     0.2364    0.3171    0.2708        41
  Comparison     0.4643    0.2453    0.3210        53
   Expansion     0.5242    0.5652    0.5439       115

    accuracy                         0.4552       268
   macro avg     0.4333    0.4132    0.4131       268
weighted avg     0.4648    0.4552    0.4521       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4918    0.5085    0.5000        59
         Temporal.Synchrony     0.2414    0.3415    0.2828        41
          Contingency.Cause     0.1429    0.1905    0.1633        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4928    0.3178    0.3864       107
      Comparison.Concession     0.3750    0.3750    0.3750         8

                  micro avg     0.3795    0.3172    0.3455       268
                  macro avg     0.2906    0.2889    0.2846       268
               weighted avg     0.3643    0.3172    0.3316       268

Epoch [12/30]
top-down:TOP: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   7.0,  Val Acc: 59.31%, Val F1: 50.18% Time: 67.53567790985107 
top-down:SEC: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   7.0,  Val Acc: 45.67%, Val F1: 30.33% Time: 67.53567790985107 
top-down:CONN: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   7.0,  Val Acc: 25.24%, Val F1:  7.22% Time: 67.53567790985107 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 59.40%, Val F1: 49.22% Time: 154.61447978019714 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   7.0,  Val Acc: 44.12%, Val F1: 27.14% Time: 154.61447978019714 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   7.0,  Val Acc: 25.32%, Val F1:  6.61% Time: 154.61447978019714 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.1,  Val Acc: 60.69%, Val F1: 47.92% Time: 241.32028102874756 
top-down:SEC: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   7.1,  Val Acc: 44.64%, Val F1: 27.49% Time: 241.32028102874756 
top-down:CONN: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   7.1,  Val Acc: 24.12%, Val F1:  6.75% Time: 241.32028102874756 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 61.12%, Val F1: 49.02% Time: 328.04584980010986 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 46.01%, Val F1: 29.66% Time: 328.04584980010986 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   7.1,  Val Acc: 24.29%, Val F1:  6.37% Time: 328.04584980010986 
 
 
Train time usage: 342.5888512134552
Test time usage: 0.5448169708251953
TOP: Test Loss:   8.9,  Test Acc: 48.13%, Test F1: 44.60%
SEC: Test Loss:   8.9,  Test Acc: 31.34%, Test F1: 18.54%
CONN: Test Loss:   8.9,  Test Acc: 18.66%, Test F1:  1.57%
consistency_top_sec:  8.08%,  consistency_sec_conn:  1.92%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.4769    0.5254    0.5000        59
 Contingency     0.2632    0.3659    0.3061        41
  Comparison     0.6400    0.3019    0.4103        53
   Expansion     0.5537    0.5826    0.5678       115

    accuracy                         0.4813       268
   macro avg     0.4834    0.4439    0.4460       268
weighted avg     0.5094    0.4813    0.4817       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4688    0.5085    0.4878        59
         Temporal.Synchrony     0.2500    0.3659    0.2970        41
          Contingency.Cause     0.1905    0.1905    0.1905        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5156    0.3084    0.3860       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.3925    0.3134    0.3485       268
                  macro avg     0.3041    0.2705    0.2782       268
               weighted avg     0.3742    0.3134    0.3310       268

Epoch [13/30]
top-down:TOP: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.2,  Val Acc: 59.57%, Val F1: 49.25% Time: 73.56428074836731 
top-down:SEC: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 44.03%, Val F1: 27.75% Time: 73.56428074836731 
top-down:CONN: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   7.2,  Val Acc: 24.55%, Val F1:  6.87% Time: 73.56428074836731 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.4,  Val Acc: 60.94%, Val F1: 50.46% Time: 160.8255798816681 
top-down:SEC: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.4,  Val Acc: 43.78%, Val F1: 27.05% Time: 160.8255798816681 
top-down:CONN: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   7.4,  Val Acc: 24.46%, Val F1:  6.64% Time: 160.8255798816681 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 61.03%, Val F1: 50.39% Time: 247.74049019813538 
top-down:SEC: Iter:   5000,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 44.64%, Val F1: 28.99% Time: 247.74049019813538 
top-down:CONN: Iter:   5000,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   7.4,  Val Acc: 24.03%, Val F1:  6.58% Time: 247.74049019813538 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 61.29%, Val F1: 50.94% Time: 334.7313208580017 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   7.3,  Val Acc: 43.86%, Val F1: 26.73% Time: 334.7313208580017 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   7.3,  Val Acc: 24.29%, Val F1:  6.71% Time: 334.7313208580017 
 
 
Train time usage: 343.31793546676636
Test time usage: 0.5407326221466064
TOP: Test Loss:   9.4,  Test Acc: 44.40%, Test F1: 39.66%
SEC: Test Loss:   9.4,  Test Acc: 29.85%, Test F1: 17.84%
CONN: Test Loss:   9.4,  Test Acc: 14.55%, Test F1:  1.34%
consistency_top_sec:  7.51%,  consistency_sec_conn:  1.35%, consistency_top_sec_conn:  1.35%
              precision    recall  f1-score   support

    Temporal     0.4839    0.5085    0.4959        59
 Contingency     0.2340    0.2683    0.2500        41
  Comparison     0.3824    0.2453    0.2989        53
   Expansion     0.5200    0.5652    0.5417       115

    accuracy                         0.4440       268
   macro avg     0.4051    0.3968    0.3966       268
weighted avg     0.4411    0.4440    0.4389       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4833    0.4915    0.4874        59
         Temporal.Synchrony     0.2083    0.2439    0.2247        41
          Contingency.Cause     0.2000    0.3333    0.2500        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5079    0.2991    0.3765       107
      Comparison.Concession     0.2857    0.2500    0.2667         8

                  micro avg     0.3756    0.2985    0.3326       268
                  macro avg     0.2809    0.2696    0.2675       268
               weighted avg     0.3653    0.2985    0.3195       268

Epoch [14/30]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 59.91%, Val F1: 48.79% Time: 79.41610026359558 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.4,  Val Acc: 44.29%, Val F1: 28.48% Time: 79.41610026359558 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   7.4,  Val Acc: 23.95%, Val F1:  6.59% Time: 79.41610026359558 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.4,  Val Acc: 59.66%, Val F1: 48.67% Time: 166.43852496147156 
top-down:SEC: Iter:   5300,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   7.4,  Val Acc: 44.55%, Val F1: 27.34% Time: 166.43852496147156 
top-down:CONN: Iter:   5300,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   7.4,  Val Acc: 24.72%, Val F1:  6.27% Time: 166.43852496147156 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 100.00%,Val Loss:   7.5,  Val Acc: 59.66%, Val F1: 48.29% Time: 253.410964012146 
top-down:SEC: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   7.5,  Val Acc: 44.38%, Val F1: 28.46% Time: 253.410964012146 
top-down:CONN: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   7.5,  Val Acc: 23.43%, Val F1:  6.79% Time: 253.410964012146 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   7.5,  Val Acc: 59.06%, Val F1: 49.15% Time: 340.5737555027008 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 90.62%,Val Loss:   7.5,  Val Acc: 43.09%, Val F1: 26.56% Time: 340.5737555027008 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 43.75%,Val Loss:   7.5,  Val Acc: 23.35%, Val F1:  6.50% Time: 340.5737555027008 
 
 
Train time usage: 343.22271609306335
Test time usage: 0.5450568199157715
TOP: Test Loss:   9.5,  Test Acc: 46.64%, Test F1: 42.37%
SEC: Test Loss:   9.5,  Test Acc: 31.34%, Test F1: 17.00%
CONN: Test Loss:   9.5,  Test Acc: 19.78%, Test F1:  1.74%
consistency_top_sec:  7.70%,  consistency_sec_conn:  2.21%, consistency_top_sec_conn:  2.12%
              precision    recall  f1-score   support

    Temporal     0.5085    0.5085    0.5085        59
 Contingency     0.2642    0.3415    0.2979        41
  Comparison     0.5417    0.2453    0.3377        53
   Expansion     0.5152    0.5913    0.5506       115

    accuracy                         0.4664       268
   macro avg     0.4574    0.4216    0.4237       268
weighted avg     0.4805    0.4664    0.4606       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4576    0.4576    0.4576        59
         Temporal.Synchrony     0.2500    0.3659    0.2970        41
          Contingency.Cause     0.1579    0.1429    0.1500        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4800    0.3364    0.3956       107
      Comparison.Concession     0.4286    0.3750    0.4000         8

                  micro avg     0.3818    0.3134    0.3443       268
                  macro avg     0.2957    0.2796    0.2834       268
               weighted avg     0.3558    0.3134    0.3278       268

Epoch [15/30]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.5,  Val Acc: 59.31%, Val F1: 49.23% Time: 85.18964672088623 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   7.5,  Val Acc: 43.78%, Val F1: 27.22% Time: 85.18964672088623 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   7.5,  Val Acc: 23.95%, Val F1:  6.82% Time: 85.18964672088623 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 60.69%, Val F1: 49.63% Time: 172.0447609424591 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 44.89%, Val F1: 28.87% Time: 172.0447609424591 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   7.7,  Val Acc: 23.61%, Val F1:  6.21% Time: 172.0447609424591 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 81.25%,Val Loss:   7.5,  Val Acc: 60.52%, Val F1: 49.79% Time: 258.9505922794342 
top-down:SEC: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   7.5,  Val Acc: 46.01%, Val F1: 28.62% Time: 258.9505922794342 
top-down:CONN: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 53.12%,Val Loss:   7.5,  Val Acc: 23.86%, Val F1:  6.71% Time: 258.9505922794342 
 
 
Train time usage: 340.3962502479553
Test time usage: 0.543907642364502
TOP: Test Loss:   9.9,  Test Acc: 45.52%, Test F1: 40.77%
SEC: Test Loss:   9.9,  Test Acc: 31.72%, Test F1: 16.83%
CONN: Test Loss:   9.9,  Test Acc: 16.04%, Test F1:  1.32%
consistency_top_sec:  7.99%,  consistency_sec_conn:  1.73%, consistency_top_sec_conn:  1.73%
              precision    recall  f1-score   support

    Temporal     0.5091    0.4746    0.4912        59
 Contingency     0.2459    0.3659    0.2941        41
  Comparison     0.4583    0.2075    0.2857        53
   Expansion     0.5312    0.5913    0.5597       115

    accuracy                         0.4552       268
   macro avg     0.4361    0.4098    0.4077       268
weighted avg     0.4683    0.4552    0.4498       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4815    0.4407    0.4602        59
         Temporal.Synchrony     0.2462    0.3902    0.3019        41
          Contingency.Cause     0.1905    0.1905    0.1905        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5441    0.3458    0.4229       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.3991    0.3172    0.3534       268
                  macro avg     0.3104    0.2695    0.2805       268
               weighted avg     0.3878    0.3172    0.3404       268

Epoch [16/30]
top-down:TOP: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 60.00%, Val F1: 49.12% Time: 6.51336669921875 
top-down:SEC: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.6,  Val Acc: 43.18%, Val F1: 26.72% Time: 6.51336669921875 
top-down:CONN: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   7.6,  Val Acc: 22.58%, Val F1:  6.41% Time: 6.51336669921875 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 60.52%, Val F1: 49.61% Time: 93.34883499145508 
top-down:SEC: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 45.24%, Val F1: 27.85% Time: 93.34883499145508 
top-down:CONN: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   7.8,  Val Acc: 23.86%, Val F1:  6.89% Time: 93.34883499145508 
 
 
top-down:TOP: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 60.34%, Val F1: 48.41% Time: 180.22334051132202 
top-down:SEC: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.7,  Val Acc: 44.38%, Val F1: 27.27% Time: 180.22334051132202 
top-down:CONN: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 56.25%,Val Loss:   7.7,  Val Acc: 23.69%, Val F1:  6.54% Time: 180.22334051132202 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 58.88%, Val F1: 48.55% Time: 267.1878321170807 
top-down:SEC: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   7.6,  Val Acc: 44.89%, Val F1: 28.65% Time: 267.1878321170807 
top-down:CONN: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   7.6,  Val Acc: 22.23%, Val F1:  6.65% Time: 267.1878321170807 
 
 
Train time usage: 342.5395350456238
Test time usage: 0.5447204113006592
TOP: Test Loss:   9.6,  Test Acc: 46.64%, Test F1: 42.25%
SEC: Test Loss:   9.6,  Test Acc: 32.46%, Test F1: 16.76%
CONN: Test Loss:   9.6,  Test Acc: 20.15%, Test F1:  1.60%
consistency_top_sec:  8.08%,  consistency_sec_conn:  2.12%, consistency_top_sec_conn:  2.02%
              precision    recall  f1-score   support

    Temporal     0.5000    0.4576    0.4779        59
 Contingency     0.2667    0.3902    0.3168        41
  Comparison     0.4815    0.2453    0.3250        53
   Expansion     0.5433    0.6000    0.5702       115

    accuracy                         0.4664       268
   macro avg     0.4479    0.4233    0.4225       268
weighted avg     0.4792    0.4664    0.4626       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4906    0.4407    0.4643        59
         Temporal.Synchrony     0.2712    0.3902    0.3200        41
          Contingency.Cause     0.1667    0.1905    0.1778        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5200    0.3645    0.4286       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4009    0.3246    0.3588       268
                  macro avg     0.2970    0.2726    0.2794       268
               weighted avg     0.3801    0.3246    0.3447       268

Epoch [17/30]
top-down:TOP: Iter:   6300,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 60.77%, Val F1: 51.21% Time: 12.452388048171997 
top-down:SEC: Iter:   6300,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 45.32%, Val F1: 27.83% Time: 12.452388048171997 
top-down:CONN: Iter:   6300,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 23.86%, Val F1:  6.64% Time: 12.452388048171997 
 
 
top-down:TOP: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 61.63%, Val F1: 51.24% Time: 99.37756752967834 
top-down:SEC: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 81.25%,Val Loss:   7.7,  Val Acc: 43.69%, Val F1: 27.41% Time: 99.37756752967834 
top-down:CONN: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 22.92%, Val F1:  6.81% Time: 99.37756752967834 
 
 
top-down:TOP: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 59.57%, Val F1: 50.06% Time: 186.20497107505798 
top-down:SEC: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   7.9,  Val Acc: 43.09%, Val F1: 26.64% Time: 186.20497107505798 
top-down:CONN: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   7.9,  Val Acc: 22.75%, Val F1:  6.43% Time: 186.20497107505798 
 
 
top-down:TOP: Iter:   6600,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 59.23%, Val F1: 50.26% Time: 273.1146309375763 
top-down:SEC: Iter:   6600,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   7.9,  Val Acc: 43.35%, Val F1: 26.59% Time: 273.1146309375763 
top-down:CONN: Iter:   6600,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   7.9,  Val Acc: 23.18%, Val F1:  6.54% Time: 273.1146309375763 
 
 
Train time usage: 342.7466821670532
Test time usage: 0.5470857620239258
TOP: Test Loss: 1e+01,  Test Acc: 43.66%, Test F1: 40.79%
SEC: Test Loss: 1e+01,  Test Acc: 32.09%, Test F1: 20.30%
CONN: Test Loss: 1e+01,  Test Acc: 18.66%, Test F1:  1.43%
consistency_top_sec:  8.08%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.83%
              precision    recall  f1-score   support

    Temporal     0.5455    0.4068    0.4660        59
 Contingency     0.2500    0.4390    0.3186        41
  Comparison     0.4242    0.2642    0.3256        53
   Expansion     0.5126    0.5304    0.5214       115

    accuracy                         0.4366       268
   macro avg     0.4331    0.4101    0.4079       268
weighted avg     0.4622    0.4366    0.4394       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5217    0.4068    0.4571        59
         Temporal.Synchrony     0.2254    0.3902    0.2857        41
          Contingency.Cause     0.2069    0.2857    0.2400        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5211    0.3458    0.4157       107
      Comparison.Concession     0.5000    0.3750    0.4286         8

                  micro avg     0.3857    0.3209    0.3503       268
                  macro avg     0.3292    0.3006    0.3045       268
               weighted avg     0.3885    0.3209    0.3419       268

Epoch [18/30]
top-down:TOP: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 60.17%, Val F1: 50.13% Time: 18.34404492378235 
top-down:SEC: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 44.38%, Val F1: 28.86% Time: 18.34404492378235 
top-down:CONN: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 53.12%,Val Loss:   7.8,  Val Acc: 22.58%, Val F1:  6.83% Time: 18.34404492378235 
 
 
top-down:TOP: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 59.23%, Val F1: 49.56% Time: 105.43519592285156 
top-down:SEC: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 45.32%, Val F1: 27.99% Time: 105.43519592285156 
top-down:CONN: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   7.8,  Val Acc: 23.69%, Val F1:  6.59% Time: 105.43519592285156 
 
 
top-down:TOP: Iter:   6900,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 58.71%, Val F1: 48.45% Time: 192.53505873680115 
top-down:SEC: Iter:   6900,  Train Loss: 2.2e+01,  Train Acc: 81.25%,Val Loss:   7.9,  Val Acc: 43.43%, Val F1: 27.17% Time: 192.53505873680115 
top-down:CONN: Iter:   6900,  Train Loss: 2.2e+01,  Train Acc: 50.00%,Val Loss:   7.9,  Val Acc: 22.58%, Val F1:  6.55% Time: 192.53505873680115 
 
 
top-down:TOP: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.1,  Val Acc: 59.14%, Val F1: 47.74% Time: 279.69474744796753 
top-down:SEC: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   8.1,  Val Acc: 41.80%, Val F1: 26.13% Time: 279.69474744796753 
top-down:CONN: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   8.1,  Val Acc: 21.97%, Val F1:  6.63% Time: 279.69474744796753 
 
 
Train time usage: 343.4063575267792
Test time usage: 0.5445868968963623
TOP: Test Loss: 1e+01,  Test Acc: 45.90%, Test F1: 42.07%
SEC: Test Loss: 1e+01,  Test Acc: 31.72%, Test F1: 16.66%
CONN: Test Loss: 1e+01,  Test Acc: 17.54%, Test F1:  1.36%
consistency_top_sec:  7.99%,  consistency_sec_conn:  1.64%, consistency_top_sec_conn:  1.64%
              precision    recall  f1-score   support

    Temporal     0.5417    0.4407    0.4860        59
 Contingency     0.2838    0.5122    0.3652        41
  Comparison     0.4231    0.2075    0.2785        53
   Expansion     0.5417    0.5652    0.5532       115

    accuracy                         0.4590       268
   macro avg     0.4475    0.4314    0.4207       268
weighted avg     0.4788    0.4590    0.4553       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4237    0.4587        59
         Temporal.Synchrony     0.2632    0.4878    0.3419        41
          Contingency.Cause     0.1667    0.1905    0.1778        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5484    0.3178    0.4024       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.3899    0.3172    0.3498       268
                  macro avg     0.3019    0.2783    0.2777       268
               weighted avg     0.3923    0.3172    0.3364       268

Epoch [19/30]
top-down:TOP: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 58.11%, Val F1: 48.41% Time: 24.26116108894348 
top-down:SEC: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 43.35%, Val F1: 27.91% Time: 24.26116108894348 
top-down:CONN: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   8.0,  Val Acc: 23.26%, Val F1:  7.01% Time: 24.26116108894348 
 
 
top-down:TOP: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 61.46%, Val F1: 51.11% Time: 111.02755999565125 
top-down:SEC: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 81.25%,Val Loss:   8.0,  Val Acc: 43.09%, Val F1: 27.07% Time: 111.02755999565125 
top-down:CONN: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 59.38%,Val Loss:   8.0,  Val Acc: 23.61%, Val F1:  6.74% Time: 111.02755999565125 
 
 
top-down:TOP: Iter:   7300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.03%, Val F1: 50.76% Time: 197.83070421218872 
top-down:SEC: Iter:   7300,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 43.18%, Val F1: 27.15% Time: 197.83070421218872 
top-down:CONN: Iter:   7300,  Train Loss: 2.5e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 23.26%, Val F1:  6.46% Time: 197.83070421218872 
 
 
top-down:TOP: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 60.00%, Val F1: 48.83% Time: 284.93226408958435 
top-down:SEC: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   8.1,  Val Acc: 42.06%, Val F1: 27.52% Time: 284.93226408958435 
top-down:CONN: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   8.1,  Val Acc: 22.49%, Val F1:  6.77% Time: 284.93226408958435 
 
 
Train time usage: 342.94668889045715
Test time usage: 0.545107364654541
TOP: Test Loss: 1.1e+01,  Test Acc: 46.27%, Test F1: 43.15%
SEC: Test Loss: 1.1e+01,  Test Acc: 32.46%, Test F1: 17.96%
CONN: Test Loss: 1.1e+01,  Test Acc: 15.67%, Test F1:  1.29%
consistency_top_sec:  8.28%,  consistency_sec_conn:  1.44%, consistency_top_sec_conn:  1.44%
              precision    recall  f1-score   support

    Temporal     0.5385    0.4746    0.5045        59
 Contingency     0.2687    0.4390    0.3333        41
  Comparison     0.5000    0.2642    0.3457        53
   Expansion     0.5289    0.5565    0.5424       115

    accuracy                         0.4627       268
   macro avg     0.4590    0.4336    0.4315       268
weighted avg     0.4855    0.4627    0.4632       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4407    0.4685        59
         Temporal.Synchrony     0.2609    0.4390    0.3273        41
          Contingency.Cause     0.2174    0.2381    0.2273        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5303    0.3271    0.4046       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.4028    0.3246    0.3595       268
                  macro avg     0.4848    0.2877    0.2993       268
               weighted avg     0.5101    0.3246    0.3490       268

Epoch [20/30]
top-down:TOP: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 60.00%, Val F1: 50.10% Time: 30.251506805419922 
top-down:SEC: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 41.12%, Val F1: 25.83% Time: 30.251506805419922 
top-down:CONN: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   8.2,  Val Acc: 22.66%, Val F1:  6.76% Time: 30.251506805419922 
 
 
top-down:TOP: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 59.74%, Val F1: 48.66% Time: 117.52521872520447 
top-down:SEC: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   8.2,  Val Acc: 42.75%, Val F1: 26.64% Time: 117.52521872520447 
top-down:CONN: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   8.2,  Val Acc: 22.49%, Val F1:  6.88% Time: 117.52521872520447 
 
 
top-down:TOP: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.29%, Val F1: 50.79% Time: 204.6320939064026 
top-down:SEC: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 43.35%, Val F1: 26.60% Time: 204.6320939064026 
top-down:CONN: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   8.0,  Val Acc: 23.86%, Val F1:  6.76% Time: 204.6320939064026 
 
 
top-down:TOP: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 59.57%, Val F1: 49.81% Time: 291.53908252716064 
top-down:SEC: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 84.38%,Val Loss:   8.1,  Val Acc: 43.52%, Val F1: 28.69% Time: 291.53908252716064 
top-down:CONN: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 59.38%,Val Loss:   8.1,  Val Acc: 23.35%, Val F1:  7.15% Time: 291.53908252716064 
 
 
Train time usage: 343.7368743419647
Test time usage: 0.538783073425293
TOP: Test Loss: 1e+01,  Test Acc: 45.15%, Test F1: 41.46%
SEC: Test Loss: 1e+01,  Test Acc: 31.34%, Test F1: 16.85%
CONN: Test Loss: 1e+01,  Test Acc: 14.93%, Test F1:  1.24%
consistency_top_sec:  7.89%,  consistency_sec_conn:  1.35%, consistency_top_sec_conn:  1.35%
              precision    recall  f1-score   support

    Temporal     0.5510    0.4576    0.5000        59
 Contingency     0.2639    0.4634    0.3363        41
  Comparison     0.4400    0.2075    0.2821        53
   Expansion     0.5246    0.5565    0.5401       115

    accuracy                         0.4515       268
   macro avg     0.4449    0.4213    0.4146       268
weighted avg     0.4738    0.4515    0.4491       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5102    0.4237    0.4630        59
         Temporal.Synchrony     0.2603    0.4634    0.3333        41
          Contingency.Cause     0.1818    0.1905    0.1860        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5231    0.3178    0.3953       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.3925    0.3134    0.3485       268
                  macro avg     0.3126    0.2742    0.2809       268
               weighted avg     0.3872    0.3134    0.3345       268

Epoch [21/30]
top-down:TOP: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   8.1,  Val Acc: 61.12%, Val F1: 49.78% Time: 36.21820330619812 
top-down:SEC: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   8.1,  Val Acc: 43.78%, Val F1: 27.40% Time: 36.21820330619812 
top-down:CONN: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 71.88%,Val Loss:   8.1,  Val Acc: 22.75%, Val F1:  6.88% Time: 36.21820330619812 
 
 
top-down:TOP: Iter:   8000,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 60.86%, Val F1: 50.81% Time: 123.55251979827881 
top-down:SEC: Iter:   8000,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   8.2,  Val Acc: 43.52%, Val F1: 27.86% Time: 123.55251979827881 
top-down:CONN: Iter:   8000,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   8.2,  Val Acc: 23.00%, Val F1:  6.89% Time: 123.55251979827881 
 
 
top-down:TOP: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   8.1,  Val Acc: 61.03%, Val F1: 50.24% Time: 210.84113192558289 
top-down:SEC: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   8.1,  Val Acc: 43.95%, Val F1: 26.83% Time: 210.84113192558289 
top-down:CONN: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   8.1,  Val Acc: 22.83%, Val F1:  6.92% Time: 210.84113192558289 
 
 
top-down:TOP: Iter:   8200,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 61.55%, Val F1: 51.36% Time: 298.27799558639526 
top-down:SEC: Iter:   8200,  Train Loss: 3.2e+01,  Train Acc: 84.38%,Val Loss:   8.2,  Val Acc: 42.66%, Val F1: 26.79% Time: 298.27799558639526 
top-down:CONN: Iter:   8200,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   8.2,  Val Acc: 22.40%, Val F1:  6.73% Time: 298.27799558639526 
 
 
Train time usage: 344.43630146980286
Test time usage: 0.5447123050689697
TOP: Test Loss: 1.1e+01,  Test Acc: 48.13%, Test F1: 43.06%
SEC: Test Loss: 1.1e+01,  Test Acc: 32.84%, Test F1: 17.85%
CONN: Test Loss: 1.1e+01,  Test Acc: 15.30%, Test F1:  1.11%
consistency_top_sec:  8.08%,  consistency_sec_conn:  1.25%, consistency_top_sec_conn:  1.25%
              precision    recall  f1-score   support

    Temporal     0.5439    0.5254    0.5345        59
 Contingency     0.2545    0.3415    0.2917        41
  Comparison     0.5217    0.2264    0.3158        53
   Expansion     0.5414    0.6261    0.5806       115

    accuracy                         0.4813       268
   macro avg     0.4654    0.4298    0.4306       268
weighted avg     0.4941    0.4813    0.4739       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5185    0.4746    0.4956        59
         Temporal.Synchrony     0.2857    0.3902    0.3299        41
          Contingency.Cause     0.2000    0.1905    0.1951        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5286    0.3458    0.4181       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4251    0.3284    0.3705       268
                  macro avg     0.4777    0.2804    0.2975       268
               weighted avg     0.5139    0.3284    0.3575       268

Epoch [22/30]
top-down:TOP: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 60.69%, Val F1: 49.81% Time: 42.215502977371216 
top-down:SEC: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   8.2,  Val Acc: 43.26%, Val F1: 27.37% Time: 42.215502977371216 
top-down:CONN: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   8.2,  Val Acc: 23.52%, Val F1:  6.84% Time: 42.215502977371216 
 
 
top-down:TOP: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 60.77%, Val F1: 49.64% Time: 129.53827142715454 
top-down:SEC: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   8.2,  Val Acc: 42.92%, Val F1: 27.29% Time: 129.53827142715454 
top-down:CONN: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   8.2,  Val Acc: 22.66%, Val F1:  6.68% Time: 129.53827142715454 
 
 
top-down:TOP: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 61.37%, Val F1: 49.56% Time: 216.6735861301422 
top-down:SEC: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   8.4,  Val Acc: 42.58%, Val F1: 28.25% Time: 216.6735861301422 
top-down:CONN: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   8.4,  Val Acc: 21.97%, Val F1:  6.40% Time: 216.6735861301422 
 
 
top-down:TOP: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   8.3,  Val Acc: 57.94%, Val F1: 48.34% Time: 303.6666338443756 
top-down:SEC: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.3,  Val Acc: 42.23%, Val F1: 27.43% Time: 303.6666338443756 
top-down:CONN: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   8.3,  Val Acc: 22.06%, Val F1:  6.81% Time: 303.6666338443756 
 
 
Train time usage: 343.6595287322998
Test time usage: 0.5436954498291016
TOP: Test Loss: 1e+01,  Test Acc: 47.39%, Test F1: 43.42%
SEC: Test Loss: 1e+01,  Test Acc: 33.96%, Test F1: 18.93%
CONN: Test Loss: 1e+01,  Test Acc: 15.67%, Test F1:  1.08%
consistency_top_sec:  8.47%,  consistency_sec_conn:  1.25%, consistency_top_sec_conn:  1.25%
              precision    recall  f1-score   support

    Temporal     0.5536    0.5254    0.5391        59
 Contingency     0.2712    0.3902    0.3200        41
  Comparison     0.4483    0.2453    0.3171        53
   Expansion     0.5403    0.5826    0.5607       115

    accuracy                         0.4739       268
   macro avg     0.4533    0.4359    0.4342       268
weighted avg     0.4839    0.4739    0.4709       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5179    0.4915    0.5043        59
         Temporal.Synchrony     0.2982    0.4146    0.3469        41
          Contingency.Cause     0.2083    0.2381    0.2222        21
Contingency.Pragmatic cause     1.0000    0.0625    0.1176        32
        Comparison.Contrast     0.5455    0.3364    0.4162       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4313    0.3396    0.3800       268
                  macro avg     0.4839    0.2989    0.3155       268
               weighted avg     0.5231    0.3396    0.3703       268

Epoch [23/30]
top-down:TOP: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.3,  Val Acc: 61.55%, Val F1: 50.88% Time: 48.10249710083008 
top-down:SEC: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.3,  Val Acc: 42.92%, Val F1: 27.30% Time: 48.10249710083008 
top-down:CONN: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   8.3,  Val Acc: 23.18%, Val F1:  6.79% Time: 48.10249710083008 
 
 
top-down:TOP: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.3,  Val Acc: 60.43%, Val F1: 50.03% Time: 135.13188004493713 
top-down:SEC: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   8.3,  Val Acc: 42.66%, Val F1: 28.56% Time: 135.13188004493713 
top-down:CONN: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   8.3,  Val Acc: 23.18%, Val F1:  7.02% Time: 135.13188004493713 
 
 
top-down:TOP: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.3,  Val Acc: 59.91%, Val F1: 47.86% Time: 222.31932497024536 
top-down:SEC: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.3,  Val Acc: 42.40%, Val F1: 28.07% Time: 222.31932497024536 
top-down:CONN: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   8.3,  Val Acc: 23.00%, Val F1:  6.97% Time: 222.31932497024536 
 
 
top-down:TOP: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 61.20%, Val F1: 49.47% Time: 309.34404015541077 
top-down:SEC: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.2,  Val Acc: 42.49%, Val F1: 27.37% Time: 309.34404015541077 
top-down:CONN: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   8.2,  Val Acc: 22.06%, Val F1:  6.44% Time: 309.34404015541077 
 
 
Train time usage: 343.1750566959381
Test time usage: 0.5461804866790771
TOP: Test Loss: 1.1e+01,  Test Acc: 46.64%, Test F1: 41.54%
SEC: Test Loss: 1.1e+01,  Test Acc: 32.46%, Test F1: 18.72%
CONN: Test Loss: 1.1e+01,  Test Acc: 17.91%, Test F1:  1.32%
consistency_top_sec:  7.89%,  consistency_sec_conn:  1.25%, consistency_top_sec_conn:  1.25%
              precision    recall  f1-score   support

    Temporal     0.5400    0.4576    0.4954        59
 Contingency     0.2500    0.3415    0.2887        41
  Comparison     0.4615    0.2264    0.3038        53
   Expansion     0.5294    0.6261    0.5737       115

    accuracy                         0.4664       268
   macro avg     0.4452    0.4129    0.4154       268
weighted avg     0.4756    0.4664    0.4595       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5385    0.4746    0.5045        59
         Temporal.Synchrony     0.2807    0.3902    0.3265        41
          Contingency.Cause     0.2174    0.2381    0.2273        21
Contingency.Pragmatic cause     1.0000    0.0625    0.1176        32
        Comparison.Contrast     0.5000    0.3178    0.3886       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.4203    0.3246    0.3663       268
                  macro avg     0.4894    0.2889    0.3120       268
               weighted avg     0.5095    0.3246    0.3572       268

Epoch [24/30]
top-down:TOP: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 61.20%, Val F1: 49.86% Time: 53.93932771682739 
top-down:SEC: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 90.62%,Val Loss:   8.4,  Val Acc: 43.09%, Val F1: 27.49% Time: 53.93932771682739 
top-down:CONN: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 65.62%,Val Loss:   8.4,  Val Acc: 22.32%, Val F1:  6.70% Time: 53.93932771682739 
 
 
top-down:TOP: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 60.86%, Val F1: 49.63% Time: 141.2936155796051 
top-down:SEC: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 87.50%,Val Loss:   8.4,  Val Acc: 42.58%, Val F1: 28.49% Time: 141.2936155796051 
top-down:CONN: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 62.50%,Val Loss:   8.4,  Val Acc: 22.92%, Val F1:  6.87% Time: 141.2936155796051 
 
 
top-down:TOP: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.3,  Val Acc: 61.55%, Val F1: 50.87% Time: 229.32787799835205 
top-down:SEC: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   8.3,  Val Acc: 42.06%, Val F1: 27.45% Time: 229.32787799835205 
top-down:CONN: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   8.3,  Val Acc: 22.23%, Val F1:  6.81% Time: 229.32787799835205 
 
 
top-down:TOP: Iter:   9400,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.3,  Val Acc: 60.94%, Val F1: 49.84% Time: 316.7769606113434 
top-down:SEC: Iter:   9400,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.3,  Val Acc: 43.00%, Val F1: 27.93% Time: 316.7769606113434 
top-down:CONN: Iter:   9400,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   8.3,  Val Acc: 22.49%, Val F1:  6.58% Time: 316.7769606113434 
 
 
Train time usage: 344.8680715560913
Test time usage: 0.5416641235351562
TOP: Test Loss: 1.1e+01,  Test Acc: 47.39%, Test F1: 42.41%
SEC: Test Loss: 1.1e+01,  Test Acc: 32.46%, Test F1: 17.23%
CONN: Test Loss: 1.1e+01,  Test Acc: 15.67%, Test F1:  1.04%
consistency_top_sec:  8.28%,  consistency_sec_conn:  1.44%, consistency_top_sec_conn:  1.44%
              precision    recall  f1-score   support

    Temporal     0.5345    0.5254    0.5299        59
 Contingency     0.2545    0.3415    0.2917        41
  Comparison     0.4286    0.2264    0.2963        53
   Expansion     0.5512    0.6087    0.5785       115

    accuracy                         0.4739       268
   macro avg     0.4422    0.4255    0.4241       268
weighted avg     0.4779    0.4739    0.4681       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4833    0.4915    0.4874        59
         Temporal.Synchrony     0.2545    0.3415    0.2917        41
          Contingency.Cause     0.1739    0.1905    0.1818        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5211    0.3458    0.4157       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4028    0.3246    0.3595       268
                  macro avg     0.4610    0.2751    0.2872       268
               weighted avg     0.4964    0.3246    0.3479       268

Epoch [25/30]
top-down:TOP: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 60.94%, Val F1: 49.49% Time: 60.13466429710388 
top-down:SEC: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   8.4,  Val Acc: 42.92%, Val F1: 28.42% Time: 60.13466429710388 
top-down:CONN: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   8.4,  Val Acc: 21.89%, Val F1:  6.86% Time: 60.13466429710388 
 
 
top-down:TOP: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 61.55%, Val F1: 51.17% Time: 147.44078660011292 
top-down:SEC: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   8.5,  Val Acc: 42.66%, Val F1: 27.96% Time: 147.44078660011292 
top-down:CONN: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   8.5,  Val Acc: 22.40%, Val F1:  6.71% Time: 147.44078660011292 
 
 
top-down:TOP: Iter:   9700,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 61.37%, Val F1: 49.92% Time: 235.30833435058594 
top-down:SEC: Iter:   9700,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 42.66%, Val F1: 28.44% Time: 235.30833435058594 
top-down:CONN: Iter:   9700,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   8.4,  Val Acc: 22.58%, Val F1:  6.92% Time: 235.30833435058594 
 
 
top-down:TOP: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 60.69%, Val F1: 50.07% Time: 322.64850902557373 
top-down:SEC: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 42.40%, Val F1: 27.57% Time: 322.64850902557373 
top-down:CONN: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 78.12%,Val Loss:   8.5,  Val Acc: 22.49%, Val F1:  6.78% Time: 322.64850902557373 
 
 
Train time usage: 344.856876373291
Test time usage: 0.5473570823669434
TOP: Test Loss: 1.1e+01,  Test Acc: 45.90%, Test F1: 41.23%
SEC: Test Loss: 1.1e+01,  Test Acc: 33.58%, Test F1: 17.83%
CONN: Test Loss: 1.1e+01,  Test Acc: 14.93%, Test F1:  1.04%
consistency_top_sec:  8.37%,  consistency_sec_conn:  1.35%, consistency_top_sec_conn:  1.35%
              precision    recall  f1-score   support

    Temporal     0.5246    0.5424    0.5333        59
 Contingency     0.2414    0.3415    0.2828        41
  Comparison     0.4231    0.2075    0.2785        53
   Expansion     0.5366    0.5739    0.5546       115

    accuracy                         0.4590       268
   macro avg     0.4314    0.4163    0.4123       268
weighted avg     0.4663    0.4590    0.4537       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4839    0.5085    0.4959        59
         Temporal.Synchrony     0.2542    0.3659    0.3000        41
          Contingency.Cause     0.2174    0.2381    0.2273        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5139    0.3458    0.4134       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4036    0.3358    0.3666       268
                  macro avg     0.4671    0.2899    0.2971       268
               weighted avg     0.4970    0.3358    0.3537       268

Epoch [26/30]
top-down:TOP: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 60.69%, Val F1: 49.70% Time: 66.00925397872925 
top-down:SEC: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 43.35%, Val F1: 27.75% Time: 66.00925397872925 
top-down:CONN: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   8.5,  Val Acc: 22.15%, Val F1:  6.85% Time: 66.00925397872925 
 
 
top-down:TOP: Iter:  10000,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 60.60%, Val F1: 49.63% Time: 153.03640222549438 
top-down:SEC: Iter:  10000,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 42.15%, Val F1: 26.94% Time: 153.03640222549438 
top-down:CONN: Iter:  10000,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   8.5,  Val Acc: 22.32%, Val F1:  6.86% Time: 153.03640222549438 
 
 
top-down:TOP: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 60.69%, Val F1: 49.67% Time: 240.28073120117188 
top-down:SEC: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 43.52%, Val F1: 29.05% Time: 240.28073120117188 
top-down:CONN: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   8.4,  Val Acc: 21.97%, Val F1:  6.77% Time: 240.28073120117188 
 
 
top-down:TOP: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 60.94%, Val F1: 49.78% Time: 327.3109438419342 
top-down:SEC: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 43.18%, Val F1: 28.18% Time: 327.3109438419342 
top-down:CONN: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   8.5,  Val Acc: 21.89%, Val F1:  6.57% Time: 327.3109438419342 
 
 
Train time usage: 343.523654460907
Test time usage: 0.5479466915130615
TOP: Test Loss: 1.1e+01,  Test Acc: 47.76%, Test F1: 43.84%
SEC: Test Loss: 1.1e+01,  Test Acc: 33.96%, Test F1: 18.64%
CONN: Test Loss: 1.1e+01,  Test Acc: 16.79%, Test F1:  1.20%
consistency_top_sec:  8.47%,  consistency_sec_conn:  1.44%, consistency_top_sec_conn:  1.35%
              precision    recall  f1-score   support

    Temporal     0.5769    0.5085    0.5405        59
 Contingency     0.2985    0.4878    0.3704        41
  Comparison     0.4400    0.2075    0.2821        53
   Expansion     0.5403    0.5826    0.5607       115

    accuracy                         0.4776       268
   macro avg     0.4639    0.4466    0.4384       268
weighted avg     0.4915    0.4776    0.4720       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5490    0.4746    0.5091        59
         Temporal.Synchrony     0.2687    0.4390    0.3333        41
          Contingency.Cause     0.2273    0.2381    0.2326        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5362    0.3458    0.4205       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.4233    0.3396    0.3768       268
                  macro avg     0.4969    0.2965    0.3106       268
               weighted avg     0.5252    0.3396    0.3656       268

Epoch [27/30]
top-down:TOP: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 60.86%, Val F1: 50.59% Time: 71.79169869422913 
top-down:SEC: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 43.26%, Val F1: 28.06% Time: 71.79169869422913 
top-down:CONN: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   8.4,  Val Acc: 22.06%, Val F1:  6.63% Time: 71.79169869422913 
 
 
top-down:TOP: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 61.46%, Val F1: 50.72% Time: 158.96436738967896 
top-down:SEC: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 43.43%, Val F1: 28.39% Time: 158.96436738967896 
top-down:CONN: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   8.5,  Val Acc: 22.49%, Val F1:  6.81% Time: 158.96436738967896 
 
 
top-down:TOP: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 60.43%, Val F1: 50.06% Time: 246.1853530406952 
top-down:SEC: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 43.26%, Val F1: 28.75% Time: 246.1853530406952 
top-down:CONN: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   8.5,  Val Acc: 21.97%, Val F1:  6.78% Time: 246.1853530406952 
 
 
top-down:TOP: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 60.86%, Val F1: 50.41% Time: 333.46433186531067 
top-down:SEC: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   8.5,  Val Acc: 44.03%, Val F1: 29.08% Time: 333.46433186531067 
top-down:CONN: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 62.50%,Val Loss:   8.5,  Val Acc: 21.97%, Val F1:  6.73% Time: 333.46433186531067 
 
 
Train time usage: 343.70275950431824
Test time usage: 0.5459957122802734
TOP: Test Loss: 1.1e+01,  Test Acc: 46.64%, Test F1: 43.44%
SEC: Test Loss: 1.1e+01,  Test Acc: 33.58%, Test F1: 18.52%
CONN: Test Loss: 1.1e+01,  Test Acc: 14.55%, Test F1:  1.06%
consistency_top_sec:  8.57%,  consistency_sec_conn:  1.15%, consistency_top_sec_conn:  1.15%
              precision    recall  f1-score   support

    Temporal     0.5660    0.5085    0.5357        59
 Contingency     0.2941    0.4878    0.3670        41
  Comparison     0.4286    0.2264    0.2963        53
   Expansion     0.5294    0.5478    0.5385       115

    accuracy                         0.4664       268
   macro avg     0.4545    0.4426    0.4344       268
weighted avg     0.4815    0.4664    0.4637       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5385    0.4746    0.5045        59
         Temporal.Synchrony     0.2879    0.4634    0.3551        41
          Contingency.Cause     0.2083    0.2381    0.2222        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5224    0.3271    0.4023       107
      Comparison.Concession     0.4000    0.2500    0.3077         8

                  micro avg     0.4186    0.3358    0.3727       268
                  macro avg     0.4928    0.2974    0.3087       268
               weighted avg     0.5188    0.3358    0.3599       268

Epoch [28/30]
top-down:TOP: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 61.29%, Val F1: 49.94% Time: 77.62906384468079 
top-down:SEC: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 43.95%, Val F1: 29.10% Time: 77.62906384468079 
top-down:CONN: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   8.5,  Val Acc: 21.80%, Val F1:  6.78% Time: 77.62906384468079 
 
 
top-down:TOP: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 60.69%, Val F1: 50.30% Time: 164.73952651023865 
top-down:SEC: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 43.86%, Val F1: 28.42% Time: 164.73952651023865 
top-down:CONN: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   8.5,  Val Acc: 22.15%, Val F1:  6.73% Time: 164.73952651023865 
 
 
top-down:TOP: Iter:  10900,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 61.63%, Val F1: 50.91% Time: 251.92716479301453 
top-down:SEC: Iter:  10900,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 43.43%, Val F1: 28.21% Time: 251.92716479301453 
top-down:CONN: Iter:  10900,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   8.5,  Val Acc: 22.06%, Val F1:  6.79% Time: 251.92716479301453 
 
 
top-down:TOP: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 60.94%, Val F1: 50.68% Time: 339.2179684638977 
top-down:SEC: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 42.92%, Val F1: 28.52% Time: 339.2179684638977 
top-down:CONN: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 81.25%,Val Loss:   8.5,  Val Acc: 21.89%, Val F1:  6.61% Time: 339.2179684638977 
 
 
Train time usage: 343.5118553638458
Test time usage: 0.5506062507629395
TOP: Test Loss: 1.1e+01,  Test Acc: 47.76%, Test F1: 43.77%
SEC: Test Loss: 1.1e+01,  Test Acc: 32.46%, Test F1: 17.71%
CONN: Test Loss: 1.1e+01,  Test Acc: 13.81%, Test F1:  0.97%
consistency_top_sec:  8.37%,  consistency_sec_conn:  1.15%, consistency_top_sec_conn:  1.15%
              precision    recall  f1-score   support

    Temporal     0.5536    0.5254    0.5391        59
 Contingency     0.2857    0.4390    0.3462        41
  Comparison     0.4444    0.2264    0.3000        53
   Expansion     0.5492    0.5826    0.5654       115

    accuracy                         0.4776       268
   macro avg     0.4582    0.4434    0.4377       268
weighted avg     0.4891    0.4776    0.4736       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4915    0.4957        59
         Temporal.Synchrony     0.2500    0.3659    0.2970        41
          Contingency.Cause     0.2174    0.2381    0.2273        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5303    0.3271    0.4046       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4065    0.3246    0.3610       268
                  macro avg     0.4718    0.2840    0.2952       268
               weighted avg     0.5064    0.3246    0.3497       268

Epoch [29/30]
top-down:TOP: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 61.03%, Val F1: 50.39% Time: 83.70810341835022 
top-down:SEC: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 44.12%, Val F1: 29.23% Time: 83.70810341835022 
top-down:CONN: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 81.25%,Val Loss:   8.5,  Val Acc: 22.06%, Val F1:  6.74% Time: 83.70810341835022 
 
 
top-down:TOP: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 60.69%, Val F1: 50.28% Time: 170.87267017364502 
top-down:SEC: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 43.95%, Val F1: 28.71% Time: 170.87267017364502 
top-down:CONN: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   8.5,  Val Acc: 22.49%, Val F1:  6.76% Time: 170.87267017364502 
 
 
top-down:TOP: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 61.29%, Val F1: 50.39% Time: 257.9139425754547 
top-down:SEC: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 43.78%, Val F1: 29.06% Time: 257.9139425754547 
top-down:CONN: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   8.5,  Val Acc: 22.15%, Val F1:  6.77% Time: 257.9139425754547 
 
 
Train time usage: 341.33198642730713
Test time usage: 0.558408260345459
TOP: Test Loss: 1.1e+01,  Test Acc: 47.39%, Test F1: 43.57%
SEC: Test Loss: 1.1e+01,  Test Acc: 33.96%, Test F1: 18.42%
CONN: Test Loss: 1.1e+01,  Test Acc: 15.30%, Test F1:  1.06%
consistency_top_sec:  8.76%,  consistency_sec_conn:  1.25%, consistency_top_sec_conn:  1.25%
              precision    recall  f1-score   support

    Temporal     0.5536    0.5254    0.5391        59
 Contingency     0.2812    0.4390    0.3429        41
  Comparison     0.4615    0.2264    0.3038        53
   Expansion     0.5410    0.5739    0.5570       115

    accuracy                         0.4739       268
   macro avg     0.4593    0.4412    0.4357       268
weighted avg     0.4883    0.4739    0.4702       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5370    0.4915    0.5133        59
         Temporal.Synchrony     0.2698    0.4146    0.3269        41
          Contingency.Cause     0.2273    0.2381    0.2326        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5441    0.3458    0.4229       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4252    0.3396    0.3776       268
                  macro avg     0.4853    0.2952    0.3070       268
               weighted avg     0.5239    0.3396    0.3658       268

Epoch [30/30]
top-down:TOP: Iter:  11400,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 61.29%, Val F1: 50.67% Time: 4.849038124084473 
top-down:SEC: Iter:  11400,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   8.5,  Val Acc: 43.43%, Val F1: 28.80% Time: 4.849038124084473 
top-down:CONN: Iter:  11400,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   8.5,  Val Acc: 21.97%, Val F1:  6.66% Time: 4.849038124084473 
 
 
top-down:TOP: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.6,  Val Acc: 61.37%, Val F1: 50.50% Time: 92.00125861167908 
top-down:SEC: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 43.43%, Val F1: 28.84% Time: 92.00125861167908 
top-down:CONN: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   8.6,  Val Acc: 22.06%, Val F1:  6.64% Time: 92.00125861167908 
 
 
top-down:TOP: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 61.37%, Val F1: 50.88% Time: 178.98814368247986 
top-down:SEC: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 43.78%, Val F1: 29.09% Time: 178.98814368247986 
top-down:CONN: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 62.50%,Val Loss:   8.5,  Val Acc: 21.89%, Val F1:  6.47% Time: 178.98814368247986 
 
 
top-down:TOP: Iter:  11700,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 61.20%, Val F1: 50.25% Time: 266.4433915615082 
top-down:SEC: Iter:  11700,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.6,  Val Acc: 43.69%, Val F1: 28.95% Time: 266.4433915615082 
top-down:CONN: Iter:  11700,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   8.6,  Val Acc: 22.06%, Val F1:  6.65% Time: 266.4433915615082 
 
 
Train time usage: 344.23326325416565
Test time usage: 0.5525197982788086
TOP: Test Loss: 1.1e+01,  Test Acc: 47.01%, Test F1: 43.15%
SEC: Test Loss: 1.1e+01,  Test Acc: 32.84%, Test F1: 18.11%
CONN: Test Loss: 1.1e+01,  Test Acc: 13.81%, Test F1:  0.97%
consistency_top_sec:  8.47%,  consistency_sec_conn:  1.06%, consistency_top_sec_conn:  1.06%
              precision    recall  f1-score   support

    Temporal     0.5536    0.5254    0.5391        59
 Contingency     0.2698    0.4146    0.3269        41
  Comparison     0.4800    0.2264    0.3077        53
   Expansion     0.5323    0.5739    0.5523       115

    accuracy                         0.4701       268
   macro avg     0.4589    0.4351    0.4315       268
weighted avg     0.4865    0.4701    0.4665       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5179    0.4915    0.5043        59
         Temporal.Synchrony     0.2581    0.3902    0.3107        41
          Contingency.Cause     0.2381    0.2381    0.2381        21
Contingency.Pragmatic cause     1.0000    0.0312    0.0606        32
        Comparison.Contrast     0.5556    0.3271    0.4118       107
      Comparison.Concession     0.3333    0.2500    0.2857         8

                  micro avg     0.4211    0.3284    0.3690       268
                  macro avg     0.4838    0.2880    0.3019       268
               weighted avg     0.5233    0.3284    0.3574       268

dev_best_acc_top: 63.00%,  dev_best_f1_top: 50.60%, 
dev_best_acc_sec: 48.33%,  dev_best_f1_sec: 28.42%, 
dev_best_acc_conn: 27.04%,  dev_best_f1_conn:  6.34%
