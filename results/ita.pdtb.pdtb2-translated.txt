nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_it/data/', 'log_file': 'data/pdtb_it/log/', 'save_file': 'data/pdtb_it/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 30, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March07-00:13:11', 'log': 'data/pdtb_it/log/March07-00:13:11.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]6it [00:00, 59.42it/s]105it [00:00, 603.47it/s]170it [00:00, 621.93it/s]233it [00:00, 581.27it/s]292it [00:00, 583.90it/s]363it [00:00, 594.71it/s]423it [00:00, 595.45it/s]506it [00:00, 666.58it/s]573it [00:00, 647.01it/s]691it [00:01, 804.59it/s]774it [00:01, 811.77it/s]856it [00:01, 784.87it/s]968it [00:01, 873.79it/s]1090it [00:01, 972.81it/s]1196it [00:01, 997.69it/s]1311it [00:01, 1040.81it/s]1416it [00:01, 1034.16it/s]1520it [00:01, 1010.84it/s]1622it [00:02, 932.36it/s] 1735it [00:02, 985.90it/s]1835it [00:02, 954.11it/s]1950it [00:02, 1008.04it/s]2057it [00:02, 1024.16it/s]2161it [00:02, 849.83it/s] 2252it [00:02, 839.33it/s]2340it [00:02, 709.53it/s]2434it [00:02, 760.64it/s]2516it [00:03, 722.41it/s]2592it [00:03, 708.92it/s]2686it [00:03, 767.89it/s]2796it [00:03, 839.05it/s]2883it [00:03, 712.79it/s]2986it [00:03, 789.20it/s]3089it [00:03, 850.39it/s]3179it [00:03, 764.00it/s]3277it [00:04, 818.62it/s]3387it [00:04, 893.57it/s]3481it [00:04, 875.78it/s]3572it [00:04, 778.80it/s]3702it [00:04, 912.47it/s]3798it [00:04, 905.44it/s]3892it [00:04, 855.91it/s]3996it [00:04, 893.97it/s]4088it [00:04, 897.10it/s]4180it [00:05, 797.61it/s]4264it [00:05, 808.02it/s]4372it [00:05, 881.78it/s]4463it [00:05, 847.30it/s]4550it [00:05, 681.62it/s]4625it [00:05, 679.05it/s]4704it [00:05, 706.70it/s]4780it [00:05, 720.00it/s]4858it [00:06, 735.97it/s]4939it [00:06, 739.99it/s]5015it [00:06, 656.81it/s]5084it [00:06, 605.20it/s]5147it [00:06, 579.17it/s]5207it [00:06, 314.98it/s]5287it [00:07, 394.35it/s]5343it [00:07, 396.87it/s]5420it [00:07, 472.17it/s]5541it [00:07, 637.32it/s]5625it [00:07, 681.70it/s]5704it [00:07, 663.61it/s]5801it [00:07, 740.59it/s]5912it [00:07, 837.52it/s]6017it [00:07, 894.92it/s]6117it [00:08, 915.52it/s]6221it [00:08, 948.84it/s]6321it [00:08, 961.53it/s]6424it [00:08, 973.82it/s]6523it [00:08, 831.65it/s]6611it [00:08, 712.44it/s]6712it [00:08, 772.34it/s]6816it [00:08, 838.20it/s]6919it [00:08, 881.12it/s]7028it [00:09, 937.28it/s]7125it [00:09, 935.72it/s]7231it [00:09, 968.51it/s]7341it [00:09, 1005.30it/s]7443it [00:09, 965.08it/s] 7550it [00:09, 994.58it/s]7664it [00:09, 1034.94it/s]7781it [00:09, 1073.14it/s]7909it [00:09, 1116.79it/s]8022it [00:10, 832.75it/s] 8117it [00:10, 624.70it/s]8194it [00:10, 579.22it/s]8262it [00:10, 516.71it/s]8321it [00:10, 522.94it/s]8379it [00:10, 524.44it/s]8485it [00:11, 649.07it/s]8557it [00:11, 651.19it/s]8627it [00:11, 649.69it/s]8717it [00:11, 707.40it/s]8791it [00:11, 644.84it/s]8906it [00:11, 775.42it/s]9000it [00:11, 819.53it/s]9090it [00:11, 832.66it/s]9176it [00:12, 691.65it/s]9251it [00:12, 687.72it/s]9347it [00:12, 741.69it/s]9425it [00:12, 681.17it/s]9513it [00:12, 730.75it/s]9626it [00:12, 821.01it/s]9711it [00:12, 768.08it/s]9830it [00:12, 879.44it/s]9928it [00:12, 906.37it/s]10029it [00:13, 934.48it/s]10134it [00:13, 966.06it/s]10233it [00:13, 953.67it/s]10350it [00:13, 1013.39it/s]10458it [00:13, 1031.66it/s]10562it [00:13, 959.58it/s] 10666it [00:13, 959.33it/s]10780it [00:13, 1008.01it/s]10887it [00:13, 1023.86it/s]11006it [00:13, 1071.38it/s]11114it [00:14, 931.96it/s] 11211it [00:14, 923.45it/s]11313it [00:14, 948.53it/s]11425it [00:14, 995.26it/s]11527it [00:14, 988.40it/s]11642it [00:14, 1032.85it/s]11751it [00:14, 1049.00it/s]11857it [00:14, 1025.54it/s]11961it [00:14, 998.22it/s] 12062it [00:15, 926.85it/s]12156it [00:15, 925.12it/s]12260it [00:15, 957.13it/s]12357it [00:15, 903.43it/s]12449it [00:15, 668.80it/s]12526it [00:15, 624.97it/s]12547it [00:15, 793.66it/s]
0it [00:00, ?it/s]44it [00:00, 434.73it/s]125it [00:00, 654.12it/s]215it [00:00, 738.00it/s]289it [00:00, 629.54it/s]360it [00:00, 653.91it/s]463it [00:00, 770.73it/s]562it [00:00, 837.38it/s]648it [00:00, 837.31it/s]733it [00:00, 760.50it/s]814it [00:01, 772.44it/s]902it [00:01, 795.34it/s]983it [00:01, 702.47it/s]1090it [00:01, 799.49it/s]1165it [00:01, 770.04it/s]
0it [00:00, ?it/s]91it [00:00, 908.62it/s]196it [00:00, 987.73it/s]321it [00:00, 1103.91it/s]435it [00:00, 1117.56it/s]558it [00:00, 1157.09it/s]674it [00:00, 1157.94it/s]790it [00:00, 1133.57it/s]904it [00:00, 1072.16it/s]1018it [00:00, 1091.05it/s]1039it [00:00, 1105.03it/s]
Time usage: 36.63953351974487
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/30]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 55.88%, Val F1: 17.92% Time: 176.80947709083557 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   7.5,  Val Acc: 24.21%, Val F1:  3.54% Time: 176.80947709083557 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  0.00%,Val Loss:   7.5,  Val Acc: 12.88%, Val F1:  0.34% Time: 176.80947709083557 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 55.88%, Val F1: 17.92% Time: 333.3798236846924 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.6,  Val Acc: 28.07%, Val F1:  6.36% Time: 333.3798236846924 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.6,  Val Acc: 12.88%, Val F1:  0.34% Time: 333.3798236846924 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 68.75%,Val Loss:   6.4,  Val Acc: 55.28%, Val F1: 19.79% Time: 487.0463581085205 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 34.38%,Val Loss:   6.4,  Val Acc: 28.76%, Val F1:  8.98% Time: 487.0463581085205 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 15.62%,Val Loss:   6.4,  Val Acc: 15.45%, Val F1:  1.03% Time: 487.0463581085205 *
 
 
Train time usage: 629.3655288219452
Test time usage: 1.824544906616211
TOP: Test Loss:   6.3,  Test Acc: 53.61%, Test F1: 17.62%
SEC: Test Loss:   6.3,  Test Acc: 31.28%, Test F1: 10.61%
CONN: Test Loss:   6.3,  Test Acc: 16.17%, Test F1:  1.24%
consistency_top_sec: 21.56%,  consistency_sec_conn: 10.11%, consistency_top_sec_conn:  5.77%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        68
 Contingency     0.5000    0.0037    0.0074       270
  Comparison     0.0000    0.0000    0.0000       144
   Expansion     0.5362    0.9982    0.6976       557

    accuracy                         0.5361      1039
   macro avg     0.2590    0.2505    0.1762      1039
weighted avg     0.4174    0.5361    0.3759      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4953    0.3918    0.4375       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.0000    0.0000    0.0000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3486    0.4950    0.4091       200
    Expansion.Instantiation     0.0000    0.0000    0.0000       118
      Expansion.Restatement     0.2228    0.5708    0.3205       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.3128      1039
                  macro avg     0.0970    0.1325    0.1061      1039
               weighted avg     0.2403    0.3128    0.2570      1039

Epoch [2/30]
top-down:TOP: Iter:    400,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   6.3,  Val Acc: 55.28%, Val F1: 23.69% Time: 17.070708751678467 *
top-down:SEC: Iter:    400,  Train Loss: 3e+01,  Train Acc: 31.25%,Val Loss:   6.3,  Val Acc: 34.94%, Val F1: 11.43% Time: 17.070708751678467 *
top-down:CONN: Iter:    400,  Train Loss: 3e+01,  Train Acc: 12.50%,Val Loss:   6.3,  Val Acc: 16.31%, Val F1:  1.24% Time: 17.070708751678467 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   6.0,  Val Acc: 57.34%, Val F1: 32.21% Time: 166.60238790512085 *
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   6.0,  Val Acc: 38.80%, Val F1: 14.70% Time: 166.60238790512085 *
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 25.00%,Val Loss:   6.0,  Val Acc: 20.26%, Val F1:  1.90% Time: 166.60238790512085 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   5.8,  Val Acc: 56.48%, Val F1: 40.51% Time: 321.5836639404297 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   5.8,  Val Acc: 42.23%, Val F1: 21.08% Time: 321.5836639404297 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 25.00%,Val Loss:   5.8,  Val Acc: 22.66%, Val F1:  2.98% Time: 321.5836639404297 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 58.37%, Val F1: 41.21% Time: 481.5971963405609 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   5.7,  Val Acc: 42.66%, Val F1: 18.98% Time: 481.5971963405609 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 23.09%, Val F1:  3.06% Time: 481.5971963405609 *
 
 
Train time usage: 613.7921245098114
Test time usage: 1.984712839126587
TOP: Test Loss:   5.2,  Test Acc: 63.04%, Test F1: 52.40%
SEC: Test Loss:   5.2,  Test Acc: 50.34%, Test F1: 27.75%
CONN: Test Loss:   5.2,  Test Acc: 25.89%, Test F1:  4.32%
consistency_top_sec: 45.24%,  consistency_sec_conn: 22.43%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.3947    0.4412    0.4167        68
 Contingency     0.5765    0.4109    0.4798       275
  Comparison     0.6375    0.3542    0.4554       144
   Expansion     0.6710    0.8351    0.7441       552

    accuracy                         0.6304      1039
   macro avg     0.5700    0.5103    0.5240      1039
weighted avg     0.6233    0.6304    0.6127      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3373    0.5185    0.4088        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5248    0.5911    0.5559       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5106    0.3750    0.4324       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4764    0.6050    0.5330       200
    Expansion.Instantiation     0.7091    0.6610    0.6842       118
      Expansion.Restatement     0.4564    0.4218    0.4384       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5034      1039
                  macro avg     0.2741    0.2884    0.2775      1039
               weighted avg     0.4812    0.5034    0.4878      1039

Epoch [3/30]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   5.5,  Val Acc: 58.88%, Val F1: 47.99% Time: 27.300978660583496 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 37.50%,Val Loss:   5.5,  Val Acc: 44.46%, Val F1: 23.79% Time: 27.300978660583496 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 18.75%,Val Loss:   5.5,  Val Acc: 22.58%, Val F1:  3.88% Time: 27.300978660583496 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 61.72%, Val F1: 47.81% Time: 184.82219862937927 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.5,  Val Acc: 46.18%, Val F1: 26.49% Time: 184.82219862937927 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 21.88%,Val Loss:   5.5,  Val Acc: 24.98%, Val F1:  4.47% Time: 184.82219862937927 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.4,  Val Acc: 61.29%, Val F1: 42.61% Time: 335.1823844909668 
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   5.4,  Val Acc: 46.70%, Val F1: 23.76% Time: 335.1823844909668 
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   5.4,  Val Acc: 24.98%, Val F1:  4.49% Time: 335.1823844909668 
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 62.92%, Val F1: 48.93% Time: 496.88180136680603 *
top-down:SEC: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 47.81%, Val F1: 27.80% Time: 496.88180136680603 *
top-down:CONN: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 25.41%, Val F1:  5.00% Time: 496.88180136680603 *
 
 
Train time usage: 617.8324069976807
Test time usage: 1.8187706470489502
TOP: Test Loss:   5.1,  Test Acc: 63.43%, Test F1: 53.59%
SEC: Test Loss:   5.1,  Test Acc: 49.57%, Test F1: 29.24%
CONN: Test Loss:   5.1,  Test Acc: 24.25%, Test F1:  5.76%
consistency_top_sec: 45.52%,  consistency_sec_conn: 20.60%, consistency_top_sec_conn: 19.06%
              precision    recall  f1-score   support

    Temporal     0.4203    0.4265    0.4234        68
 Contingency     0.5967    0.3942    0.4747       274
  Comparison     0.5603    0.4514    0.5000       144
   Expansion     0.6790    0.8264    0.7455       553

    accuracy                         0.6343      1039
   macro avg     0.5641    0.5246    0.5359      1039
weighted avg     0.6239    0.6343    0.6190      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3797    0.5556    0.4511        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5573    0.5242    0.5402       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4127    0.4062    0.4094       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4385    0.6600    0.5269       200
    Expansion.Instantiation     0.7048    0.6271    0.6637       118
      Expansion.Restatement     0.4913    0.4028    0.4427       211
      Expansion.Alternative     0.5000    0.1111    0.1818         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4957      1039
                  macro avg     0.3168    0.2988    0.2924      1039
               weighted avg     0.4834    0.4957    0.4820      1039

Epoch [4/30]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.4,  Val Acc: 61.37%, Val F1: 49.01% Time: 32.711066246032715 
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 46.01%, Val F1: 26.69% Time: 32.711066246032715 
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 43.75%,Val Loss:   5.4,  Val Acc: 26.01%, Val F1:  5.48% Time: 32.711066246032715 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 62.15%, Val F1: 49.37% Time: 186.72544693946838 
top-down:SEC: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 47.12%, Val F1: 27.69% Time: 186.72544693946838 
top-down:CONN: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 43.75%,Val Loss:   5.4,  Val Acc: 26.01%, Val F1:  5.38% Time: 186.72544693946838 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 60.77%, Val F1: 51.19% Time: 345.5918297767639 *
top-down:SEC: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 46.01%, Val F1: 28.25% Time: 345.5918297767639 *
top-down:CONN: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   5.5,  Val Acc: 26.52%, Val F1:  6.42% Time: 345.5918297767639 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 61.03%, Val F1: 50.03% Time: 505.74478697776794 *
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 46.35%, Val F1: 29.26% Time: 505.74478697776794 *
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 34.38%,Val Loss:   5.5,  Val Acc: 26.95%, Val F1:  6.49% Time: 505.74478697776794 *
 
 
Train time usage: 612.3687927722931
Test time usage: 1.9398930072784424
TOP: Test Loss:   4.9,  Test Acc: 63.23%, Test F1: 55.13%
SEC: Test Loss:   4.9,  Test Acc: 51.59%, Test F1: 32.70%
CONN: Test Loss:   4.9,  Test Acc: 26.56%, Test F1:  7.96%
consistency_top_sec: 47.93%,  consistency_sec_conn: 22.14%, consistency_top_sec_conn: 20.89%
              precision    recall  f1-score   support

    Temporal     0.6667    0.2941    0.4082        68
 Contingency     0.5365    0.5326    0.5345       276
  Comparison     0.4941    0.5833    0.5350       144
   Expansion     0.7186    0.7368    0.7276       551

    accuracy                         0.6323      1039
   macro avg     0.6040    0.5367    0.5513      1039
weighted avg     0.6357    0.6323    0.6287      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6250    0.3704    0.4651        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5134    0.6394    0.5695       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4503    0.6016    0.5151       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5412    0.4600    0.4973       200
    Expansion.Instantiation     0.7156    0.6610    0.6872       118
      Expansion.Restatement     0.4429    0.4408    0.4418       211
      Expansion.Alternative     0.4000    0.4444    0.4211         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5159      1039
                  macro avg     0.3353    0.3289    0.3270      1039
               weighted avg     0.4997    0.5159    0.5022      1039

Epoch [5/30]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   5.4,  Val Acc: 63.43%, Val F1: 53.46% Time: 46.80697727203369 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   5.4,  Val Acc: 49.53%, Val F1: 30.53% Time: 46.80697727203369 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   5.4,  Val Acc: 27.81%, Val F1:  5.92% Time: 46.80697727203369 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   5.6,  Val Acc: 61.97%, Val F1: 53.83% Time: 201.29185390472412 
top-down:SEC: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   5.6,  Val Acc: 47.55%, Val F1: 29.43% Time: 201.29185390472412 
top-down:CONN: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 26.18%, Val F1:  6.07% Time: 201.29185390472412 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 59.66%, Val F1: 50.48% Time: 352.8858177661896 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   5.6,  Val Acc: 45.58%, Val F1: 28.42% Time: 352.8858177661896 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 24.98%, Val F1:  5.46% Time: 352.8858177661896 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 60.09%, Val F1: 49.21% Time: 506.9191098213196 
top-down:SEC: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   5.6,  Val Acc: 47.38%, Val F1: 29.09% Time: 506.9191098213196 
top-down:CONN: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 27.81%, Val F1:  6.21% Time: 506.9191098213196 
 
 
Train time usage: 609.8110592365265
Test time usage: 1.800718069076538
TOP: Test Loss:   5.2,  Test Acc: 61.79%, Test F1: 54.71%
SEC: Test Loss:   5.2,  Test Acc: 49.95%, Test F1: 31.20%
CONN: Test Loss:   5.2,  Test Acc: 26.37%, Test F1:  7.81%
consistency_top_sec: 47.45%,  consistency_sec_conn: 22.43%, consistency_top_sec_conn: 21.66%
              precision    recall  f1-score   support

    Temporal     0.5854    0.3529    0.4404        68
 Contingency     0.5492    0.5254    0.5370       276
  Comparison     0.4257    0.5931    0.4957       145
   Expansion     0.7274    0.7036    0.7153       550

    accuracy                         0.6179      1039
   macro avg     0.5719    0.5438    0.5471      1039
weighted avg     0.6287    0.6179    0.6193      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5306    0.4815    0.5049        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5251    0.5836    0.5528       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.3871    0.5625    0.4586       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5023    0.5350    0.5182       200
    Expansion.Instantiation     0.7100    0.6017    0.6514       118
      Expansion.Restatement     0.4713    0.3886    0.4260       211
      Expansion.Alternative     0.2500    0.4444    0.3200         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4995      1039
                  macro avg     0.3069    0.3270    0.3120      1039
               weighted avg     0.4864    0.4995    0.4889      1039

Epoch [6/30]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.7,  Val Acc: 62.15%, Val F1: 49.65% Time: 56.361846685409546 
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   5.7,  Val Acc: 47.47%, Val F1: 28.93% Time: 56.361846685409546 
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   5.7,  Val Acc: 27.73%, Val F1:  6.86% Time: 56.361846685409546 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   5.8,  Val Acc: 60.00%, Val F1: 50.90% Time: 206.62382650375366 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.8,  Val Acc: 47.81%, Val F1: 29.80% Time: 206.62382650375366 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   5.8,  Val Acc: 26.95%, Val F1:  6.73% Time: 206.62382650375366 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 61.03%, Val F1: 50.02% Time: 367.33016204833984 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   5.7,  Val Acc: 47.55%, Val F1: 29.20% Time: 367.33016204833984 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 27.21%, Val F1:  6.72% Time: 367.33016204833984 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 62.49%, Val F1: 52.18% Time: 518.4648327827454 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 65.62%,Val Loss:   5.8,  Val Acc: 46.27%, Val F1: 28.67% Time: 518.4648327827454 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 37.50%,Val Loss:   5.8,  Val Acc: 27.21%, Val F1:  7.32% Time: 518.4648327827454 
 
 
Train time usage: 611.3283412456512
Test time usage: 1.879197120666504
TOP: Test Loss:   5.2,  Test Acc: 63.91%, Test F1: 56.60%
SEC: Test Loss:   5.2,  Test Acc: 51.40%, Test F1: 33.11%
CONN: Test Loss:   5.2,  Test Acc: 28.59%, Test F1:  9.07%
consistency_top_sec: 48.89%,  consistency_sec_conn: 23.10%, consistency_top_sec_conn: 22.43%
              precision    recall  f1-score   support

    Temporal     0.5000    0.4412    0.4688        68
 Contingency     0.5777    0.5254    0.5503       276
  Comparison     0.5333    0.5000    0.5161       144
   Expansion     0.7032    0.7568    0.7290       551

    accuracy                         0.6391      1039
   macro avg     0.5786    0.5558    0.5660      1039
weighted avg     0.6330    0.6391    0.6350      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4828    0.5185    0.5000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5606    0.6022    0.5806       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4610    0.5078    0.4833       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4899    0.4850    0.4874       200
    Expansion.Instantiation     0.6814    0.6525    0.6667       118
      Expansion.Restatement     0.4562    0.4692    0.4626       211
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5140      1039
                  macro avg     0.3168    0.3547    0.3311      1039
               weighted avg     0.4944    0.5140    0.5033      1039

Epoch [7/30]
top-down:TOP: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   5.9,  Val Acc: 61.89%, Val F1: 50.92% Time: 62.22344255447388 
top-down:SEC: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.9,  Val Acc: 47.21%, Val F1: 29.22% Time: 62.22344255447388 
top-down:CONN: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   5.9,  Val Acc: 27.55%, Val F1:  6.96% Time: 62.22344255447388 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   5.9,  Val Acc: 62.32%, Val F1: 52.29% Time: 218.0048635005951 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   5.9,  Val Acc: 46.18%, Val F1: 28.62% Time: 218.0048635005951 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.9,  Val Acc: 25.75%, Val F1:  6.77% Time: 218.0048635005951 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 81.25%,Val Loss:   5.9,  Val Acc: 62.49%, Val F1: 50.83% Time: 364.14798879623413 
top-down:SEC: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 59.38%,Val Loss:   5.9,  Val Acc: 46.01%, Val F1: 27.38% Time: 364.14798879623413 
top-down:CONN: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 31.25%,Val Loss:   5.9,  Val Acc: 27.12%, Val F1:  7.33% Time: 364.14798879623413 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 78.12%,Val Loss:   5.9,  Val Acc: 61.03%, Val F1: 50.80% Time: 520.0865705013275 
top-down:SEC: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 46.01%, Val F1: 27.43% Time: 520.0865705013275 
top-down:CONN: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 28.12%,Val Loss:   5.9,  Val Acc: 25.92%, Val F1:  6.96% Time: 520.0865705013275 
 
 
Train time usage: 601.866721868515
Test time usage: 1.8125362396240234
TOP: Test Loss:   5.5,  Test Acc: 62.95%, Test F1: 57.26%
SEC: Test Loss:   5.5,  Test Acc: 51.01%, Test F1: 32.96%
CONN: Test Loss:   5.5,  Test Acc: 26.28%, Test F1:  8.45%
consistency_top_sec: 49.28%,  consistency_sec_conn: 22.62%, consistency_top_sec_conn: 21.94%
              precision    recall  f1-score   support

    Temporal     0.5556    0.4412    0.4918        68
 Contingency     0.5263    0.6159    0.5676       276
  Comparison     0.5099    0.5310    0.5203       145
   Expansion     0.7378    0.6855    0.7107       550

    accuracy                         0.6295      1039
   macro avg     0.5824    0.5684    0.5726      1039
weighted avg     0.6379    0.6295    0.6318      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5490    0.5185    0.5333        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5203    0.6654    0.5840       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4437    0.5234    0.4803       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5212    0.4300    0.4712       200
    Expansion.Instantiation     0.6364    0.6525    0.6444       118
      Expansion.Restatement     0.4583    0.4171    0.4367       211
      Expansion.Alternative     0.4167    0.5556    0.4762         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5101      1039
                  macro avg     0.3223    0.3420    0.3296      1039
               weighted avg     0.4872    0.5101    0.4948      1039

Epoch [8/30]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   6.2,  Val Acc: 58.97%, Val F1: 49.88% Time: 76.23368716239929 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 45.06%, Val F1: 27.10% Time: 76.23368716239929 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 25.06%, Val F1:  6.44% Time: 76.23368716239929 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 59.74%, Val F1: 50.48% Time: 230.49366760253906 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.4,  Val Acc: 44.12%, Val F1: 27.36% Time: 230.49366760253906 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   6.4,  Val Acc: 25.92%, Val F1:  7.23% Time: 230.49366760253906 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.4,  Val Acc: 60.60%, Val F1: 50.65% Time: 384.4249475002289 
top-down:SEC: Iter:   3000,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 44.89%, Val F1: 26.85% Time: 384.4249475002289 
top-down:CONN: Iter:   3000,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   6.4,  Val Acc: 26.18%, Val F1:  7.05% Time: 384.4249475002289 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.4,  Val Acc: 60.60%, Val F1: 49.38% Time: 537.3360729217529 
top-down:SEC: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.4,  Val Acc: 45.41%, Val F1: 28.05% Time: 537.3360729217529 
top-down:CONN: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   6.4,  Val Acc: 26.44%, Val F1:  7.22% Time: 537.3360729217529 
 
 
Train time usage: 598.527108669281
Test time usage: 1.9149746894836426
TOP: Test Loss:   5.6,  Test Acc: 64.49%, Test F1: 56.67%
SEC: Test Loss:   5.6,  Test Acc: 51.01%, Test F1: 34.39%
CONN: Test Loss:   5.6,  Test Acc: 25.31%, Test F1:  8.51%
consistency_top_sec: 48.60%,  consistency_sec_conn: 20.98%, consistency_top_sec_conn: 20.50%
              precision    recall  f1-score   support

    Temporal     0.6341    0.3824    0.4771        68
 Contingency     0.5781    0.4982    0.5352       275
  Comparison     0.5139    0.5139    0.5139       144
   Expansion     0.7018    0.7844    0.7408       552

    accuracy                         0.6449      1039
   macro avg     0.6070    0.5447    0.5667      1039
weighted avg     0.6386    0.6449    0.6377      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7222    0.4815    0.5778        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5496    0.5353    0.5424       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4710    0.5078    0.4887       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4491    0.5950    0.5118       200
    Expansion.Instantiation     0.6107    0.6780    0.6426       118
      Expansion.Restatement     0.4712    0.4265    0.4478       211
      Expansion.Alternative     0.5000    0.6667    0.5714         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5101      1039
                  macro avg     0.3431    0.3537    0.3439      1039
               weighted avg     0.4937    0.5101    0.4980      1039

Epoch [9/30]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.4,  Val Acc: 60.09%, Val F1: 48.73% Time: 89.32288002967834 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.4,  Val Acc: 44.55%, Val F1: 27.82% Time: 89.32288002967834 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   6.4,  Val Acc: 26.70%, Val F1:  6.99% Time: 89.32288002967834 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.4,  Val Acc: 61.37%, Val F1: 51.03% Time: 249.8971610069275 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   6.4,  Val Acc: 45.75%, Val F1: 28.21% Time: 249.8971610069275 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.4,  Val Acc: 27.12%, Val F1:  7.46% Time: 249.8971610069275 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 84.38%,Val Loss:   6.6,  Val Acc: 61.12%, Val F1: 49.94% Time: 401.73413944244385 
top-down:SEC: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   6.6,  Val Acc: 44.55%, Val F1: 28.08% Time: 401.73413944244385 
top-down:CONN: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 26.27%, Val F1:  7.07% Time: 401.73413944244385 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 59.57%, Val F1: 48.95% Time: 553.5895764827728 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   6.7,  Val Acc: 44.64%, Val F1: 27.40% Time: 553.5895764827728 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 26.35%, Val F1:  7.53% Time: 553.5895764827728 
 
 
Train time usage: 611.2524175643921
Test time usage: 2.0810368061065674
TOP: Test Loss:   6.0,  Test Acc: 63.14%, Test F1: 55.84%
SEC: Test Loss:   6.0,  Test Acc: 49.95%, Test F1: 31.30%
CONN: Test Loss:   6.0,  Test Acc: 25.41%, Test F1:  8.03%
consistency_top_sec: 48.99%,  consistency_sec_conn: 21.37%, consistency_top_sec_conn: 20.79%
              precision    recall  f1-score   support

    Temporal     0.4359    0.5000    0.4658        68
 Contingency     0.5714    0.5217    0.5455       276
  Comparison     0.4832    0.5000    0.4915       144
   Expansion     0.7250    0.7368    0.7309       551

    accuracy                         0.6314      1039
   macro avg     0.5539    0.5646    0.5584      1039
weighted avg     0.6318    0.6314    0.6311      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4179    0.5185    0.4628        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5564    0.5502    0.5533       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4384    0.5000    0.4672       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4350    0.5350    0.4798       200
    Expansion.Instantiation     0.6555    0.6610    0.6582       118
      Expansion.Restatement     0.5174    0.4218    0.4648       211
      Expansion.Alternative     0.2632    0.5556    0.3571         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4995      1039
                  macro avg     0.2985    0.3402    0.3130      1039
               weighted avg     0.4853    0.4995    0.4894      1039

Epoch [10/30]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 60.17%, Val F1: 50.68% Time: 97.96203708648682 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.8,  Val Acc: 44.46%, Val F1: 27.02% Time: 97.96203708648682 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   6.8,  Val Acc: 25.84%, Val F1:  6.79% Time: 97.96203708648682 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 62.06%, Val F1: 51.72% Time: 242.9977216720581 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   6.8,  Val Acc: 45.92%, Val F1: 27.74% Time: 242.9977216720581 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.8,  Val Acc: 25.58%, Val F1:  7.63% Time: 242.9977216720581 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 60.60%, Val F1: 50.50% Time: 396.6375479698181 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.8,  Val Acc: 45.24%, Val F1: 27.23% Time: 396.6375479698181 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   6.8,  Val Acc: 26.18%, Val F1:  7.42% Time: 396.6375479698181 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 61.37%, Val F1: 48.58% Time: 551.7919747829437 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 44.12%, Val F1: 26.72% Time: 551.7919747829437 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.8,  Val Acc: 27.30%, Val F1:  7.62% Time: 551.7919747829437 
 
 
Train time usage: 598.7067568302155
Test time usage: 1.9092588424682617
TOP: Test Loss:   6.1,  Test Acc: 63.72%, Test F1: 56.06%
SEC: Test Loss:   6.1,  Test Acc: 49.09%, Test F1: 31.49%
CONN: Test Loss:   6.1,  Test Acc: 25.51%, Test F1:  8.73%
consistency_top_sec: 47.93%,  consistency_sec_conn: 21.08%, consistency_top_sec_conn: 20.60%
              precision    recall  f1-score   support

    Temporal     0.5102    0.3676    0.4274        68
 Contingency     0.5560    0.5580    0.5570       276
  Comparison     0.5429    0.5241    0.5333       145
   Expansion     0.7103    0.7400    0.7248       550

    accuracy                         0.6372      1039
   macro avg     0.5798    0.5474    0.5606      1039
weighted avg     0.6328    0.6372    0.6341      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3704    0.4255        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5401    0.5762    0.5576       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4565    0.4922    0.4737       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4318    0.5700    0.4914       200
    Expansion.Instantiation     0.6271    0.6271    0.6271       118
      Expansion.Restatement     0.4785    0.3697    0.4171       211
      Expansion.Alternative     0.2941    0.5556    0.3846         9
             Expansion.List     0.0909    0.0833    0.0870        12

                   accuracy                         0.4909      1039
                  macro avg     0.3108    0.3313    0.3149      1039
               weighted avg     0.4772    0.4909    0.4797      1039

Epoch [11/30]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 60.77%, Val F1: 50.16% Time: 110.60721945762634 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 81.25%,Val Loss:   7.0,  Val Acc: 45.15%, Val F1: 27.33% Time: 110.60721945762634 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 40.62%,Val Loss:   7.0,  Val Acc: 25.15%, Val F1:  7.50% Time: 110.60721945762634 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 61.72%, Val F1: 49.49% Time: 262.4595935344696 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   7.2,  Val Acc: 43.78%, Val F1: 26.55% Time: 262.4595935344696 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 37.50%,Val Loss:   7.2,  Val Acc: 25.15%, Val F1:  7.14% Time: 262.4595935344696 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 59.91%, Val F1: 50.70% Time: 404.27498960494995 
top-down:SEC: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   7.1,  Val Acc: 44.89%, Val F1: 29.99% Time: 404.27498960494995 
top-down:CONN: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   7.1,  Val Acc: 25.41%, Val F1:  7.35% Time: 404.27498960494995 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.2,  Val Acc: 59.74%, Val F1: 50.03% Time: 560.85302901268 
top-down:SEC: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 43.52%, Val F1: 27.19% Time: 560.85302901268 
top-down:CONN: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 46.88%,Val Loss:   7.2,  Val Acc: 25.67%, Val F1:  7.38% Time: 560.85302901268 
 
 
Train time usage: 598.1379287242889
Test time usage: 2.015076160430908
TOP: Test Loss:   6.3,  Test Acc: 62.37%, Test F1: 54.91%
SEC: Test Loss:   6.3,  Test Acc: 48.60%, Test F1: 31.72%
CONN: Test Loss:   6.3,  Test Acc: 26.47%, Test F1:  9.26%
consistency_top_sec: 47.16%,  consistency_sec_conn: 21.17%, consistency_top_sec_conn: 20.60%
              precision    recall  f1-score   support

    Temporal     0.4667    0.4118    0.4375        68
 Contingency     0.5530    0.5309    0.5417       275
  Comparison     0.5070    0.4966    0.5017       145
   Expansion     0.7016    0.7296    0.7153       551

    accuracy                         0.6237      1039
   macro avg     0.5571    0.5422    0.5491      1039
weighted avg     0.6197    0.6237    0.6214      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5102    0.4630    0.4854        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5418    0.5539    0.5478       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4476    0.5000    0.4723       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4317    0.4900    0.4590       200
    Expansion.Instantiation     0.6417    0.6525    0.6471       118
      Expansion.Restatement     0.4550    0.4076    0.4300       211
      Expansion.Alternative     0.2778    0.5556    0.3704         9
             Expansion.List     0.0714    0.0833    0.0769        12

                   accuracy                         0.4860      1039
                  macro avg     0.3070    0.3369    0.3172      1039
               weighted avg     0.4735    0.4860    0.4785      1039

Epoch [12/30]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 58.71%, Val F1: 49.93% Time: 121.42413210868835 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 42.92%, Val F1: 25.94% Time: 121.42413210868835 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   7.1,  Val Acc: 26.09%, Val F1:  7.92% Time: 121.42413210868835 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 60.77%, Val F1: 50.79% Time: 275.6075053215027 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   7.2,  Val Acc: 43.18%, Val F1: 25.98% Time: 275.6075053215027 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   7.2,  Val Acc: 25.41%, Val F1:  7.05% Time: 275.6075053215027 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 60.34%, Val F1: 50.28% Time: 427.59810996055603 
top-down:SEC: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   7.3,  Val Acc: 44.64%, Val F1: 27.85% Time: 427.59810996055603 
top-down:CONN: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   7.3,  Val Acc: 25.32%, Val F1:  7.63% Time: 427.59810996055603 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   7.3,  Val Acc: 60.60%, Val F1: 51.39% Time: 578.9889643192291 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   7.3,  Val Acc: 43.95%, Val F1: 26.81% Time: 578.9889643192291 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   7.3,  Val Acc: 26.87%, Val F1:  7.76% Time: 578.9889643192291 
 
 
Train time usage: 605.4180481433868
Test time usage: 2.0464656352996826
TOP: Test Loss:   6.4,  Test Acc: 63.04%, Test F1: 57.14%
SEC: Test Loss:   6.4,  Test Acc: 51.30%, Test F1: 32.86%
CONN: Test Loss:   6.4,  Test Acc: 26.28%, Test F1:  9.18%
consistency_top_sec: 50.05%,  consistency_sec_conn: 22.62%, consistency_top_sec_conn: 22.23%
              precision    recall  f1-score   support

    Temporal     0.4930    0.5147    0.5036        68
 Contingency     0.5356    0.5745    0.5544       275
  Comparison     0.5338    0.4897    0.5108       145
   Expansion     0.7241    0.7096    0.7168       551

    accuracy                         0.6304      1039
   macro avg     0.5716    0.5721    0.5714      1039
weighted avg     0.6325    0.6304    0.6311      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5246    0.5926    0.5565        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5265    0.5911    0.5569       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4706    0.5000    0.4848       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5103    0.4950    0.5025       200
    Expansion.Instantiation     0.6752    0.6695    0.6723       118
      Expansion.Restatement     0.4634    0.4502    0.4567       211
      Expansion.Alternative     0.2941    0.5556    0.3846         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5130      1039
                  macro avg     0.3150    0.3504    0.3286      1039
               weighted avg     0.4931    0.5130    0.5020      1039

Epoch [13/30]
top-down:TOP: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 61.37%, Val F1: 49.81% Time: 125.55521869659424 
top-down:SEC: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   7.2,  Val Acc: 44.46%, Val F1: 26.75% Time: 125.55521869659424 
top-down:CONN: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   7.2,  Val Acc: 25.75%, Val F1:  7.53% Time: 125.55521869659424 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 60.17%, Val F1: 49.94% Time: 274.9021909236908 
top-down:SEC: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   7.3,  Val Acc: 44.64%, Val F1: 26.84% Time: 274.9021909236908 
top-down:CONN: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   7.3,  Val Acc: 26.09%, Val F1:  7.41% Time: 274.9021909236908 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.5,  Val Acc: 58.54%, Val F1: 49.51% Time: 434.32308292388916 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   7.5,  Val Acc: 43.61%, Val F1: 26.92% Time: 434.32308292388916 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   7.5,  Val Acc: 26.01%, Val F1:  7.27% Time: 434.32308292388916 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 60.00%, Val F1: 51.34% Time: 586.3174264431 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 43.95%, Val F1: 26.26% Time: 586.3174264431 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   7.3,  Val Acc: 26.01%, Val F1:  7.27% Time: 586.3174264431 
 
 
Train time usage: 602.2505338191986
Test time usage: 1.8977892398834229
TOP: Test Loss:   6.6,  Test Acc: 62.18%, Test F1: 54.44%
SEC: Test Loss:   6.6,  Test Acc: 49.47%, Test F1: 32.55%
CONN: Test Loss:   6.6,  Test Acc: 27.43%, Test F1:  9.49%
consistency_top_sec: 48.22%,  consistency_sec_conn: 23.00%, consistency_top_sec_conn: 22.52%
              precision    recall  f1-score   support

    Temporal     0.4643    0.3824    0.4194        68
 Contingency     0.5640    0.5109    0.5361       276
  Comparison     0.4702    0.5448    0.5048       145
   Expansion     0.7080    0.7273    0.7175       550

    accuracy                         0.6218      1039
   macro avg     0.5516    0.5413    0.5444      1039
weighted avg     0.6206    0.6218    0.6201      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4681    0.4074    0.4356        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5538    0.5353    0.5444       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4167    0.5469    0.4730       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4661    0.5150    0.4893       200
    Expansion.Instantiation     0.6429    0.6864    0.6639       118
      Expansion.Restatement     0.4754    0.4123    0.4416       211
      Expansion.Alternative     0.2273    0.5556    0.3226         9
             Expansion.List     0.2857    0.1667    0.2105        12

                   accuracy                         0.4947      1039
                  macro avg     0.3214    0.3478    0.3255      1039
               weighted avg     0.4836    0.4947    0.4864      1039

Epoch [14/30]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.5,  Val Acc: 62.32%, Val F1: 51.79% Time: 139.50516200065613 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.5,  Val Acc: 44.21%, Val F1: 27.00% Time: 139.50516200065613 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 25.84%, Val F1:  7.78% Time: 139.50516200065613 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 60.77%, Val F1: 51.03% Time: 282.75124979019165 
top-down:SEC: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   7.6,  Val Acc: 44.21%, Val F1: 27.44% Time: 282.75124979019165 
top-down:CONN: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   7.6,  Val Acc: 25.75%, Val F1:  7.52% Time: 282.75124979019165 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 59.66%, Val F1: 50.06% Time: 434.5328061580658 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 87.50%,Val Loss:   7.7,  Val Acc: 43.78%, Val F1: 27.40% Time: 434.5328061580658 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   7.7,  Val Acc: 24.72%, Val F1:  7.42% Time: 434.5328061580658 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   7.5,  Val Acc: 60.94%, Val F1: 49.79% Time: 592.6051099300385 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 90.62%,Val Loss:   7.5,  Val Acc: 44.46%, Val F1: 28.17% Time: 592.6051099300385 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 46.88%,Val Loss:   7.5,  Val Acc: 26.87%, Val F1:  7.82% Time: 592.6051099300385 
 
 
Train time usage: 597.683278799057
Test time usage: 2.251579523086548
TOP: Test Loss:   6.7,  Test Acc: 62.75%, Test F1: 53.02%
SEC: Test Loss:   6.7,  Test Acc: 49.95%, Test F1: 32.96%
CONN: Test Loss:   6.7,  Test Acc: 26.37%, Test F1:  9.26%
consistency_top_sec: 48.22%,  consistency_sec_conn: 22.04%, consistency_top_sec_conn: 21.46%
              precision    recall  f1-score   support

    Temporal     0.3889    0.3088    0.3443        68
 Contingency     0.5691    0.5091    0.5374       275
  Comparison     0.4903    0.5278    0.5084       144
   Expansion     0.7106    0.7518    0.7306       552

    accuracy                         0.6275      1039
   macro avg     0.5397    0.5244    0.5302      1039
weighted avg     0.6216    0.6275    0.6234      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4643    0.4815    0.4727        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5608    0.5316    0.5458       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4575    0.5469    0.4982       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4587    0.5000    0.4785       200
    Expansion.Instantiation     0.6525    0.6525    0.6525       118
      Expansion.Restatement     0.4597    0.4597    0.4597       211
      Expansion.Alternative     0.2353    0.4444    0.3077         9
             Expansion.List     0.2857    0.1667    0.2105        12

                   accuracy                         0.4995      1039
                  macro avg     0.3250    0.3439    0.3296      1039
               weighted avg     0.4868    0.4995    0.4919      1039

Epoch [15/30]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 61.72%, Val F1: 51.58% Time: 150.12578058242798 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.7,  Val Acc: 44.98%, Val F1: 27.13% Time: 150.12578058242798 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   7.7,  Val Acc: 26.18%, Val F1:  7.96% Time: 150.12578058242798 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 60.86%, Val F1: 50.41% Time: 304.9889931678772 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 44.29%, Val F1: 28.82% Time: 304.9889931678772 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   7.7,  Val Acc: 26.52%, Val F1:  7.97% Time: 304.9889931678772 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 61.72%, Val F1: 51.37% Time: 449.9958848953247 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.7,  Val Acc: 45.32%, Val F1: 27.13% Time: 449.9958848953247 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 62.50%,Val Loss:   7.7,  Val Acc: 25.84%, Val F1:  7.98% Time: 449.9958848953247 
 
 
Train time usage: 590.2997710704803
Test time usage: 1.9154274463653564
TOP: Test Loss:   6.9,  Test Acc: 62.75%, Test F1: 55.18%
SEC: Test Loss:   6.9,  Test Acc: 49.28%, Test F1: 33.07%
CONN: Test Loss:   6.9,  Test Acc: 26.47%, Test F1:  8.91%
consistency_top_sec: 47.74%,  consistency_sec_conn: 22.23%, consistency_top_sec_conn: 21.85%
              precision    recall  f1-score   support

    Temporal     0.3587    0.4853    0.4125        68
 Contingency     0.5613    0.5471    0.5541       276
  Comparison     0.5462    0.4931    0.5182       144
   Expansion     0.7245    0.7205    0.7225       551

    accuracy                         0.6275      1039
   macro avg     0.5477    0.5615    0.5518      1039
weighted avg     0.6325    0.6275    0.6292      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4110    0.5556    0.4724        54
         Temporal.Synchrony     0.1000    0.0714    0.0833        14
          Contingency.Cause     0.5540    0.5725    0.5631       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5000    0.4844    0.4921       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4317    0.4900    0.4590       200
    Expansion.Instantiation     0.6863    0.5932    0.6364       118
      Expansion.Restatement     0.4691    0.4313    0.4494       211
      Expansion.Alternative     0.2353    0.4444    0.3077         9
             Expansion.List     0.1818    0.1667    0.1739        12

                   accuracy                         0.4928      1039
                  macro avg     0.3245    0.3463    0.3307      1039
               weighted avg     0.4882    0.4928    0.4886      1039

Epoch [16/30]
top-down:TOP: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 59.91%, Val F1: 49.32% Time: 8.528625965118408 
top-down:SEC: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   7.7,  Val Acc: 44.12%, Val F1: 27.13% Time: 8.528625965118408 
top-down:CONN: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   7.7,  Val Acc: 25.32%, Val F1:  7.47% Time: 8.528625965118408 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 60.77%, Val F1: 50.72% Time: 159.21782755851746 
top-down:SEC: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 44.21%, Val F1: 26.83% Time: 159.21782755851746 
top-down:CONN: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   7.8,  Val Acc: 26.52%, Val F1:  7.82% Time: 159.21782755851746 
 
 
top-down:TOP: Iter:   6100,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 61.12%, Val F1: 50.78% Time: 316.9779989719391 
top-down:SEC: Iter:   6100,  Train Loss: 2.4e+01,  Train Acc: 81.25%,Val Loss:   7.8,  Val Acc: 45.41%, Val F1: 26.90% Time: 316.9779989719391 
top-down:CONN: Iter:   6100,  Train Loss: 2.4e+01,  Train Acc: 53.12%,Val Loss:   7.8,  Val Acc: 26.09%, Val F1:  7.71% Time: 316.9779989719391 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 60.77%, Val F1: 51.13% Time: 473.2032446861267 
top-down:SEC: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   7.8,  Val Acc: 44.81%, Val F1: 27.20% Time: 473.2032446861267 
top-down:CONN: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   7.8,  Val Acc: 24.72%, Val F1:  7.32% Time: 473.2032446861267 
 
 
Train time usage: 602.3482234477997
Test time usage: 1.866804838180542
TOP: Test Loss:   7.0,  Test Acc: 64.10%, Test F1: 56.65%
SEC: Test Loss:   7.0,  Test Acc: 48.70%, Test F1: 32.46%
CONN: Test Loss:   7.0,  Test Acc: 26.08%, Test F1:  9.00%
consistency_top_sec: 47.74%,  consistency_sec_conn: 21.46%, consistency_top_sec_conn: 20.89%
              precision    recall  f1-score   support

    Temporal     0.4156    0.4706    0.4414        68
 Contingency     0.6076    0.5217    0.5614       276
  Comparison     0.5310    0.5347    0.5329       144
   Expansion     0.7121    0.7495    0.7303       551

    accuracy                         0.6410      1039
   macro avg     0.5666    0.5691    0.5665      1039
weighted avg     0.6398    0.6410    0.6392      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4407    0.4815    0.4602        54
         Temporal.Synchrony     0.1429    0.0714    0.0952        14
          Contingency.Cause     0.5622    0.5204    0.5405       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4621    0.5234    0.4908       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4408    0.4650    0.4526       200
    Expansion.Instantiation     0.7216    0.5932    0.6512       118
      Expansion.Restatement     0.4228    0.4929    0.4551       211
      Expansion.Alternative     0.2500    0.4444    0.3200         9
             Expansion.List     0.1429    0.0833    0.1053        12

                   accuracy                         0.4870      1039
                  macro avg     0.3260    0.3342    0.3246      1039
               weighted avg     0.4838    0.4870    0.4831      1039

Epoch [17/30]
top-down:TOP: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 59.91%, Val F1: 50.78% Time: 18.58090090751648 
top-down:SEC: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 44.12%, Val F1: 27.18% Time: 18.58090090751648 
top-down:CONN: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   7.9,  Val Acc: 25.92%, Val F1:  7.85% Time: 18.58090090751648 
 
 
top-down:TOP: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 61.46%, Val F1: 51.26% Time: 168.62646579742432 
top-down:SEC: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 43.95%, Val F1: 28.29% Time: 168.62646579742432 
top-down:CONN: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 62.50%,Val Loss:   7.8,  Val Acc: 25.49%, Val F1:  8.11% Time: 168.62646579742432 
 
 
top-down:TOP: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 60.60%, Val F1: 50.35% Time: 318.6812324523926 
top-down:SEC: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   8.0,  Val Acc: 44.89%, Val F1: 27.57% Time: 318.6812324523926 
top-down:CONN: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   8.0,  Val Acc: 25.84%, Val F1:  7.97% Time: 318.6812324523926 
 
 
top-down:TOP: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 60.77%, Val F1: 50.51% Time: 467.42098093032837 
top-down:SEC: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 44.98%, Val F1: 27.78% Time: 467.42098093032837 
top-down:CONN: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   8.0,  Val Acc: 26.27%, Val F1:  8.36% Time: 467.42098093032837 
 
 
Train time usage: 595.8061966896057
Test time usage: 1.8112905025482178
TOP: Test Loss:   7.2,  Test Acc: 62.37%, Test F1: 54.07%
SEC: Test Loss:   7.2,  Test Acc: 48.70%, Test F1: 33.26%
CONN: Test Loss:   7.2,  Test Acc: 25.89%, Test F1:  9.06%
consistency_top_sec: 47.16%,  consistency_sec_conn: 21.27%, consistency_top_sec_conn: 20.60%
              precision    recall  f1-score   support

    Temporal     0.4211    0.3529    0.3840        68
 Contingency     0.5520    0.5580    0.5550       276
  Comparison     0.5068    0.5103    0.5086       145
   Expansion     0.7110    0.7200    0.7154       550

    accuracy                         0.6237      1039
   macro avg     0.5477    0.5353    0.5407      1039
weighted avg     0.6213    0.6237    0.6223      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4468    0.3889    0.4158        54
         Temporal.Synchrony     0.1667    0.0714    0.1000        14
          Contingency.Cause     0.5470    0.5836    0.5647       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4960    0.4844    0.4901       128
      Comparison.Concession     0.1000    0.0588    0.0741        17
      Expansion.Conjunction     0.4135    0.4900    0.4485       200
    Expansion.Instantiation     0.6990    0.6102    0.6516       118
      Expansion.Restatement     0.4652    0.4123    0.4372       211
      Expansion.Alternative     0.2381    0.5556    0.3333         9
             Expansion.List     0.1250    0.1667    0.1429        12

                   accuracy                         0.4870      1039
                  macro avg     0.3361    0.3474    0.3326      1039
               weighted avg     0.4868    0.4870    0.4844      1039

Epoch [18/30]
top-down:TOP: Iter:   6700,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.80%, Val F1: 52.05% Time: 29.55056357383728 
top-down:SEC: Iter:   6700,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   7.9,  Val Acc: 44.46%, Val F1: 28.26% Time: 29.55056357383728 
top-down:CONN: Iter:   6700,  Train Loss: 2.4e+01,  Train Acc: 53.12%,Val Loss:   7.9,  Val Acc: 25.41%, Val F1:  7.65% Time: 29.55056357383728 
 
 
top-down:TOP: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 60.60%, Val F1: 49.43% Time: 178.39302015304565 
top-down:SEC: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   8.0,  Val Acc: 44.29%, Val F1: 26.97% Time: 178.39302015304565 
top-down:CONN: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 24.98%, Val F1:  7.57% Time: 178.39302015304565 
 
 
top-down:TOP: Iter:   6900,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.46%, Val F1: 51.95% Time: 350.50413036346436 
top-down:SEC: Iter:   6900,  Train Loss: 2.2e+01,  Train Acc: 87.50%,Val Loss:   8.0,  Val Acc: 44.55%, Val F1: 27.85% Time: 350.50413036346436 
top-down:CONN: Iter:   6900,  Train Loss: 2.2e+01,  Train Acc: 50.00%,Val Loss:   8.0,  Val Acc: 25.49%, Val F1:  7.88% Time: 350.50413036346436 
 
 
top-down:TOP: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.1,  Val Acc: 61.37%, Val F1: 50.87% Time: 504.7308557033539 
top-down:SEC: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   8.1,  Val Acc: 45.32%, Val F1: 29.42% Time: 504.7308557033539 
top-down:CONN: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   8.1,  Val Acc: 25.67%, Val F1:  7.99% Time: 504.7308557033539 
 
 
Train time usage: 617.637861251831
Test time usage: 1.8874821662902832
TOP: Test Loss:   7.3,  Test Acc: 63.62%, Test F1: 55.32%
SEC: Test Loss:   7.3,  Test Acc: 49.09%, Test F1: 33.25%
CONN: Test Loss:   7.3,  Test Acc: 25.12%, Test F1:  9.05%
consistency_top_sec: 47.55%,  consistency_sec_conn: 20.21%, consistency_top_sec_conn: 19.83%
              precision    recall  f1-score   support

    Temporal     0.4028    0.4265    0.4143        68
 Contingency     0.5941    0.5145    0.5515       276
  Comparison     0.4967    0.5278    0.5118       144
   Expansion     0.7200    0.7514    0.7353       551

    accuracy                         0.6362      1039
   macro avg     0.5534    0.5550    0.5532      1039
weighted avg     0.6349    0.6362    0.6345      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4259    0.4259    0.4259        54
         Temporal.Synchrony     0.0769    0.0714    0.0741        14
          Contingency.Cause     0.5909    0.5316    0.5597       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4514    0.5078    0.4779       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4337    0.5400    0.4811       200
    Expansion.Instantiation     0.6500    0.6610    0.6555       118
      Expansion.Restatement     0.4670    0.4028    0.4326       211
      Expansion.Alternative     0.2941    0.5556    0.3846         9
             Expansion.List     0.1667    0.1667    0.1667        12

                   accuracy                         0.4909      1039
                  macro avg     0.3233    0.3512    0.3325      1039
               weighted avg     0.4884    0.4909    0.4871      1039

Epoch [19/30]
top-down:TOP: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.2,  Val Acc: 59.23%, Val F1: 48.91% Time: 44.10759234428406 
top-down:SEC: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.2,  Val Acc: 43.95%, Val F1: 26.76% Time: 44.10759234428406 
top-down:CONN: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   8.2,  Val Acc: 24.89%, Val F1:  7.54% Time: 44.10759234428406 
 
 
top-down:TOP: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 62.15%, Val F1: 50.07% Time: 195.98999691009521 
top-down:SEC: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 84.38%,Val Loss:   8.1,  Val Acc: 43.35%, Val F1: 25.84% Time: 195.98999691009521 
top-down:CONN: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 59.38%,Val Loss:   8.1,  Val Acc: 25.58%, Val F1:  7.92% Time: 195.98999691009521 
 
 
top-down:TOP: Iter:   7300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 61.20%, Val F1: 50.60% Time: 352.36920642852783 
top-down:SEC: Iter:   7300,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 44.64%, Val F1: 28.64% Time: 352.36920642852783 
top-down:CONN: Iter:   7300,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   8.2,  Val Acc: 26.35%, Val F1:  7.75% Time: 352.36920642852783 
 
 
top-down:TOP: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.3,  Val Acc: 60.17%, Val F1: 49.87% Time: 501.3973376750946 
top-down:SEC: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.3,  Val Acc: 43.26%, Val F1: 26.42% Time: 501.3973376750946 
top-down:CONN: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   8.3,  Val Acc: 25.24%, Val F1:  7.61% Time: 501.3973376750946 
 
 
Train time usage: 609.074369430542
Test time usage: 1.8028464317321777
TOP: Test Loss:   7.2,  Test Acc: 63.33%, Test F1: 55.72%
SEC: Test Loss:   7.2,  Test Acc: 49.57%, Test F1: 33.64%
CONN: Test Loss:   7.2,  Test Acc: 26.66%, Test F1: 10.14%
consistency_top_sec: 48.03%,  consistency_sec_conn: 21.66%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.4348    0.4412    0.4380        68
 Contingency     0.5556    0.5254    0.5400       276
  Comparison     0.5368    0.5069    0.5214       144
   Expansion     0.7155    0.7441    0.7295       551

    accuracy                         0.6333      1039
   macro avg     0.5607    0.5544    0.5572      1039
weighted avg     0.6299    0.6333    0.6313      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4906    0.4815    0.4860        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5468    0.5651    0.5558       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4806    0.4844    0.4825       128
      Comparison.Concession     0.1429    0.0588    0.0833        17
      Expansion.Conjunction     0.4378    0.5450    0.4855       200
    Expansion.Instantiation     0.6491    0.6271    0.6379       118
      Expansion.Restatement     0.4884    0.3981    0.4386       211
      Expansion.Alternative     0.2632    0.5556    0.3571         9
             Expansion.List     0.1818    0.1667    0.1739        12

                   accuracy                         0.4957      1039
                  macro avg     0.3346    0.3529    0.3364      1039
               weighted avg     0.4901    0.4957    0.4900      1039

Epoch [20/30]
top-down:TOP: Iter:   7500,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.3,  Val Acc: 62.23%, Val F1: 50.76% Time: 53.58308386802673 
top-down:SEC: Iter:   7500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   8.3,  Val Acc: 43.95%, Val F1: 28.78% Time: 53.58308386802673 
top-down:CONN: Iter:   7500,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   8.3,  Val Acc: 25.41%, Val F1:  8.12% Time: 53.58308386802673 
 
 
top-down:TOP: Iter:   7600,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 62.15%, Val F1: 51.33% Time: 202.62941360473633 
top-down:SEC: Iter:   7600,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   8.2,  Val Acc: 43.26%, Val F1: 26.63% Time: 202.62941360473633 
top-down:CONN: Iter:   7600,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   8.2,  Val Acc: 26.09%, Val F1:  8.22% Time: 202.62941360473633 
 
 
top-down:TOP: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 61.89%, Val F1: 51.19% Time: 359.4640567302704 
top-down:SEC: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 44.72%, Val F1: 28.47% Time: 359.4640567302704 
top-down:CONN: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   8.1,  Val Acc: 26.35%, Val F1:  8.39% Time: 359.4640567302704 
 
 
top-down:TOP: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   8.2,  Val Acc: 61.20%, Val F1: 50.86% Time: 513.4286956787109 
top-down:SEC: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   8.2,  Val Acc: 43.35%, Val F1: 27.43% Time: 513.4286956787109 
top-down:CONN: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   8.2,  Val Acc: 25.41%, Val F1:  7.83% Time: 513.4286956787109 
 
 
Train time usage: 599.3753206729889
Test time usage: 2.013841390609741
TOP: Test Loss:   7.3,  Test Acc: 64.20%, Test F1: 56.77%
SEC: Test Loss:   7.3,  Test Acc: 48.99%, Test F1: 33.31%
CONN: Test Loss:   7.3,  Test Acc: 26.37%, Test F1:  9.21%
consistency_top_sec: 48.12%,  consistency_sec_conn: 21.94%, consistency_top_sec_conn: 21.66%
              precision    recall  f1-score   support

    Temporal     0.4627    0.4559    0.4593        68
 Contingency     0.5725    0.5600    0.5662       275
  Comparison     0.5141    0.5069    0.5105       144
   Expansion     0.7291    0.7409    0.7350       552

    accuracy                         0.6420      1039
   macro avg     0.5696    0.5659    0.5677      1039
weighted avg     0.6404    0.6420    0.6411      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4912    0.5185    0.5045        54
         Temporal.Synchrony     0.1429    0.0714    0.0952        14
          Contingency.Cause     0.5576    0.5576    0.5576       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4638    0.5000    0.4812       128
      Comparison.Concession     0.1667    0.0588    0.0870        17
      Expansion.Conjunction     0.4589    0.4750    0.4668       200
    Expansion.Instantiation     0.6893    0.6017    0.6425       118
      Expansion.Restatement     0.4215    0.4455    0.4332       211
      Expansion.Alternative     0.1667    0.3333    0.2222         9
             Expansion.List     0.1818    0.1667    0.1739        12

                   accuracy                         0.4899      1039
                  macro avg     0.3400    0.3390    0.3331      1039
               weighted avg     0.4875    0.4899    0.4873      1039

Epoch [21/30]
top-down:TOP: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 61.55%, Val F1: 50.52% Time: 61.89592885971069 
top-down:SEC: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 43.09%, Val F1: 26.39% Time: 61.89592885971069 
top-down:CONN: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 71.88%,Val Loss:   8.4,  Val Acc: 24.12%, Val F1:  7.74% Time: 61.89592885971069 
 
 
top-down:TOP: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 60.77%, Val F1: 49.42% Time: 215.8237464427948 
top-down:SEC: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.4,  Val Acc: 43.78%, Val F1: 27.09% Time: 215.8237464427948 
top-down:CONN: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   8.4,  Val Acc: 23.78%, Val F1:  7.10% Time: 215.8237464427948 
 
 
top-down:TOP: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   8.3,  Val Acc: 61.55%, Val F1: 51.51% Time: 366.9087996482849 
top-down:SEC: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   8.3,  Val Acc: 44.21%, Val F1: 26.15% Time: 366.9087996482849 
top-down:CONN: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   8.3,  Val Acc: 25.24%, Val F1:  7.70% Time: 366.9087996482849 
 
 
top-down:TOP: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 61.46%, Val F1: 51.69% Time: 516.6935229301453 
top-down:SEC: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   8.2,  Val Acc: 43.95%, Val F1: 28.00% Time: 516.6935229301453 
top-down:CONN: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   8.2,  Val Acc: 23.69%, Val F1:  7.34% Time: 516.6935229301453 
 
 
Train time usage: 598.5737442970276
Test time usage: 1.7777705192565918
TOP: Test Loss:   7.3,  Test Acc: 64.68%, Test F1: 56.63%
SEC: Test Loss:   7.3,  Test Acc: 48.80%, Test F1: 33.17%
CONN: Test Loss:   7.3,  Test Acc: 26.18%, Test F1:  9.33%
consistency_top_sec: 47.64%,  consistency_sec_conn: 21.85%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.4559    0.4559    0.4559        68
 Contingency     0.5771    0.5309    0.5530       275
  Comparison     0.5620    0.4722    0.5132       144
   Expansion     0.7152    0.7736    0.7433       552

    accuracy                         0.6468      1039
   macro avg     0.5775    0.5581    0.5663      1039
weighted avg     0.6405    0.6468    0.6422      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5098    0.4815    0.4952        54
         Temporal.Synchrony     0.0714    0.0714    0.0714        14
          Contingency.Cause     0.5703    0.5428    0.5562       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4912    0.4375    0.4628       128
      Comparison.Concession     0.1111    0.0588    0.0769        17
      Expansion.Conjunction     0.4585    0.5250    0.4895       200
    Expansion.Instantiation     0.6063    0.6525    0.6286       118
      Expansion.Restatement     0.4300    0.4218    0.4258       211
      Expansion.Alternative     0.2000    0.4444    0.2759         9
             Expansion.List     0.1667    0.1667    0.1667        12

                   accuracy                         0.4880      1039
                  macro avg     0.3287    0.3457    0.3317      1039
               weighted avg     0.4855    0.4880    0.4854      1039

Epoch [22/30]
top-down:TOP: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.3,  Val Acc: 61.72%, Val F1: 52.02% Time: 71.70915865898132 
top-down:SEC: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   8.3,  Val Acc: 44.12%, Val F1: 27.42% Time: 71.70915865898132 
top-down:CONN: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   8.3,  Val Acc: 25.24%, Val F1:  7.64% Time: 71.70915865898132 
 
 
top-down:TOP: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.3,  Val Acc: 61.20%, Val F1: 50.82% Time: 222.17485785484314 
top-down:SEC: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.3,  Val Acc: 43.61%, Val F1: 26.59% Time: 222.17485785484314 
top-down:CONN: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   8.3,  Val Acc: 24.46%, Val F1:  7.48% Time: 222.17485785484314 
 
 
top-down:TOP: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 61.55%, Val F1: 49.81% Time: 378.54830265045166 
top-down:SEC: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   8.5,  Val Acc: 43.00%, Val F1: 26.21% Time: 378.54830265045166 
top-down:CONN: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   8.5,  Val Acc: 23.86%, Val F1:  7.43% Time: 378.54830265045166 
 
 
top-down:TOP: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.4,  Val Acc: 59.91%, Val F1: 51.16% Time: 530.1701509952545 
top-down:SEC: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.4,  Val Acc: 42.75%, Val F1: 26.05% Time: 530.1701509952545 
top-down:CONN: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   8.4,  Val Acc: 24.64%, Val F1:  7.78% Time: 530.1701509952545 
 
 
Train time usage: 604.1275534629822
Test time usage: 1.99601149559021
TOP: Test Loss:   7.5,  Test Acc: 63.91%, Test F1: 55.69%
SEC: Test Loss:   7.5,  Test Acc: 49.09%, Test F1: 34.41%
CONN: Test Loss:   7.5,  Test Acc: 25.51%, Test F1:  9.54%
consistency_top_sec: 48.22%,  consistency_sec_conn: 20.98%, consistency_top_sec_conn: 20.69%
              precision    recall  f1-score   support

    Temporal     0.4286    0.3971    0.4122        68
 Contingency     0.5752    0.5564    0.5656       275
  Comparison     0.5211    0.5139    0.5175       144
   Expansion     0.7218    0.7428    0.7321       552

    accuracy                         0.6391      1039
   macro avg     0.5617    0.5525    0.5569      1039
weighted avg     0.6360    0.6391    0.6374      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4694    0.4259    0.4466        54
         Temporal.Synchrony     0.1538    0.1429    0.1481        14
          Contingency.Cause     0.5655    0.5613    0.5634       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4737    0.4922    0.4828       128
      Comparison.Concession     0.1250    0.0588    0.0800        17
      Expansion.Conjunction     0.4512    0.4850    0.4675       200
    Expansion.Instantiation     0.6667    0.6271    0.6463       118
      Expansion.Restatement     0.4306    0.4408    0.4356       211
      Expansion.Alternative     0.2727    0.3333    0.3000         9
             Expansion.List     0.1875    0.2500    0.2143        12

                   accuracy                         0.4909      1039
                  macro avg     0.3451    0.3470    0.3441      1039
               weighted avg     0.4878    0.4909    0.4888      1039

Epoch [23/30]
top-down:TOP: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 61.46%, Val F1: 51.24% Time: 82.98742461204529 
top-down:SEC: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 42.92%, Val F1: 26.22% Time: 82.98742461204529 
top-down:CONN: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   8.4,  Val Acc: 24.29%, Val F1:  8.01% Time: 82.98742461204529 
 
 
top-down:TOP: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 61.46%, Val F1: 52.24% Time: 234.72799921035767 
top-down:SEC: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   8.4,  Val Acc: 44.12%, Val F1: 27.53% Time: 234.72799921035767 
top-down:CONN: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   8.4,  Val Acc: 24.72%, Val F1:  7.48% Time: 234.72799921035767 
 
 
top-down:TOP: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.4,  Val Acc: 62.15%, Val F1: 52.06% Time: 381.748774766922 
top-down:SEC: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.4,  Val Acc: 43.95%, Val F1: 28.43% Time: 381.748774766922 
top-down:CONN: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   8.4,  Val Acc: 24.21%, Val F1:  7.49% Time: 381.748774766922 
 
 
top-down:TOP: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 62.15%, Val F1: 52.59% Time: 536.5535337924957 
top-down:SEC: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.4,  Val Acc: 43.61%, Val F1: 27.21% Time: 536.5535337924957 
top-down:CONN: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   8.4,  Val Acc: 23.95%, Val F1:  7.42% Time: 536.5535337924957 
 
 
Train time usage: 599.0617084503174
Test time usage: 1.8433706760406494
TOP: Test Loss:   7.5,  Test Acc: 64.29%, Test F1: 56.01%
SEC: Test Loss:   7.5,  Test Acc: 49.57%, Test F1: 35.15%
CONN: Test Loss:   7.5,  Test Acc: 26.18%, Test F1:  9.58%
consistency_top_sec: 48.22%,  consistency_sec_conn: 21.94%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.4493    0.4559    0.4526        68
 Contingency     0.5800    0.5273    0.5524       275
  Comparison     0.5234    0.4653    0.4926       144
   Expansion     0.7179    0.7699    0.7430       552

    accuracy                         0.6429      1039
   macro avg     0.5677    0.5546    0.5601      1039
weighted avg     0.6369    0.6429    0.6388      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5094    0.5000    0.5047        54
         Temporal.Synchrony     0.3333    0.1429    0.2000        14
          Contingency.Cause     0.5676    0.5465    0.5568       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4836    0.4609    0.4720       128
      Comparison.Concession     0.1000    0.0588    0.0741        17
      Expansion.Conjunction     0.4569    0.5300    0.4907       200
    Expansion.Instantiation     0.6410    0.6356    0.6383       118
      Expansion.Restatement     0.4483    0.4313    0.4396       211
      Expansion.Alternative     0.2222    0.4444    0.2963         9
             Expansion.List     0.1579    0.2500    0.1935        12

                   accuracy                         0.4957      1039
                  macro avg     0.3564    0.3637    0.3515      1039
               weighted avg     0.4947    0.4957    0.4935      1039

Epoch [24/30]
top-down:TOP: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 61.97%, Val F1: 52.38% Time: 93.74135851860046 
top-down:SEC: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 43.61%, Val F1: 27.23% Time: 93.74135851860046 
top-down:CONN: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 68.75%,Val Loss:   8.5,  Val Acc: 23.61%, Val F1:  7.40% Time: 93.74135851860046 
 
 
top-down:TOP: Iter:   9200,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 60.94%, Val F1: 51.19% Time: 252.2204713821411 
top-down:SEC: Iter:   9200,  Train Loss: 2.3e+01,  Train Acc: 87.50%,Val Loss:   8.5,  Val Acc: 43.61%, Val F1: 27.71% Time: 252.2204713821411 
top-down:CONN: Iter:   9200,  Train Loss: 2.3e+01,  Train Acc: 62.50%,Val Loss:   8.5,  Val Acc: 23.78%, Val F1:  7.49% Time: 252.2204713821411 
 
 
top-down:TOP: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 61.80%, Val F1: 51.02% Time: 405.5763375759125 
top-down:SEC: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 43.95%, Val F1: 27.89% Time: 405.5763375759125 
top-down:CONN: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   8.5,  Val Acc: 24.03%, Val F1:  7.60% Time: 405.5763375759125 
 
 
top-down:TOP: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 62.49%, Val F1: 51.97% Time: 544.318829536438 
top-down:SEC: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.6,  Val Acc: 44.12%, Val F1: 27.77% Time: 544.318829536438 
top-down:CONN: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   8.6,  Val Acc: 23.78%, Val F1:  7.44% Time: 544.318829536438 
 
 
Train time usage: 591.3790366649628
Test time usage: 1.871295690536499
TOP: Test Loss:   7.5,  Test Acc: 64.20%, Test F1: 55.68%
SEC: Test Loss:   7.5,  Test Acc: 49.57%, Test F1: 33.62%
CONN: Test Loss:   7.5,  Test Acc: 26.47%, Test F1:  9.59%
consistency_top_sec: 47.83%,  consistency_sec_conn: 22.43%, consistency_top_sec_conn: 21.85%
              precision    recall  f1-score   support

    Temporal     0.4265    0.4265    0.4265        68
 Contingency     0.5924    0.5109    0.5486       276
  Comparison     0.5105    0.5069    0.5087       144
   Expansion     0.7186    0.7695    0.7432       551

    accuracy                         0.6420      1039
   macro avg     0.5620    0.5534    0.5568      1039
weighted avg     0.6371    0.6420    0.6383      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4545    0.4630    0.4587        54
         Temporal.Synchrony     0.1429    0.0714    0.0952        14
          Contingency.Cause     0.5926    0.5353    0.5625       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4773    0.4922    0.4846       128
      Comparison.Concession     0.1000    0.0588    0.0741        17
      Expansion.Conjunction     0.4735    0.5350    0.5023       200
    Expansion.Instantiation     0.6333    0.6441    0.6387       118
      Expansion.Restatement     0.4279    0.4360    0.4319       211
      Expansion.Alternative     0.2000    0.4444    0.2759         9
             Expansion.List     0.1818    0.1667    0.1739        12

                   accuracy                         0.4957      1039
                  macro avg     0.3349    0.3497    0.3362      1039
               weighted avg     0.4932    0.4957    0.4930      1039

Epoch [25/30]
top-down:TOP: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 61.55%, Val F1: 50.87% Time: 103.62164568901062 
top-down:SEC: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.5,  Val Acc: 44.29%, Val F1: 27.63% Time: 103.62164568901062 
top-down:CONN: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   8.5,  Val Acc: 23.43%, Val F1:  7.25% Time: 103.62164568901062 
 
 
top-down:TOP: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 60.34%, Val F1: 49.43% Time: 257.69806265830994 
top-down:SEC: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 43.35%, Val F1: 26.42% Time: 257.69806265830994 
top-down:CONN: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   8.6,  Val Acc: 23.61%, Val F1:  7.52% Time: 257.69806265830994 
 
 
top-down:TOP: Iter:   9700,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 60.52%, Val F1: 51.25% Time: 412.4377694129944 
top-down:SEC: Iter:   9700,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 44.03%, Val F1: 28.40% Time: 412.4377694129944 
top-down:CONN: Iter:   9700,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   8.6,  Val Acc: 24.12%, Val F1:  7.69% Time: 412.4377694129944 
 
 
top-down:TOP: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   8.5,  Val Acc: 60.60%, Val F1: 51.80% Time: 567.8956038951874 
top-down:SEC: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 43.09%, Val F1: 26.24% Time: 567.8956038951874 
top-down:CONN: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 84.38%,Val Loss:   8.5,  Val Acc: 23.86%, Val F1:  7.41% Time: 567.8956038951874 
 
 
Train time usage: 607.1165585517883
Test time usage: 2.0210766792297363
TOP: Test Loss:   7.6,  Test Acc: 63.91%, Test F1: 55.78%
SEC: Test Loss:   7.6,  Test Acc: 48.80%, Test F1: 33.87%
CONN: Test Loss:   7.6,  Test Acc: 24.93%, Test F1:  8.85%
consistency_top_sec: 47.16%,  consistency_sec_conn: 20.69%, consistency_top_sec_conn: 20.12%
              precision    recall  f1-score   support

    Temporal     0.4225    0.4412    0.4317        68
 Contingency     0.5974    0.5000    0.5444       276
  Comparison     0.5067    0.5278    0.5170       144
   Expansion     0.7155    0.7623    0.7381       551

    accuracy                         0.6391      1039
   macro avg     0.5605    0.5578    0.5578      1039
weighted avg     0.6360    0.6391    0.6360      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4576    0.5000    0.4779        54
         Temporal.Synchrony     0.1250    0.0714    0.0909        14
          Contingency.Cause     0.5897    0.5130    0.5487       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4926    0.5234    0.5076       128
      Comparison.Concession     0.1000    0.0588    0.0741        17
      Expansion.Conjunction     0.4372    0.5400    0.4832       200
    Expansion.Instantiation     0.6303    0.6356    0.6329       118
      Expansion.Restatement     0.4346    0.3934    0.4129       211
      Expansion.Alternative     0.1905    0.4444    0.2667         9
             Expansion.List     0.2143    0.2500    0.2308        12

                   accuracy                         0.4880      1039
                  macro avg     0.3338    0.3573    0.3387      1039
               weighted avg     0.4886    0.4880    0.4856      1039

Epoch [26/30]
top-down:TOP: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.5,  Val Acc: 62.32%, Val F1: 51.57% Time: 111.57871413230896 
top-down:SEC: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   8.5,  Val Acc: 44.12%, Val F1: 27.86% Time: 111.57871413230896 
top-down:CONN: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   8.5,  Val Acc: 23.95%, Val F1:  7.68% Time: 111.57871413230896 
 
 
top-down:TOP: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 60.86%, Val F1: 50.71% Time: 258.05910634994507 
top-down:SEC: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 44.12%, Val F1: 27.79% Time: 258.05910634994507 
top-down:CONN: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   8.6,  Val Acc: 24.29%, Val F1:  7.77% Time: 258.05910634994507 
 
 
top-down:TOP: Iter:  10100,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 61.55%, Val F1: 51.61% Time: 414.42099499702454 
top-down:SEC: Iter:  10100,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 44.21%, Val F1: 28.00% Time: 414.42099499702454 
top-down:CONN: Iter:  10100,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   8.6,  Val Acc: 23.69%, Val F1:  7.47% Time: 414.42099499702454 
 
 
top-down:TOP: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 61.63%, Val F1: 51.50% Time: 567.0809979438782 
top-down:SEC: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 43.52%, Val F1: 27.66% Time: 567.0809979438782 
top-down:CONN: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   8.6,  Val Acc: 23.95%, Val F1:  7.91% Time: 567.0809979438782 
 
 
Train time usage: 594.4250209331512
Test time usage: 2.03406023979187
TOP: Test Loss:   7.6,  Test Acc: 64.10%, Test F1: 56.12%
SEC: Test Loss:   7.6,  Test Acc: 49.47%, Test F1: 33.96%
CONN: Test Loss:   7.6,  Test Acc: 25.31%, Test F1:  9.16%
consistency_top_sec: 47.55%,  consistency_sec_conn: 20.31%, consistency_top_sec_conn: 19.83%
              precision    recall  f1-score   support

    Temporal     0.4603    0.4265    0.4427        68
 Contingency     0.5743    0.5200    0.5458       275
  Comparison     0.5414    0.5000    0.5199       144
   Expansion     0.7104    0.7645    0.7365       552

    accuracy                         0.6410      1039
   macro avg     0.5716    0.5527    0.5612      1039
weighted avg     0.6346    0.6410    0.6368      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4490    0.4074    0.4272        54
         Temporal.Synchrony     0.2222    0.1429    0.1739        14
          Contingency.Cause     0.5827    0.5502    0.5660       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4958    0.4609    0.4777       128
      Comparison.Concession     0.0714    0.0588    0.0645        17
      Expansion.Conjunction     0.4612    0.5350    0.4954       200
    Expansion.Instantiation     0.6325    0.6271    0.6298       118
      Expansion.Restatement     0.4460    0.4502    0.4481       211
      Expansion.Alternative     0.1765    0.3333    0.2308         9
             Expansion.List     0.2000    0.2500    0.2222        12

                   accuracy                         0.4947      1039
                  macro avg     0.3398    0.3469    0.3396      1039
               weighted avg     0.4945    0.4947    0.4934      1039

Epoch [27/30]
top-down:TOP: Iter:  10300,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 61.55%, Val F1: 51.43% Time: 122.63079953193665 
top-down:SEC: Iter:  10300,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 44.38%, Val F1: 28.21% Time: 122.63079953193665 
top-down:CONN: Iter:  10300,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   8.6,  Val Acc: 23.61%, Val F1:  7.33% Time: 122.63079953193665 
 
 
top-down:TOP: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 61.72%, Val F1: 52.47% Time: 277.380676984787 
top-down:SEC: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 43.95%, Val F1: 28.27% Time: 277.380676984787 
top-down:CONN: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   8.6,  Val Acc: 23.69%, Val F1:  7.78% Time: 277.380676984787 
 
 
top-down:TOP: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   8.6,  Val Acc: 62.58%, Val F1: 53.23% Time: 420.71340775489807 
top-down:SEC: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 43.86%, Val F1: 28.10% Time: 420.71340775489807 
top-down:CONN: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   8.6,  Val Acc: 24.55%, Val F1:  7.85% Time: 420.71340775489807 
 
 
top-down:TOP: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 61.89%, Val F1: 53.15% Time: 568.680816411972 
top-down:SEC: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   8.6,  Val Acc: 44.03%, Val F1: 28.05% Time: 568.680816411972 
top-down:CONN: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 75.00%,Val Loss:   8.6,  Val Acc: 23.86%, Val F1:  7.66% Time: 568.680816411972 
 
 
Train time usage: 589.0246486663818
Test time usage: 2.1355507373809814
TOP: Test Loss:   7.7,  Test Acc: 64.68%, Test F1: 56.44%
SEC: Test Loss:   7.7,  Test Acc: 49.86%, Test F1: 34.40%
CONN: Test Loss:   7.7,  Test Acc: 24.74%, Test F1:  9.06%
consistency_top_sec: 48.51%,  consistency_sec_conn: 20.12%, consistency_top_sec_conn: 19.83%
              precision    recall  f1-score   support

    Temporal     0.4531    0.4265    0.4394        68
 Contingency     0.5894    0.5273    0.5566       275
  Comparison     0.5504    0.4931    0.5201       144
   Expansion     0.7117    0.7736    0.7413       552

    accuracy                         0.6468      1039
   macro avg     0.5762    0.5551    0.5644      1039
weighted avg     0.6400    0.6468    0.6420      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4407    0.4815    0.4602        54
         Temporal.Synchrony     0.1667    0.0714    0.1000        14
          Contingency.Cause     0.5817    0.5428    0.5615       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5128    0.4688    0.4898       128
      Comparison.Concession     0.0714    0.0588    0.0645        17
      Expansion.Conjunction     0.4641    0.5500    0.5034       200
    Expansion.Instantiation     0.6311    0.6525    0.6417       118
      Expansion.Restatement     0.4478    0.4265    0.4369       211
      Expansion.Alternative     0.2105    0.4444    0.2857         9
             Expansion.List     0.2308    0.2500    0.2400        12

                   accuracy                         0.4986      1039
                  macro avg     0.3416    0.3588    0.3440      1039
               weighted avg     0.4965    0.4986    0.4958      1039

Epoch [28/30]
top-down:TOP: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 62.92%, Val F1: 52.93% Time: 144.1929886341095 
top-down:SEC: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 44.64%, Val F1: 28.52% Time: 144.1929886341095 
top-down:CONN: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   8.6,  Val Acc: 23.18%, Val F1:  7.53% Time: 144.1929886341095 
 
 
top-down:TOP: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.6,  Val Acc: 62.23%, Val F1: 53.43% Time: 300.9484372138977 
top-down:SEC: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.6,  Val Acc: 43.95%, Val F1: 27.92% Time: 300.9484372138977 
top-down:CONN: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   8.6,  Val Acc: 23.86%, Val F1:  7.70% Time: 300.9484372138977 
 
 
top-down:TOP: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.7,  Val Acc: 61.20%, Val F1: 52.32% Time: 459.0304672718048 
top-down:SEC: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.7,  Val Acc: 43.86%, Val F1: 28.15% Time: 459.0304672718048 
top-down:CONN: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   8.7,  Val Acc: 23.18%, Val F1:  7.28% Time: 459.0304672718048 
 
 
top-down:TOP: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   8.7,  Val Acc: 61.37%, Val F1: 52.14% Time: 603.0407192707062 
top-down:SEC: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   8.7,  Val Acc: 43.95%, Val F1: 28.35% Time: 603.0407192707062 
top-down:CONN: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 75.00%,Val Loss:   8.7,  Val Acc: 23.86%, Val F1:  7.71% Time: 603.0407192707062 
 
 
Train time usage: 613.0992560386658
Test time usage: 1.8883025646209717
TOP: Test Loss:   7.7,  Test Acc: 63.72%, Test F1: 55.77%
SEC: Test Loss:   7.7,  Test Acc: 48.41%, Test F1: 33.06%
CONN: Test Loss:   7.7,  Test Acc: 25.79%, Test F1:  9.56%
consistency_top_sec: 47.16%,  consistency_sec_conn: 20.69%, consistency_top_sec_conn: 20.40%
              precision    recall  f1-score   support

    Temporal     0.4051    0.4706    0.4354        68
 Contingency     0.5885    0.5200    0.5521       275
  Comparison     0.5180    0.5000    0.5088       144
   Expansion     0.7180    0.7518    0.7345       552

    accuracy                         0.6372      1039
   macro avg     0.5574    0.5606    0.5577      1039
weighted avg     0.6355    0.6372    0.6354      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4265    0.5370    0.4754        54
         Temporal.Synchrony     0.0909    0.0714    0.0800        14
          Contingency.Cause     0.5844    0.5279    0.5547       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4797    0.4609    0.4701       128
      Comparison.Concession     0.0588    0.0588    0.0588        17
      Expansion.Conjunction     0.4633    0.5050    0.4833       200
    Expansion.Instantiation     0.6239    0.6186    0.6213       118
      Expansion.Restatement     0.4313    0.4313    0.4313       211
      Expansion.Alternative     0.1667    0.3333    0.2222         9
             Expansion.List     0.2308    0.2500    0.2400        12

                   accuracy                         0.4841      1039
                  macro avg     0.3233    0.3449    0.3306      1039
               weighted avg     0.4865    0.4841    0.4841      1039

Epoch [29/30]
top-down:TOP: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 100.00%,Val Loss:   8.7,  Val Acc: 61.63%, Val F1: 52.09% Time: 148.5851399898529 
top-down:SEC: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 100.00%,Val Loss:   8.7,  Val Acc: 44.46%, Val F1: 28.66% Time: 148.5851399898529 
top-down:CONN: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 84.38%,Val Loss:   8.7,  Val Acc: 23.95%, Val F1:  7.68% Time: 148.5851399898529 
 
 
top-down:TOP: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.7,  Val Acc: 61.80%, Val F1: 52.46% Time: 298.05640387535095 
top-down:SEC: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   8.7,  Val Acc: 43.95%, Val F1: 28.21% Time: 298.05640387535095 
top-down:CONN: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   8.7,  Val Acc: 23.78%, Val F1:  7.56% Time: 298.05640387535095 
 
 
top-down:TOP: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   8.7,  Val Acc: 62.15%, Val F1: 52.88% Time: 457.6004033088684 
top-down:SEC: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   8.7,  Val Acc: 44.46%, Val F1: 28.61% Time: 457.6004033088684 
top-down:CONN: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   8.7,  Val Acc: 23.43%, Val F1:  7.60% Time: 457.6004033088684 
 
 
Train time usage: 609.0625138282776
Test time usage: 1.858224630355835
TOP: Test Loss:   7.7,  Test Acc: 64.39%, Test F1: 56.72%
SEC: Test Loss:   7.7,  Test Acc: 48.99%, Test F1: 33.63%
CONN: Test Loss:   7.7,  Test Acc: 25.02%, Test F1:  9.17%
consistency_top_sec: 47.45%,  consistency_sec_conn: 20.69%, consistency_top_sec_conn: 20.21%
              precision    recall  f1-score   support

    Temporal     0.4507    0.4706    0.4604        68
 Contingency     0.5902    0.5236    0.5549       275
  Comparison     0.5214    0.5069    0.5141       144
   Expansion     0.7192    0.7609    0.7394       552

    accuracy                         0.6439      1039
   macro avg     0.5704    0.5655    0.5672      1039
weighted avg     0.6401    0.6439    0.6411      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4746    0.5185    0.4956        54
         Temporal.Synchrony     0.1250    0.0714    0.0909        14
          Contingency.Cause     0.5840    0.5428    0.5626       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4878    0.4688    0.4781       128
      Comparison.Concession     0.0625    0.0588    0.0606        17
      Expansion.Conjunction     0.4602    0.5200    0.4883       200
    Expansion.Instantiation     0.6271    0.6271    0.6271       118
      Expansion.Restatement     0.4258    0.4218    0.4238       211
      Expansion.Alternative     0.1667    0.3333    0.2222         9
             Expansion.List     0.2500    0.2500    0.2500        12

                   accuracy                         0.4899      1039
                  macro avg     0.3331    0.3466    0.3363      1039
               weighted avg     0.4893    0.4899    0.4886      1039

Epoch [30/30]
top-down:TOP: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.7,  Val Acc: 61.89%, Val F1: 52.63% Time: 6.751543045043945 
top-down:SEC: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   8.7,  Val Acc: 43.86%, Val F1: 28.25% Time: 6.751543045043945 
top-down:CONN: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   8.7,  Val Acc: 23.78%, Val F1:  7.67% Time: 6.751543045043945 
 
 
top-down:TOP: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.7,  Val Acc: 61.89%, Val F1: 52.02% Time: 157.03416061401367 
top-down:SEC: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.7,  Val Acc: 44.38%, Val F1: 28.36% Time: 157.03416061401367 
top-down:CONN: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   8.7,  Val Acc: 23.61%, Val F1:  7.59% Time: 157.03416061401367 
 
 
top-down:TOP: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   8.7,  Val Acc: 61.55%, Val F1: 51.41% Time: 326.6907408237457 
top-down:SEC: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   8.7,  Val Acc: 44.29%, Val F1: 28.41% Time: 326.6907408237457 
top-down:CONN: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   8.7,  Val Acc: 23.52%, Val F1:  7.55% Time: 326.6907408237457 
 
 
top-down:TOP: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.7,  Val Acc: 62.15%, Val F1: 52.08% Time: 490.009642124176 
top-down:SEC: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.7,  Val Acc: 44.46%, Val F1: 28.47% Time: 490.009642124176 
top-down:CONN: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   8.7,  Val Acc: 23.61%, Val F1:  7.61% Time: 490.009642124176 
 
 
No optimization for a long time, auto-stopping...
dev_best_acc_top: 63.43%,  dev_best_f1_top: 53.46%, 
dev_best_acc_sec: 49.53%,  dev_best_f1_sec: 30.53%, 
dev_best_acc_conn: 27.81%,  dev_best_f1_conn:  5.92%
