nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'pdtb_it/data/', 'log_file': 'pdtb_it/log/', 'save_file': 'pdtb_it/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'February27-14:06:08', 'log': 'pdtb_it/log/February27-14:06:08.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]86it [00:00, 858.96it/s]278it [00:00, 1482.37it/s]461it [00:00, 1638.59it/s]656it [00:00, 1761.34it/s]859it [00:00, 1857.63it/s]1050it [00:00, 1874.96it/s]1244it [00:00, 1895.89it/s]1440it [00:00, 1916.03it/s]1632it [00:00, 1840.68it/s]1817it [00:01, 1842.91it/s]2011it [00:01, 1871.19it/s]2199it [00:01, 1851.67it/s]2385it [00:01, 1824.28it/s]2571it [00:01, 1829.20it/s]2765it [00:01, 1859.66it/s]2952it [00:01, 1845.43it/s]3139it [00:01, 1852.13it/s]3325it [00:01, 1828.69it/s]3508it [00:01, 1820.83it/s]3699it [00:02, 1845.14it/s]3884it [00:02, 1756.16it/s]4077it [00:02, 1804.99it/s]4259it [00:02, 1758.92it/s]4436it [00:02, 1755.30it/s]4615it [00:02, 1761.99it/s]4804it [00:02, 1798.32it/s]4985it [00:02, 1756.00it/s]5167it [00:02, 1773.63it/s]5345it [00:03, 1201.20it/s]5542it [00:03, 1370.02it/s]5703it [00:03, 1421.87it/s]5907it [00:03, 1577.08it/s]6093it [00:03, 1650.46it/s]6270it [00:03, 1676.32it/s]6452it [00:03, 1715.75it/s]6630it [00:03, 1733.80it/s]6808it [00:03, 1712.07it/s]6995it [00:04, 1753.88it/s]7178it [00:04, 1773.83it/s]7362it [00:04, 1791.41it/s]7543it [00:04, 1790.74it/s]7728it [00:04, 1806.34it/s]7915it [00:04, 1823.99it/s]8098it [00:04, 1789.79it/s]8291it [00:04, 1827.21it/s]8475it [00:04, 1811.87it/s]8659it [00:04, 1818.86it/s]8844it [00:05, 1827.48it/s]9029it [00:05, 1831.26it/s]9218it [00:05, 1847.36it/s]9403it [00:05, 1777.92it/s]9594it [00:05, 1814.24it/s]9790it [00:05, 1856.79it/s]9979it [00:05, 1866.27it/s]10166it [00:05, 1841.46it/s]10355it [00:05, 1853.68it/s]10554it [00:05, 1891.69it/s]10744it [00:06, 1846.82it/s]10933it [00:06, 1858.55it/s]11125it [00:06, 1874.34it/s]11313it [00:06, 1874.35it/s]11503it [00:06, 1880.70it/s]11692it [00:06, 1859.39it/s]11879it [00:06, 1861.72it/s]12066it [00:06, 1789.12it/s]12246it [00:06, 1782.32it/s]12425it [00:07, 1754.37it/s]12547it [00:07, 1769.30it/s]
0it [00:00, ?it/s]173it [00:00, 1719.07it/s]359it [00:00, 1801.15it/s]542it [00:00, 1811.51it/s]724it [00:00, 1725.90it/s]898it [00:00, 1714.15it/s]1073it [00:00, 1723.19it/s]1165it [00:00, 1732.05it/s]
0it [00:00, ?it/s]171it [00:00, 1703.90it/s]367it [00:00, 1852.28it/s]561it [00:00, 1886.85it/s]750it [00:00, 1851.27it/s]936it [00:00, 1799.27it/s]1039it [00:00, 1831.66it/s]
Time usage: 19.636295080184937
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.8,  Val Acc: 55.88%, Val F1: 17.92% Time: 107.41743469238281 *
top-down:SEC: Iter:    100,  Train Loss: 3e+01,  Train Acc: 18.75%,Val Loss:   6.8,  Val Acc: 24.21%, Val F1:  3.54% Time: 107.41743469238281 *
top-down:CONN: Iter:    100,  Train Loss: 3e+01,  Train Acc:  3.12%,Val Loss:   6.8,  Val Acc: 12.88%, Val F1:  0.34% Time: 107.41743469238281 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   6.4,  Val Acc: 52.53%, Val F1: 22.51% Time: 188.1639974117279 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.4,  Val Acc: 29.96%, Val F1:  7.37% Time: 188.1639974117279 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.4,  Val Acc: 14.51%, Val F1:  0.71% Time: 188.1639974117279 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 78.12%,Val Loss:   6.2,  Val Acc: 55.88%, Val F1: 17.92% Time: 272.59549736976624 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 31.25%,Val Loss:   6.2,  Val Acc: 34.33%, Val F1: 11.19% Time: 272.59549736976624 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 15.62%,Val Loss:   6.2,  Val Acc: 17.42%, Val F1:  1.24% Time: 272.59549736976624 *
 
 
Train time usage: 348.5226950645447
Test time usage: 1.7239587306976318
TOP: Test Loss:   5.7,  Test Acc: 58.42%, Test F1: 36.53%
SEC: Test Loss:   5.7,  Test Acc: 45.43%, Test F1: 21.99%
CONN: Test Loss:   5.7,  Test Acc: 19.83%, Test F1:  2.46%
consistency_top_sec: 35.61%,  consistency_sec_conn: 16.75%, consistency_top_sec_conn: 12.80%
              precision    recall  f1-score   support

    Temporal     0.5556    0.1471    0.2326        68
 Contingency     0.5385    0.2316    0.3239       272
  Comparison     0.7895    0.1042    0.1840       144
   Expansion     0.5864    0.9351    0.7208       555

    accuracy                         0.5842      1039
   macro avg     0.6175    0.3545    0.3653      1039
weighted avg     0.6000    0.5842    0.5106      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5185    0.2593    0.3457        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5254    0.5805    0.5516       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5833    0.1094    0.1842       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4751    0.6200    0.5380       200
    Expansion.Instantiation     0.9615    0.2119    0.3472       118
      Expansion.Restatement     0.3448    0.6573    0.4523       213
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4543      1039
                  macro avg     0.3099    0.2217    0.2199      1039
               weighted avg     0.5052    0.4543    0.4181      1039

Epoch [2/15]
top-down:TOP: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.9,  Val Acc: 56.82%, Val F1: 41.41% Time: 8.722891807556152 *
top-down:SEC: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   5.9,  Val Acc: 40.26%, Val F1: 18.70% Time: 8.722891807556152 *
top-down:CONN: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 15.62%,Val Loss:   5.9,  Val Acc: 19.91%, Val F1:  2.51% Time: 8.722891807556152 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   5.7,  Val Acc: 56.39%, Val F1: 42.47% Time: 91.85667586326599 *
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 43.75%,Val Loss:   5.7,  Val Acc: 43.26%, Val F1: 21.71% Time: 91.85667586326599 *
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 37.50%,Val Loss:   5.7,  Val Acc: 23.61%, Val F1:  3.48% Time: 91.85667586326599 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   5.6,  Val Acc: 58.63%, Val F1: 44.61% Time: 178.5134027004242 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   5.6,  Val Acc: 45.67%, Val F1: 23.58% Time: 178.5134027004242 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 24.64%, Val F1:  3.78% Time: 178.5134027004242 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 59.83%, Val F1: 44.99% Time: 262.2251844406128 
top-down:SEC: Iter:    700,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 44.89%, Val F1: 22.35% Time: 262.2251844406128 
top-down:CONN: Iter:    700,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 24.29%, Val F1:  4.22% Time: 262.2251844406128 
 
 
Train time usage: 327.45371985435486
Test time usage: 1.7321171760559082
TOP: Test Loss:   5.0,  Test Acc: 65.74%, Test F1: 55.60%
SEC: Test Loss:   5.0,  Test Acc: 51.20%, Test F1: 28.80%
CONN: Test Loss:   5.0,  Test Acc: 26.47%, Test F1:  5.60%
consistency_top_sec: 45.81%,  consistency_sec_conn: 22.04%, consistency_top_sec_conn: 19.92%
              precision    recall  f1-score   support

    Temporal     0.5192    0.3971    0.4500        68
 Contingency     0.6412    0.3964    0.4899       275
  Comparison     0.6337    0.4444    0.5224       144
   Expansion     0.6746    0.8750    0.7618       552

    accuracy                         0.6574      1039
   macro avg     0.6172    0.5282    0.5560      1039
weighted avg     0.6499    0.6574    0.6363      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4737    0.5000    0.4865        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5672    0.5651    0.5661       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5246    0.5000    0.5120       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4867    0.5500    0.5164       200
    Expansion.Instantiation     0.6348    0.6186    0.6266       118
      Expansion.Restatement     0.4257    0.5024    0.4609       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5120      1039
                  macro avg     0.2830    0.2942    0.2880      1039
               weighted avg     0.4883    0.5120    0.4991      1039

Epoch [3/15]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   5.6,  Val Acc: 57.00%, Val F1: 49.06% Time: 15.805705070495605 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   5.6,  Val Acc: 45.24%, Val F1: 26.60% Time: 15.805705070495605 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 21.88%,Val Loss:   5.6,  Val Acc: 23.43%, Val F1:  4.75% Time: 15.805705070495605 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 58.54%, Val F1: 47.97% Time: 95.33147025108337 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 45.49%, Val F1: 26.65% Time: 95.33147025108337 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 21.88%,Val Loss:   5.5,  Val Acc: 26.35%, Val F1:  5.05% Time: 95.33147025108337 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   5.4,  Val Acc: 61.80%, Val F1: 49.03% Time: 185.787611246109 *
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   5.4,  Val Acc: 48.84%, Val F1: 25.57% Time: 185.787611246109 *
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   5.4,  Val Acc: 27.04%, Val F1:  5.30% Time: 185.787611246109 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 63.61%, Val F1: 51.54% Time: 267.217613697052 *
top-down:SEC: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 48.24%, Val F1: 30.01% Time: 267.217613697052 *
top-down:CONN: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   5.4,  Val Acc: 26.87%, Val F1:  6.19% Time: 267.217613697052 *
 
 
Train time usage: 335.85371494293213
Test time usage: 3.667591094970703
TOP: Test Loss:   5.1,  Test Acc: 64.49%, Test F1: 55.47%
SEC: Test Loss:   5.1,  Test Acc: 49.66%, Test F1: 29.77%
CONN: Test Loss:   5.1,  Test Acc: 24.45%, Test F1:  6.02%
consistency_top_sec: 46.20%,  consistency_sec_conn: 20.31%, consistency_top_sec_conn: 18.77%
              precision    recall  f1-score   support

    Temporal     0.4828    0.4118    0.4444        68
 Contingency     0.6250    0.4364    0.5139       275
  Comparison     0.5379    0.4931    0.5145       144
   Expansion     0.6865    0.8170    0.7461       552

    accuracy                         0.6449      1039
   macro avg     0.5830    0.5396    0.5547      1039
weighted avg     0.6363    0.6449    0.6328      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4262    0.4815    0.4522        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5598    0.5390    0.5492       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4255    0.4688    0.4461       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4488    0.6350    0.5259       200
    Expansion.Instantiation     0.6636    0.6017    0.6311       118
      Expansion.Restatement     0.4722    0.4028    0.4348       211
      Expansion.Alternative     0.2500    0.2222    0.2353         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.2951    0.3046    0.2977      1039
               weighted avg     0.4793    0.4966    0.4839      1039

Epoch [4/15]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 75.00%,Val Loss:   5.4,  Val Acc: 62.23%, Val F1: 50.57% Time: 22.715972423553467 
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 47.21%, Val F1: 27.79% Time: 22.715972423553467 
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 26.87%, Val F1:  6.18% Time: 22.715972423553467 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 62.32%, Val F1: 50.54% Time: 113.15078353881836 
top-down:SEC: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 53.12%,Val Loss:   5.5,  Val Acc: 46.78%, Val F1: 29.25% Time: 113.15078353881836 
top-down:CONN: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 26.70%, Val F1:  6.22% Time: 113.15078353881836 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 60.26%, Val F1: 50.51% Time: 189.35262823104858 
top-down:SEC: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 46.95%, Val F1: 28.96% Time: 189.35262823104858 
top-down:CONN: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 27.55%, Val F1:  6.14% Time: 189.35262823104858 
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 60.86%, Val F1: 51.46% Time: 276.50333881378174 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 47.64%, Val F1: 30.59% Time: 276.50333881378174 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 37.50%,Val Loss:   5.6,  Val Acc: 26.27%, Val F1:  6.28% Time: 276.50333881378174 
 
 
Train time usage: 331.07500290870667
Test time usage: 1.732851505279541
TOP: Test Loss:   4.9,  Test Acc: 64.97%, Test F1: 56.59%
SEC: Test Loss:   4.9,  Test Acc: 52.94%, Test F1: 34.31%
CONN: Test Loss:   4.9,  Test Acc: 27.43%, Test F1:  7.70%
consistency_top_sec: 49.95%,  consistency_sec_conn: 23.58%, consistency_top_sec_conn: 22.23%
              precision    recall  f1-score   support

    Temporal     0.5897    0.3382    0.4299        68
 Contingency     0.5992    0.5254    0.5598       276
  Comparison     0.4970    0.5694    0.5307       144
   Expansion     0.7167    0.7713    0.7430       551

    accuracy                         0.6497      1039
   macro avg     0.6006    0.5511    0.5659      1039
weighted avg     0.6467    0.6497    0.6444      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5745    0.5000    0.5347        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5567    0.5836    0.5699       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4371    0.5703    0.4949       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5435    0.5000    0.5208       200
    Expansion.Instantiation     0.7282    0.6356    0.6787       118
      Expansion.Restatement     0.4669    0.5355    0.4989       211
      Expansion.Alternative     0.4167    0.5556    0.4762         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5294      1039
                  macro avg     0.3385    0.3528    0.3431      1039
               weighted avg     0.5136    0.5294    0.5191      1039

Epoch [5/15]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 63.18%, Val F1: 53.45% Time: 24.311124086380005 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 47.98%, Val F1: 30.02% Time: 24.311124086380005 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 27.30%, Val F1:  6.06% Time: 24.311124086380005 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   5.7,  Val Acc: 61.89%, Val F1: 53.52% Time: 100.4173846244812 
top-down:SEC: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 47.73%, Val F1: 31.53% Time: 100.4173846244812 
top-down:CONN: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   5.7,  Val Acc: 26.44%, Val F1:  5.85% Time: 100.4173846244812 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 60.34%, Val F1: 51.52% Time: 176.65768575668335 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.8,  Val Acc: 46.61%, Val F1: 28.99% Time: 176.65768575668335 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.8,  Val Acc: 25.15%, Val F1:  5.49% Time: 176.65768575668335 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 60.09%, Val F1: 52.74% Time: 253.7260296344757 
top-down:SEC: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   5.7,  Val Acc: 46.78%, Val F1: 29.64% Time: 253.7260296344757 
top-down:CONN: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   5.7,  Val Acc: 27.98%, Val F1:  6.19% Time: 253.7260296344757 
 
 
Train time usage: 303.2437651157379
Test time usage: 1.659498691558838
TOP: Test Loss:   5.2,  Test Acc: 64.20%, Test F1: 55.63%
SEC: Test Loss:   5.2,  Test Acc: 51.11%, Test F1: 31.66%
CONN: Test Loss:   5.2,  Test Acc: 26.85%, Test F1:  8.60%
consistency_top_sec: 48.32%,  consistency_sec_conn: 22.23%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.5333    0.3529    0.4248        68
 Contingency     0.6065    0.4746    0.5325       276
  Comparison     0.4722    0.5903    0.5247       144
   Expansion     0.7140    0.7750    0.7433       551

    accuracy                         0.6420      1039
   macro avg     0.5815    0.5482    0.5563      1039
weighted avg     0.6401    0.6420    0.6361      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4583    0.4074    0.4314        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5573    0.5428    0.5499       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4144    0.5859    0.4854       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4955    0.5550    0.5236       200
    Expansion.Instantiation     0.7009    0.6356    0.6667       118
      Expansion.Restatement     0.4925    0.4645    0.4780       211
      Expansion.Alternative     0.2857    0.4444    0.3478         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5111      1039
                  macro avg     0.3095    0.3305    0.3166      1039
               weighted avg     0.4966    0.5111    0.5012      1039

Epoch [6/15]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 60.69%, Val F1: 52.31% Time: 35.63530421257019 
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   5.8,  Val Acc: 46.87%, Val F1: 29.32% Time: 35.63530421257019 
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   5.8,  Val Acc: 28.15%, Val F1:  7.03% Time: 35.63530421257019 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 61.37%, Val F1: 53.23% Time: 116.8310067653656 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   5.8,  Val Acc: 47.98%, Val F1: 30.32% Time: 116.8310067653656 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   5.8,  Val Acc: 27.90%, Val F1:  6.94% Time: 116.8310067653656 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 60.52%, Val F1: 52.33% Time: 196.22798943519592 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 46.87%, Val F1: 31.16% Time: 196.22798943519592 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.8,  Val Acc: 27.38%, Val F1:  6.95% Time: 196.22798943519592 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.5e+01,  Train Acc: 90.62%,Val Loss:   5.9,  Val Acc: 63.26%, Val F1: 54.53% Time: 289.19066429138184 *
top-down:SEC: Iter:   2300,  Train Loss: 3.5e+01,  Train Acc: 84.38%,Val Loss:   5.9,  Val Acc: 46.27%, Val F1: 30.31% Time: 289.19066429138184 *
top-down:CONN: Iter:   2300,  Train Loss: 3.5e+01,  Train Acc: 37.50%,Val Loss:   5.9,  Val Acc: 26.95%, Val F1:  6.99% Time: 289.19066429138184 *
 
 
Train time usage: 338.32262325286865
Test time usage: 1.7399623394012451
TOP: Test Loss:   5.2,  Test Acc: 64.49%, Test F1: 58.05%
SEC: Test Loss:   5.2,  Test Acc: 52.84%, Test F1: 33.91%
CONN: Test Loss:   5.2,  Test Acc: 28.01%, Test F1:  8.84%
consistency_top_sec: 50.53%,  consistency_sec_conn: 23.48%, consistency_top_sec_conn: 23.00%
              precision    recall  f1-score   support

    Temporal     0.5246    0.4706    0.4961        68
 Contingency     0.5885    0.5543    0.5709       276
  Comparison     0.4880    0.5625    0.5226       144
   Expansion     0.7319    0.7332    0.7325       551

    accuracy                         0.6449      1039
   macro avg     0.5832    0.5802    0.5805      1039
weighted avg     0.6464    0.6449    0.6450      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5179    0.5370    0.5273        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5634    0.5948    0.5787       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4260    0.5625    0.4848       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5211    0.4950    0.5077       200
    Expansion.Instantiation     0.7358    0.6610    0.6964       118
      Expansion.Restatement     0.4977    0.5024    0.5000       211
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5284      1039
                  macro avg     0.3290    0.3553    0.3391      1039
               weighted avg     0.5133    0.5284    0.5191      1039

Epoch [7/15]
top-down:TOP: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   6.0,  Val Acc: 61.80%, Val F1: 52.45% Time: 36.66808748245239 
top-down:SEC: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   6.0,  Val Acc: 45.75%, Val F1: 29.71% Time: 36.66808748245239 
top-down:CONN: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 37.50%,Val Loss:   6.0,  Val Acc: 25.92%, Val F1:  6.48% Time: 36.66808748245239 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.0,  Val Acc: 62.40%, Val F1: 52.82% Time: 124.72876572608948 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   6.0,  Val Acc: 46.44%, Val F1: 30.43% Time: 124.72876572608948 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   6.0,  Val Acc: 25.24%, Val F1:  6.48% Time: 124.72876572608948 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 61.37%, Val F1: 52.69% Time: 207.86855340003967 
top-down:SEC: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 65.62%,Val Loss:   6.1,  Val Acc: 46.95%, Val F1: 29.80% Time: 207.86855340003967 
top-down:CONN: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   6.1,  Val Acc: 25.84%, Val F1:  6.47% Time: 207.86855340003967 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 87.50%,Val Loss:   6.0,  Val Acc: 61.55%, Val F1: 52.51% Time: 290.68055295944214 
top-down:SEC: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 46.18%, Val F1: 28.95% Time: 290.68055295944214 
top-down:CONN: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   6.0,  Val Acc: 25.75%, Val F1:  6.82% Time: 290.68055295944214 
 
 
Train time usage: 332.12997794151306
Test time usage: 1.7475874423980713
TOP: Test Loss:   5.4,  Test Acc: 63.33%, Test F1: 57.27%
SEC: Test Loss:   5.4,  Test Acc: 50.91%, Test F1: 33.29%
CONN: Test Loss:   5.4,  Test Acc: 27.33%, Test F1:  8.83%
consistency_top_sec: 49.37%,  consistency_sec_conn: 22.71%, consistency_top_sec_conn: 22.23%
              precision    recall  f1-score   support

    Temporal     0.5517    0.4706    0.5079        68
 Contingency     0.5512    0.5652    0.5581       276
  Comparison     0.5143    0.5000    0.5070       144
   Expansion     0.7133    0.7223    0.7178       551

    accuracy                         0.6333      1039
   macro avg     0.5826    0.5645    0.5727      1039
weighted avg     0.6321    0.6333    0.6324      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5345    0.5741    0.5536        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5448    0.5874    0.5653       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4430    0.5156    0.4765       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4896    0.4700    0.4796       200
    Expansion.Instantiation     0.7212    0.6356    0.6757       118
      Expansion.Restatement     0.4405    0.4739    0.4566       211
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5091      1039
                  macro avg     0.3235    0.3466    0.3329      1039
               weighted avg     0.4923    0.5091    0.4996      1039

Epoch [8/15]
top-down:TOP: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 61.20%, Val F1: 53.18% Time: 38.25369620323181 
top-down:SEC: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 46.35%, Val F1: 29.16% Time: 38.25369620323181 
top-down:CONN: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 46.88%,Val Loss:   6.2,  Val Acc: 26.61%, Val F1:  6.71% Time: 38.25369620323181 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   6.4,  Val Acc: 61.63%, Val F1: 52.06% Time: 114.89030241966248 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   6.4,  Val Acc: 45.24%, Val F1: 27.72% Time: 114.89030241966248 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   6.4,  Val Acc: 26.27%, Val F1:  7.21% Time: 114.89030241966248 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   6.3,  Val Acc: 61.37%, Val F1: 52.78% Time: 191.24629664421082 
top-down:SEC: Iter:   3000,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   6.3,  Val Acc: 46.44%, Val F1: 28.66% Time: 191.24629664421082 
top-down:CONN: Iter:   3000,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.3,  Val Acc: 25.84%, Val F1:  6.76% Time: 191.24629664421082 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 62.06%, Val F1: 53.13% Time: 267.4712584018707 
top-down:SEC: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.3,  Val Acc: 46.01%, Val F1: 28.74% Time: 267.4712584018707 
top-down:CONN: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   6.3,  Val Acc: 26.52%, Val F1:  6.98% Time: 267.4712584018707 
 
 
Train time usage: 301.1301145553589
Test time usage: 1.7123992443084717
TOP: Test Loss:   5.6,  Test Acc: 65.06%, Test F1: 57.41%
SEC: Test Loss:   5.6,  Test Acc: 51.20%, Test F1: 34.49%
CONN: Test Loss:   5.6,  Test Acc: 26.85%, Test F1:  9.02%
consistency_top_sec: 49.37%,  consistency_sec_conn: 22.23%, consistency_top_sec_conn: 21.85%
              precision    recall  f1-score   support

    Temporal     0.5079    0.4706    0.4885        68
 Contingency     0.6111    0.4783    0.5366       276
  Comparison     0.5448    0.5069    0.5252       144
   Expansion     0.7013    0.7967    0.7460       551

    accuracy                         0.6506      1039
   macro avg     0.5913    0.5631    0.5741      1039
weighted avg     0.6430    0.6506    0.6429      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5345    0.5741    0.5536        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5812    0.5056    0.5408       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4692    0.4766    0.4729       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4481    0.5400    0.4898       200
    Expansion.Instantiation     0.6754    0.6525    0.6638       118
      Expansion.Restatement     0.4766    0.5308    0.5022       211
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.1000    0.0833    0.0909        12

                   accuracy                         0.5120      1039
                  macro avg     0.3327    0.3663    0.3449      1039
               weighted avg     0.5002    0.5120    0.5039      1039

Epoch [9/15]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.5,  Val Acc: 60.69%, Val F1: 51.45% Time: 43.424805879592896 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.5,  Val Acc: 45.49%, Val F1: 28.50% Time: 43.424805879592896 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   6.5,  Val Acc: 26.44%, Val F1:  6.77% Time: 43.424805879592896 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   6.4,  Val Acc: 61.46%, Val F1: 52.58% Time: 119.49671936035156 
top-down:SEC: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   6.4,  Val Acc: 45.84%, Val F1: 29.77% Time: 119.49671936035156 
top-down:CONN: Iter:   3300,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   6.4,  Val Acc: 25.67%, Val F1:  6.50% Time: 119.49671936035156 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.4,  Val Acc: 62.06%, Val F1: 52.15% Time: 195.60140752792358 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.4,  Val Acc: 47.38%, Val F1: 30.54% Time: 195.60140752792358 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   6.4,  Val Acc: 27.21%, Val F1:  7.24% Time: 195.60140752792358 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   6.5,  Val Acc: 62.15%, Val F1: 52.80% Time: 271.9050784111023 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   6.5,  Val Acc: 47.30%, Val F1: 30.11% Time: 271.9050784111023 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 43.75%,Val Loss:   6.5,  Val Acc: 25.92%, Val F1:  7.11% Time: 271.9050784111023 
 
 
Train time usage: 300.6827869415283
Test time usage: 1.7233047485351562
TOP: Test Loss:   5.8,  Test Acc: 64.20%, Test F1: 56.88%
SEC: Test Loss:   5.8,  Test Acc: 50.82%, Test F1: 32.04%
CONN: Test Loss:   5.8,  Test Acc: 26.66%, Test F1:  8.51%
consistency_top_sec: 49.57%,  consistency_sec_conn: 22.81%, consistency_top_sec_conn: 22.33%
              precision    recall  f1-score   support

    Temporal     0.4507    0.4706    0.4604        68
 Contingency     0.5935    0.5290    0.5594       276
  Comparison     0.5066    0.5347    0.5203       144
   Expansion     0.7228    0.7477    0.7351       551

    accuracy                         0.6420      1039
   macro avg     0.5684    0.5705    0.5688      1039
weighted avg     0.6407    0.6420    0.6407      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4444    0.5185    0.4786        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5677    0.5613    0.5645       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4497    0.5234    0.4838       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4519    0.5400    0.4920       200
    Expansion.Instantiation     0.6579    0.6356    0.6466       118
      Expansion.Restatement     0.5081    0.4455    0.4747       211
      Expansion.Alternative     0.2941    0.5556    0.3846         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5082      1039
                  macro avg     0.3067    0.3436    0.3204      1039
               weighted avg     0.4929    0.5082    0.4985      1039

Epoch [10/15]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 61.46%, Val F1: 52.04% Time: 48.88610339164734 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   6.6,  Val Acc: 46.70%, Val F1: 28.70% Time: 48.88610339164734 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 43.75%,Val Loss:   6.6,  Val Acc: 25.58%, Val F1:  6.43% Time: 48.88610339164734 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   6.7,  Val Acc: 62.75%, Val F1: 52.81% Time: 125.05429244041443 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   6.7,  Val Acc: 47.73%, Val F1: 29.72% Time: 125.05429244041443 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.7,  Val Acc: 26.01%, Val F1:  7.24% Time: 125.05429244041443 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 61.03%, Val F1: 52.38% Time: 201.17873072624207 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.7,  Val Acc: 46.52%, Val F1: 28.95% Time: 201.17873072624207 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   6.7,  Val Acc: 27.04%, Val F1:  7.14% Time: 201.17873072624207 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 62.15%, Val F1: 51.80% Time: 277.25935435295105 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 46.87%, Val F1: 29.79% Time: 277.25935435295105 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   6.7,  Val Acc: 25.58%, Val F1:  7.02% Time: 277.25935435295105 
 
 
Train time usage: 300.58817291259766
Test time usage: 1.713655710220337
TOP: Test Loss:   5.8,  Test Acc: 63.52%, Test F1: 55.23%
SEC: Test Loss:   5.8,  Test Acc: 49.86%, Test F1: 33.44%
CONN: Test Loss:   5.8,  Test Acc: 27.33%, Test F1:  9.35%
consistency_top_sec: 48.70%,  consistency_sec_conn: 22.91%, consistency_top_sec_conn: 22.71%
              precision    recall  f1-score   support

    Temporal     0.4576    0.3971    0.4252        68
 Contingency     0.5804    0.5362    0.5574       276
  Comparison     0.5000    0.4931    0.4965       144
   Expansion     0.7101    0.7514    0.7302       551

    accuracy                         0.6352      1039
   macro avg     0.5620    0.5444    0.5523      1039
weighted avg     0.6300    0.6352    0.6319      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4561    0.4815    0.4685        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5511    0.5613    0.5562       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4476    0.5000    0.4723       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4215    0.5100    0.4615       200
    Expansion.Instantiation     0.7037    0.6441    0.6726       118
      Expansion.Restatement     0.4946    0.4360    0.4635       211
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.1667    0.1667    0.1667        12

                   accuracy                         0.4986      1039
                  macro avg     0.3250    0.3505    0.3344      1039
               weighted avg     0.4878    0.4986    0.4914      1039

Epoch [11/15]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 84.38%,Val Loss:   6.7,  Val Acc: 60.52%, Val F1: 52.07% Time: 53.68495798110962 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   6.7,  Val Acc: 45.58%, Val F1: 28.43% Time: 53.68495798110962 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   6.7,  Val Acc: 26.44%, Val F1:  6.93% Time: 53.68495798110962 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 61.12%, Val F1: 52.06% Time: 129.77104568481445 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   6.9,  Val Acc: 45.84%, Val F1: 28.65% Time: 129.77104568481445 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   6.9,  Val Acc: 25.58%, Val F1:  7.22% Time: 129.77104568481445 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.9,  Val Acc: 61.46%, Val F1: 52.44% Time: 205.67777824401855 
top-down:SEC: Iter:   4200,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   6.9,  Val Acc: 45.84%, Val F1: 29.17% Time: 205.67777824401855 
top-down:CONN: Iter:   4200,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   6.9,  Val Acc: 25.24%, Val F1:  6.84% Time: 205.67777824401855 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.9,  Val Acc: 60.43%, Val F1: 51.17% Time: 281.5794851779938 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.9,  Val Acc: 45.24%, Val F1: 29.06% Time: 281.5794851779938 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   6.9,  Val Acc: 25.15%, Val F1:  6.89% Time: 281.5794851779938 
 
 
Train time usage: 299.652672290802
Test time usage: 1.7150094509124756
TOP: Test Loss:   6.1,  Test Acc: 61.98%, Test F1: 55.02%
SEC: Test Loss:   6.1,  Test Acc: 49.86%, Test F1: 33.49%
CONN: Test Loss:   6.1,  Test Acc: 27.14%, Test F1:  9.20%
consistency_top_sec: 48.51%,  consistency_sec_conn: 23.39%, consistency_top_sec_conn: 22.91%
              precision    recall  f1-score   support

    Temporal     0.4000    0.4706    0.4324        68
 Contingency     0.5731    0.5399    0.5560       276
  Comparison     0.4902    0.5208    0.5051       144
   Expansion     0.7106    0.7042    0.7074       551

    accuracy                         0.6198      1039
   macro avg     0.5435    0.5589    0.5502      1039
weighted avg     0.6232    0.6198    0.6211      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4324    0.5926    0.5000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5593    0.5613    0.5603       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4114    0.5078    0.4545       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4740    0.4550    0.4643       200
    Expansion.Instantiation     0.6909    0.6441    0.6667       118
      Expansion.Restatement     0.4683    0.4550    0.4615       211
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.1538    0.1667    0.1600        12

                   accuracy                         0.4986      1039
                  macro avg     0.3203    0.3580    0.3349      1039
               weighted avg     0.4874    0.4986    0.4913      1039

Epoch [12/15]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.9,  Val Acc: 60.26%, Val F1: 50.99% Time: 58.91718029975891 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.9,  Val Acc: 45.92%, Val F1: 28.58% Time: 58.91718029975891 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   6.9,  Val Acc: 25.84%, Val F1:  7.26% Time: 58.91718029975891 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.0,  Val Acc: 60.86%, Val F1: 52.37% Time: 134.93142080307007 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   7.0,  Val Acc: 44.81%, Val F1: 28.46% Time: 134.93142080307007 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   7.0,  Val Acc: 26.09%, Val F1:  7.15% Time: 134.93142080307007 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.0,  Val Acc: 60.34%, Val F1: 51.35% Time: 211.2164125442505 
top-down:SEC: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   7.0,  Val Acc: 44.81%, Val F1: 28.47% Time: 211.2164125442505 
top-down:CONN: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   7.0,  Val Acc: 25.92%, Val F1:  7.19% Time: 211.2164125442505 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   7.0,  Val Acc: 60.52%, Val F1: 52.53% Time: 288.0743799209595 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   7.0,  Val Acc: 45.67%, Val F1: 28.88% Time: 288.0743799209595 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   7.0,  Val Acc: 26.35%, Val F1:  7.25% Time: 288.0743799209595 
 
 
Train time usage: 301.058922290802
Test time usage: 1.7196502685546875
TOP: Test Loss:   6.2,  Test Acc: 63.14%, Test F1: 55.45%
SEC: Test Loss:   6.2,  Test Acc: 50.05%, Test F1: 33.14%
CONN: Test Loss:   6.2,  Test Acc: 27.33%, Test F1:  9.54%
consistency_top_sec: 48.99%,  consistency_sec_conn: 23.10%, consistency_top_sec_conn: 22.81%
              precision    recall  f1-score   support

    Temporal     0.4189    0.4559    0.4366        68
 Contingency     0.5958    0.5181    0.5543       276
  Comparison     0.4901    0.5139    0.5017       144
   Expansion     0.7108    0.7405    0.7253       551

    accuracy                         0.6314      1039
   macro avg     0.5539    0.5571    0.5545      1039
weighted avg     0.6306    0.6314    0.6300      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4697    0.5741    0.5167        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5675    0.5316    0.5489       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4231    0.5156    0.4648       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4796    0.4700    0.4747       200
    Expansion.Instantiation     0.6754    0.6525    0.6638       118
      Expansion.Restatement     0.4518    0.4882    0.4692       211
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.1000    0.0833    0.0909        12

                   accuracy                         0.5005      1039
                  macro avg     0.3182    0.3519    0.3314      1039
               weighted avg     0.4883    0.5005    0.4930      1039

Epoch [13/15]
top-down:TOP: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.0,  Val Acc: 61.29%, Val F1: 51.62% Time: 64.58652591705322 
top-down:SEC: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.0,  Val Acc: 46.27%, Val F1: 29.84% Time: 64.58652591705322 
top-down:CONN: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   7.0,  Val Acc: 25.75%, Val F1:  6.98% Time: 64.58652591705322 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.0,  Val Acc: 61.37%, Val F1: 52.30% Time: 141.0649287700653 
top-down:SEC: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 46.01%, Val F1: 29.80% Time: 141.0649287700653 
top-down:CONN: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 40.62%,Val Loss:   7.0,  Val Acc: 26.18%, Val F1:  7.15% Time: 141.0649287700653 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.0,  Val Acc: 59.91%, Val F1: 51.51% Time: 217.68302154541016 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   7.0,  Val Acc: 45.67%, Val F1: 29.02% Time: 217.68302154541016 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   7.0,  Val Acc: 25.75%, Val F1:  7.18% Time: 217.68302154541016 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.1,  Val Acc: 60.52%, Val F1: 52.25% Time: 293.9563179016113 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   7.1,  Val Acc: 45.41%, Val F1: 28.77% Time: 293.9563179016113 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 43.75%,Val Loss:   7.1,  Val Acc: 25.92%, Val F1:  7.11% Time: 293.9563179016113 
 
 
Train time usage: 301.6549623012543
Test time usage: 1.7189819812774658
TOP: Test Loss:   6.2,  Test Acc: 62.95%, Test F1: 56.07%
SEC: Test Loss:   6.2,  Test Acc: 50.14%, Test F1: 32.87%
CONN: Test Loss:   6.2,  Test Acc: 26.95%, Test F1:  9.19%
consistency_top_sec: 49.57%,  consistency_sec_conn: 23.39%, consistency_top_sec_conn: 23.10%
              precision    recall  f1-score   support

    Temporal     0.4231    0.4853    0.4521        68
 Contingency     0.5837    0.5435    0.5629       276
  Comparison     0.5034    0.5208    0.5119       144
   Expansion     0.7135    0.7187    0.7161       551

    accuracy                         0.6295      1039
   macro avg     0.5559    0.5671    0.5607      1039
weighted avg     0.6309    0.6295    0.6298      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4478    0.5556    0.4959        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5660    0.5576    0.5618       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4276    0.5078    0.4643       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4554    0.4850    0.4697       200
    Expansion.Instantiation     0.6875    0.6525    0.6696       118
      Expansion.Restatement     0.4824    0.4550    0.4683       211
      Expansion.Alternative     0.2500    0.4444    0.3200         9
             Expansion.List     0.1667    0.1667    0.1667        12

                   accuracy                         0.5014      1039
                  macro avg     0.3167    0.3477    0.3287      1039
               weighted avg     0.4903    0.5014    0.4947      1039

Epoch [14/15]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   7.1,  Val Acc: 60.60%, Val F1: 51.72% Time: 69.45210814476013 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 46.01%, Val F1: 29.31% Time: 69.45210814476013 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   7.1,  Val Acc: 26.27%, Val F1:  7.38% Time: 69.45210814476013 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.1,  Val Acc: 60.43%, Val F1: 51.89% Time: 146.90317296981812 
top-down:SEC: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   7.1,  Val Acc: 45.67%, Val F1: 29.49% Time: 146.90317296981812 
top-down:CONN: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 25.92%, Val F1:  7.28% Time: 146.90317296981812 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 60.77%, Val F1: 51.52% Time: 224.38361978530884 
top-down:SEC: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   7.1,  Val Acc: 45.92%, Val F1: 29.24% Time: 224.38361978530884 
top-down:CONN: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   7.1,  Val Acc: 25.92%, Val F1:  7.14% Time: 224.38361978530884 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 96.88%,Val Loss:   7.1,  Val Acc: 60.69%, Val F1: 51.55% Time: 300.6580080986023 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 46.18%, Val F1: 29.26% Time: 300.6580080986023 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 37.50%,Val Loss:   7.1,  Val Acc: 26.01%, Val F1:  7.36% Time: 300.6580080986023 
 
 
Train time usage: 303.15327739715576
Test time usage: 1.7324175834655762
TOP: Test Loss:   6.3,  Test Acc: 63.52%, Test F1: 55.96%
SEC: Test Loss:   6.3,  Test Acc: 50.43%, Test F1: 33.29%
CONN: Test Loss:   6.3,  Test Acc: 27.24%, Test F1:  9.18%
consistency_top_sec: 49.66%,  consistency_sec_conn: 23.20%, consistency_top_sec_conn: 22.81%
              precision    recall  f1-score   support

    Temporal     0.4348    0.4412    0.4380        68
 Contingency     0.5758    0.5507    0.5630       276
  Comparison     0.5255    0.5000    0.5125       144
   Expansion     0.7135    0.7368    0.7250       551

    accuracy                         0.6352      1039
   macro avg     0.5624    0.5572    0.5596      1039
weighted avg     0.6326    0.6352    0.6337      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4500    0.5000    0.4737        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5522    0.5502    0.5512       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4532    0.4922    0.4719       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4641    0.4850    0.4743       200
    Expansion.Instantiation     0.7170    0.6441    0.6786       118
      Expansion.Restatement     0.4753    0.5024    0.4885       211
      Expansion.Alternative     0.2632    0.5556    0.3571         9
             Expansion.List     0.1667    0.1667    0.1667        12

                   accuracy                         0.5043      1039
                  macro avg     0.3220    0.3542    0.3329      1039
               weighted avg     0.4937    0.5043    0.4981      1039

Epoch [15/15]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 61.03%, Val F1: 51.82% Time: 74.99316644668579 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   7.1,  Val Acc: 45.92%, Val F1: 29.02% Time: 74.99316644668579 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   7.1,  Val Acc: 26.09%, Val F1:  7.22% Time: 74.99316644668579 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.2,  Val Acc: 60.86%, Val F1: 51.27% Time: 151.1235055923462 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 46.01%, Val F1: 29.66% Time: 151.1235055923462 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   7.2,  Val Acc: 25.32%, Val F1:  7.17% Time: 151.1235055923462 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 60.77%, Val F1: 51.48% Time: 227.96684956550598 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 46.01%, Val F1: 29.02% Time: 227.96684956550598 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 50.00%,Val Loss:   7.2,  Val Acc: 25.67%, Val F1:  7.28% Time: 227.96684956550598 
 
 
Train time usage: 300.13085079193115
Test time usage: 1.7496387958526611
TOP: Test Loss:   6.3,  Test Acc: 63.43%, Test F1: 55.53%
SEC: Test Loss:   6.3,  Test Acc: 50.24%, Test F1: 34.56%
CONN: Test Loss:   6.3,  Test Acc: 27.53%, Test F1:  9.53%
consistency_top_sec: 49.18%,  consistency_sec_conn: 23.20%, consistency_top_sec_conn: 22.62%
              precision    recall  f1-score   support

    Temporal     0.4167    0.4412    0.4286        68
 Contingency     0.5918    0.5254    0.5566       276
  Comparison     0.5105    0.5069    0.5087       144
   Expansion     0.7098    0.7459    0.7274       551

    accuracy                         0.6343      1039
   macro avg     0.5572    0.5548    0.5553      1039
weighted avg     0.6317    0.6343    0.6322      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4754    0.5370    0.5043        54
         Temporal.Synchrony     0.3333    0.0714    0.1176        14
          Contingency.Cause     0.5613    0.5279    0.5441       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4444    0.5000    0.4706       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4533    0.4850    0.4686       200
    Expansion.Instantiation     0.7143    0.6356    0.6726       118
      Expansion.Restatement     0.4672    0.5071    0.4864       211
      Expansion.Alternative     0.2778    0.5556    0.3704         9
             Expansion.List     0.1667    0.1667    0.1667        12

                   accuracy                         0.5024      1039
                  macro avg     0.3540    0.3624    0.3456      1039
               weighted avg     0.4969    0.5024    0.4971      1039

dev_best_acc_top: 63.26%,  dev_best_f1_top: 54.53%, 
dev_best_acc_sec: 46.27%,  dev_best_f1_sec: 30.31%, 
dev_best_acc_conn: 26.95%,  dev_best_f1_conn:  6.99%
