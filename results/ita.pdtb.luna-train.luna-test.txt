nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_it_luna_train_luna_test/data/', 'log_file': 'data/pdtb_it_luna_train_luna_test/log/', 'save_file': 'data/pdtb_it_luna_train_luna_test/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March04-19:18:07', 'log': 'data/pdtb_it_luna_train_luna_test/log/March04-19:18:07.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]90it [00:00, 897.08it/s]267it [00:00, 1409.67it/s]467it [00:00, 1677.09it/s]635it [00:00, 1622.19it/s]798it [00:00, 1420.30it/s]944it [00:00, 1319.10it/s]1079it [00:00, 1210.77it/s]1203it [00:00, 1124.95it/s]1318it [00:01, 1109.36it/s]1449it [00:01, 1161.82it/s]1567it [00:01, 1151.25it/s]1687it [00:01, 1164.75it/s]1805it [00:01, 1163.70it/s]1922it [00:01, 1157.67it/s]2051it [00:01, 1193.70it/s]2171it [00:01, 1155.90it/s]2288it [00:01, 1045.26it/s]2395it [00:02, 986.18it/s] 2519it [00:02, 1052.80it/s]2627it [00:02, 1042.65it/s]2753it [00:02, 1101.36it/s]2865it [00:02, 1022.45it/s]2972it [00:02, 1034.32it/s]3078it [00:02, 1039.70it/s]3184it [00:02, 1034.30it/s]3300it [00:02, 1069.36it/s]3408it [00:02, 1059.76it/s]3531it [00:03, 1097.82it/s]3644it [00:03, 1106.86it/s]3755it [00:03, 994.74it/s] 3857it [00:03, 951.13it/s]3954it [00:03, 831.11it/s]4056it [00:03, 877.27it/s]4147it [00:03, 815.84it/s]4232it [00:03, 780.69it/s]4327it [00:04, 823.27it/s]4431it [00:04, 879.49it/s]4525it [00:04, 894.80it/s]4616it [00:04, 805.21it/s]4722it [00:04, 871.64it/s]4812it [00:04, 829.46it/s]4897it [00:04, 810.29it/s]4980it [00:04, 732.97it/s]5056it [00:04, 733.70it/s]5140it [00:05, 760.90it/s]5218it [00:05, 533.83it/s]5298it [00:05, 585.70it/s]5400it [00:05, 686.57it/s]5486it [00:05, 729.63it/s]5572it [00:05, 758.42it/s]5654it [00:05, 716.11it/s]5750it [00:05, 779.79it/s]5847it [00:06, 808.67it/s]5939it [00:06, 836.02it/s]6027it [00:06, 821.44it/s]6111it [00:06, 736.01it/s]6212it [00:06, 806.81it/s]6307it [00:06, 845.65it/s]6394it [00:06, 826.48it/s]6499it [00:06, 888.50it/s]6594it [00:06, 900.52it/s]6686it [00:07, 901.19it/s]6781it [00:07, 913.64it/s]6873it [00:07, 903.11it/s]6977it [00:07, 941.64it/s]7073it [00:07, 944.84it/s]7175it [00:07, 964.47it/s]7272it [00:07, 959.54it/s]7369it [00:07, 947.95it/s]7464it [00:07, 934.99it/s]7565it [00:07, 954.23it/s]7661it [00:08, 924.29it/s]7755it [00:08, 927.84it/s]7858it [00:08, 955.60it/s]7954it [00:08, 952.74it/s]8065it [00:08, 997.18it/s]8167it [00:08, 1002.85it/s]8289it [00:08, 1066.91it/s]8414it [00:08, 1120.95it/s]8530it [00:08, 1131.17it/s]8653it [00:08, 1158.41it/s]8769it [00:09, 1109.01it/s]8881it [00:09, 1067.26it/s]8999it [00:09, 1098.13it/s]9110it [00:09, 1050.76it/s]9216it [00:09, 1026.31it/s]9320it [00:09, 967.01it/s] 9426it [00:09, 992.24it/s]9536it [00:09, 1019.08it/s]9639it [00:09, 995.64it/s] 9740it [00:10, 957.29it/s]9854it [00:10, 1007.88it/s]9956it [00:10, 993.75it/s] 10071it [00:10, 1036.83it/s]10178it [00:10, 1045.22it/s]10288it [00:10, 1058.40it/s]10395it [00:10, 1059.69it/s]10520it [00:10, 1114.40it/s]10632it [00:10, 1100.79it/s]10743it [00:11, 1069.27it/s]10854it [00:11, 1077.00it/s]10962it [00:11, 1054.22it/s]11078it [00:11, 1082.51it/s]11187it [00:11, 1054.15it/s]11293it [00:11, 1032.69it/s]11400it [00:11, 1040.61it/s]11505it [00:11, 1015.26it/s]11607it [00:11, 974.47it/s] 11705it [00:11, 942.30it/s]11800it [00:12, 937.48it/s]11894it [00:12, 930.89it/s]12002it [00:12, 971.49it/s]12100it [00:12, 913.88it/s]12193it [00:12, 881.90it/s]12290it [00:12, 906.19it/s]12389it [00:12, 926.14it/s]12483it [00:12, 791.96it/s]12571it [00:12, 812.62it/s]12667it [00:13, 851.87it/s]12755it [00:13, 778.91it/s]12846it [00:13, 812.61it/s]12935it [00:13, 833.54it/s]13036it [00:13, 876.43it/s]13126it [00:13, 803.97it/s]13209it [00:13, 782.25it/s]13275it [00:13, 956.29it/s]
0it [00:00, ?it/s]168it [00:00, 1780.88it/s]
0it [00:00, ?it/s]179it [00:00, 1782.51it/s]292it [00:00, 1833.26it/s]
Time usage: 30.46916913986206
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 2.8e+01,  Train Acc: 34.38%,Val Loss:   5.7,  Val Acc: 29.76%, Val F1: 11.47% Time: 104.22194814682007 *
top-down:SEC: Iter:    100,  Train Loss: 2.8e+01,  Train Acc: 21.88%,Val Loss:   5.7,  Val Acc: 14.29%, Val F1:  3.35% Time: 104.22194814682007 *
top-down:CONN: Iter:    100,  Train Loss: 2.8e+01,  Train Acc:  6.25%,Val Loss:   5.7,  Val Acc: 83.33%, Val F1: 45.45% Time: 104.22194814682007 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.4e+01,  Train Acc: 59.38%,Val Loss:   4.3,  Val Acc: 35.12%, Val F1: 15.46% Time: 196.84782814979553 *
top-down:SEC: Iter:    200,  Train Loss: 3.4e+01,  Train Acc: 31.25%,Val Loss:   4.3,  Val Acc: 33.93%, Val F1:  5.63% Time: 196.84782814979553 *
top-down:CONN: Iter:    200,  Train Loss: 3.4e+01,  Train Acc: 21.88%,Val Loss:   4.3,  Val Acc: 100.00%, Val F1: 100.00% Time: 196.84782814979553 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 3.5e+01,  Train Acc: 53.12%,Val Loss:   3.6,  Val Acc: 39.88%, Val F1: 24.16% Time: 286.07639336586 
top-down:SEC: Iter:    300,  Train Loss: 3.5e+01,  Train Acc: 43.75%,Val Loss:   3.6,  Val Acc: 36.90%, Val F1:  9.82% Time: 286.07639336586 
top-down:CONN: Iter:    300,  Train Loss: 3.5e+01,  Train Acc: 31.25%,Val Loss:   3.6,  Val Acc: 98.21%, Val F1: 49.55% Time: 286.07639336586 
 
 
top-down:TOP: Iter:    400,  Train Loss: 3.4e+01,  Train Acc: 46.88%,Val Loss:   3.2,  Val Acc: 41.67%, Val F1: 38.84% Time: 377.9098446369171 *
top-down:SEC: Iter:    400,  Train Loss: 3.4e+01,  Train Acc: 43.75%,Val Loss:   3.2,  Val Acc: 39.88%, Val F1: 13.39% Time: 377.9098446369171 *
top-down:CONN: Iter:    400,  Train Loss: 3.4e+01,  Train Acc: 25.00%,Val Loss:   3.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 377.9098446369171 *
 
 
Train time usage: 393.5212769508362
Test time usage: 0.5663931369781494
TOP: Test Loss:   3.3,  Test Acc: 40.75%, Test F1: 36.30%
SEC: Test Loss:   3.3,  Test Acc: 34.25%, Test F1: 15.98%
CONN: Test Loss:   3.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  7.03%,  consistency_sec_conn:  9.62%, consistency_top_sec_conn:  7.03%
              precision    recall  f1-score   support

    Temporal     0.3361    0.7547    0.4651        53
 Contingency     0.4944    0.5176    0.5057        85
  Comparison     0.2000    0.0930    0.1270        43
   Expansion     0.4844    0.2793    0.3543       111

    accuracy                         0.4075       292
   macro avg     0.3787    0.4112    0.3630       292
weighted avg     0.4185    0.4075    0.3850       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3117    0.5333    0.3934        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4079    0.7294    0.5232        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.1633    0.2500    0.1975        32
      Comparison.Concession     0.4286    0.1017    0.1644        59
      Expansion.Conjunction     0.0000    0.0000    0.0000        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.3425       292
                  macro avg     0.1639    0.2018    0.1598       292
               weighted avg     0.2713    0.3425    0.2678       292

Epoch [2/15]
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   3.2,  Val Acc: 47.62%, Val F1: 38.55% Time: 77.70230603218079 
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 37.50%,Val Loss:   3.2,  Val Acc: 40.48%, Val F1: 14.06% Time: 77.70230603218079 
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 18.75%,Val Loss:   3.2,  Val Acc: 99.40%, Val F1: 49.85% Time: 77.70230603218079 
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   2.8,  Val Acc: 57.74%, Val F1: 53.68% Time: 166.65298080444336 
top-down:SEC: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   2.8,  Val Acc: 43.45%, Val F1: 17.60% Time: 166.65298080444336 
top-down:CONN: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 18.75%,Val Loss:   2.8,  Val Acc: 99.40%, Val F1: 49.85% Time: 166.65298080444336 
 
 
top-down:TOP: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   2.8,  Val Acc: 58.33%, Val F1: 58.15% Time: 258.43997859954834 *
top-down:SEC: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   2.8,  Val Acc: 46.43%, Val F1: 25.54% Time: 258.43997859954834 *
top-down:CONN: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   2.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 258.43997859954834 *
 
 
top-down:TOP: Iter:    800,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   2.6,  Val Acc: 60.71%, Val F1: 60.07% Time: 346.99176812171936 
top-down:SEC: Iter:    800,  Train Loss: 3.2e+01,  Train Acc: 65.62%,Val Loss:   2.6,  Val Acc: 46.43%, Val F1: 22.06% Time: 346.99176812171936 
top-down:CONN: Iter:    800,  Train Loss: 3.2e+01,  Train Acc: 28.12%,Val Loss:   2.6,  Val Acc: 99.40%, Val F1: 49.85% Time: 346.99176812171936 
 
 
Train time usage: 375.9646077156067
Test time usage: 0.6109223365783691
TOP: Test Loss:   2.8,  Test Acc: 51.03%, Test F1: 50.98%
SEC: Test Loss:   2.8,  Test Acc: 42.12%, Test F1: 23.55%
CONN: Test Loss:   2.8,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 11.26%,  consistency_sec_conn: 11.84%, consistency_top_sec_conn: 11.26%
              precision    recall  f1-score   support

    Temporal     0.4082    0.7547    0.5298        53
 Contingency     0.6933    0.6118    0.6500        85
  Comparison     0.4250    0.7907    0.5528        43
   Expansion     0.5897    0.2072    0.3067       111

    accuracy                         0.5103       292
   macro avg     0.5291    0.5911    0.5098       292
weighted avg     0.5627    0.5103    0.4834       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3571    0.7778    0.4895        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5385    0.6588    0.5926        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.3818    0.6562    0.4828        32
      Comparison.Concession     0.4444    0.1356    0.2078        59
      Expansion.Conjunction     0.3333    0.0667    0.1111        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.4212       292
                  macro avg     0.2569    0.2869    0.2355       292
               weighted avg     0.3948    0.4212    0.3600       292

Epoch [3/15]
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   2.4,  Val Acc: 62.50%, Val F1: 60.84% Time: 66.07177710533142 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   2.4,  Val Acc: 50.00%, Val F1: 28.45% Time: 66.07177710533142 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 31.25%,Val Loss:   2.4,  Val Acc: 100.00%, Val F1: 100.00% Time: 66.07177710533142 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   2.3,  Val Acc: 62.50%, Val F1: 61.85% Time: 155.6156816482544 
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   2.3,  Val Acc: 47.02%, Val F1: 22.31% Time: 155.6156816482544 
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   2.3,  Val Acc: 100.00%, Val F1: 100.00% Time: 155.6156816482544 
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   2.2,  Val Acc: 64.88%, Val F1: 64.49% Time: 244.9919011592865 
top-down:SEC: Iter:   1100,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   2.2,  Val Acc: 47.02%, Val F1: 22.86% Time: 244.9919011592865 
top-down:CONN: Iter:   1100,  Train Loss: 3.2e+01,  Train Acc: 31.25%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 244.9919011592865 
 
 
top-down:TOP: Iter:   1200,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   2.1,  Val Acc: 66.07%, Val F1: 66.10% Time: 335.55362248420715 *
top-down:SEC: Iter:   1200,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   2.1,  Val Acc: 50.00%, Val F1: 25.46% Time: 335.55362248420715 *
top-down:CONN: Iter:   1200,  Train Loss: 2.7e+01,  Train Acc: 37.50%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 335.55362248420715 *
 
 
Train time usage: 377.97008204460144
Test time usage: 0.5650463104248047
TOP: Test Loss:   2.5,  Test Acc: 59.93%, Test F1: 61.06%
SEC: Test Loss:   2.5,  Test Acc: 51.37%, Test F1: 30.62%
CONN: Test Loss:   2.5,  Test Acc: 98.97%, Test F1: 33.16%
consistency_top_sec: 13.86%,  consistency_sec_conn: 14.44%, consistency_top_sec_conn: 13.86%
              precision    recall  f1-score   support

    Temporal     0.4659    0.7736    0.5816        53
 Contingency     0.6408    0.7765    0.7021        85
  Comparison     0.7209    0.7209    0.7209        43
   Expansion     0.6379    0.3333    0.4379       111

    accuracy                         0.5993       292
   macro avg     0.6164    0.6511    0.6106       292
weighted avg     0.6198    0.5993    0.5826       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4118    0.7778    0.5385        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5932    0.8235    0.6897        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5750    0.7188    0.6389        32
      Comparison.Concession     0.6500    0.2203    0.3291        59
      Expansion.Conjunction     0.3462    0.2000    0.2535        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5137       292
                  macro avg     0.3220    0.3425    0.3062       292
               weighted avg     0.4838    0.5137    0.4593       292

Epoch [4/15]
top-down:TOP: Iter:   1300,  Train Loss: 3.2e+01,  Train Acc: 87.50%,Val Loss:   2.1,  Val Acc: 70.24%, Val F1: 70.30% Time: 50.04911732673645 
top-down:SEC: Iter:   1300,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   2.1,  Val Acc: 52.38%, Val F1: 31.06% Time: 50.04911732673645 
top-down:CONN: Iter:   1300,  Train Loss: 3.2e+01,  Train Acc: 21.88%,Val Loss:   2.1,  Val Acc: 99.40%, Val F1: 49.85% Time: 50.04911732673645 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   2.1,  Val Acc: 68.45%, Val F1: 67.85% Time: 140.59922218322754 *
top-down:SEC: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   2.1,  Val Acc: 53.57%, Val F1: 32.66% Time: 140.59922218322754 *
top-down:CONN: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 140.59922218322754 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.7e+01,  Train Acc: 62.50%,Val Loss:   2.0,  Val Acc: 69.05%, Val F1: 69.42% Time: 230.91672229766846 
top-down:SEC: Iter:   1500,  Train Loss: 3.7e+01,  Train Acc: 50.00%,Val Loss:   2.0,  Val Acc: 54.17%, Val F1: 29.00% Time: 230.91672229766846 
top-down:CONN: Iter:   1500,  Train Loss: 3.7e+01,  Train Acc: 40.62%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 230.91672229766846 
 
 
top-down:TOP: Iter:   1600,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   2.0,  Val Acc: 69.64%, Val F1: 69.49% Time: 321.00332832336426 
top-down:SEC: Iter:   1600,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   2.0,  Val Acc: 53.57%, Val F1: 28.13% Time: 321.00332832336426 
top-down:CONN: Iter:   1600,  Train Loss: 2.8e+01,  Train Acc: 37.50%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 321.00332832336426 
 
 
Train time usage: 376.7318317890167
Test time usage: 0.6132903099060059
TOP: Test Loss:   2.3,  Test Acc: 64.73%, Test F1: 66.01%
SEC: Test Loss:   2.3,  Test Acc: 54.11%, Test F1: 33.36%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.53%,  consistency_sec_conn: 15.21%, consistency_top_sec_conn: 14.53%
              precision    recall  f1-score   support

    Temporal     0.4884    0.7925    0.6043        53
 Contingency     0.7595    0.7059    0.7317        85
  Comparison     0.7692    0.6977    0.7317        43
   Expansion     0.6477    0.5135    0.5729       111

    accuracy                         0.6473       292
   macro avg     0.6662    0.6774    0.6601       292
weighted avg     0.6692    0.6473    0.6482       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4337    0.8000    0.5625        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6633    0.7647    0.7104        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6053    0.7188    0.6571        32
      Comparison.Concession     0.4898    0.4068    0.4444        59
      Expansion.Conjunction     0.4348    0.2222    0.2941        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5411       292
                  macro avg     0.3284    0.3641    0.3336       292
               weighted avg     0.4922    0.5411    0.5006       292

Epoch [5/15]
top-down:TOP: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   1.9,  Val Acc: 69.64%, Val F1: 70.08% Time: 36.89915728569031 *
top-down:SEC: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   1.9,  Val Acc: 54.76%, Val F1: 31.84% Time: 36.89915728569031 *
top-down:CONN: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 36.89915728569031 *
 
 
top-down:TOP: Iter:   1800,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   2.0,  Val Acc: 72.02%, Val F1: 72.42% Time: 129.3307864665985 *
top-down:SEC: Iter:   1800,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   2.0,  Val Acc: 57.14%, Val F1: 33.42% Time: 129.3307864665985 *
top-down:CONN: Iter:   1800,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 129.3307864665985 *
 
 
top-down:TOP: Iter:   1900,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   2.0,  Val Acc: 69.64%, Val F1: 69.88% Time: 218.15562319755554 
top-down:SEC: Iter:   1900,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   2.0,  Val Acc: 53.57%, Val F1: 25.77% Time: 218.15562319755554 
top-down:CONN: Iter:   1900,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 218.15562319755554 
 
 
top-down:TOP: Iter:   2000,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   1.9,  Val Acc: 69.64%, Val F1: 69.35% Time: 308.4253990650177 
top-down:SEC: Iter:   2000,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   1.9,  Val Acc: 57.14%, Val F1: 35.88% Time: 308.4253990650177 
top-down:CONN: Iter:   2000,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 308.4253990650177 
 
 
Train time usage: 379.68711400032043
Test time usage: 0.6008374691009521
TOP: Test Loss:   2.3,  Test Acc: 65.75%, Test F1: 66.20%
SEC: Test Loss:   2.3,  Test Acc: 54.11%, Test F1: 34.34%
CONN: Test Loss:   2.3,  Test Acc: 99.32%, Test F1: 49.83%
consistency_top_sec: 14.82%,  consistency_sec_conn: 15.21%, consistency_top_sec_conn: 14.82%
              precision    recall  f1-score   support

    Temporal     0.4921    0.5849    0.5345        53
 Contingency     0.7683    0.7412    0.7545        85
  Comparison     0.8485    0.6512    0.7368        43
   Expansion     0.6140    0.6306    0.6222       111

    accuracy                         0.6575       292
   macro avg     0.6807    0.6520    0.6620       292
weighted avg     0.6713    0.6575    0.6617       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4355    0.6000    0.5047        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6633    0.7647    0.7104        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6667    0.6875    0.6769        32
      Comparison.Concession     0.4746    0.4746    0.4746        59
      Expansion.Conjunction     0.4103    0.3556    0.3810        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5411       292
                  macro avg     0.3313    0.3603    0.3434       292
               weighted avg     0.4924    0.5411    0.5133       292

Epoch [6/15]
top-down:TOP: Iter:   2100,  Train Loss: 3.6e+01,  Train Acc: 78.12%,Val Loss:   1.8,  Val Acc: 74.40%, Val F1: 75.31% Time: 25.05269694328308 *
top-down:SEC: Iter:   2100,  Train Loss: 3.6e+01,  Train Acc: 65.62%,Val Loss:   1.8,  Val Acc: 58.93%, Val F1: 36.62% Time: 25.05269694328308 *
top-down:CONN: Iter:   2100,  Train Loss: 3.6e+01,  Train Acc: 37.50%,Val Loss:   1.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 25.05269694328308 *
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   1.9,  Val Acc: 72.02%, Val F1: 72.39% Time: 116.65645098686218 
top-down:SEC: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   1.9,  Val Acc: 58.93%, Val F1: 29.18% Time: 116.65645098686218 
top-down:CONN: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 34.38%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 116.65645098686218 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   1.8,  Val Acc: 71.43%, Val F1: 72.03% Time: 205.16011095046997 
top-down:SEC: Iter:   2300,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   1.8,  Val Acc: 58.33%, Val F1: 37.07% Time: 205.16011095046997 
top-down:CONN: Iter:   2300,  Train Loss: 2.5e+01,  Train Acc: 46.88%,Val Loss:   1.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 205.16011095046997 
 
 
top-down:TOP: Iter:   2400,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   1.9,  Val Acc: 72.62%, Val F1: 72.69% Time: 294.4036419391632 
top-down:SEC: Iter:   2400,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   1.9,  Val Acc: 58.33%, Val F1: 37.18% Time: 294.4036419391632 
top-down:CONN: Iter:   2400,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 294.4036419391632 
 
 
Train time usage: 377.8396234512329
Test time usage: 0.6126968860626221
TOP: Test Loss:   2.4,  Test Acc: 67.81%, Test F1: 68.50%
SEC: Test Loss:   2.4,  Test Acc: 54.79%, Test F1: 35.07%
CONN: Test Loss:   2.4,  Test Acc: 97.95%, Test F1: 24.74%
consistency_top_sec: 15.01%,  consistency_sec_conn: 15.21%, consistency_top_sec_conn: 14.82%
              precision    recall  f1-score   support

    Temporal     0.5270    0.7358    0.6142        53
 Contingency     0.8286    0.6824    0.7484        85
  Comparison     0.7273    0.7442    0.7356        43
   Expansion     0.6635    0.6216    0.6419       111

    accuracy                         0.6781       292
   macro avg     0.6866    0.6960    0.6850       292
weighted avg     0.6962    0.6781    0.6817       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4474    0.7556    0.5620        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7317    0.7059    0.7186        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5750    0.7188    0.6389        32
      Comparison.Concession     0.5532    0.4407    0.4906        59
      Expansion.Conjunction     0.4146    0.3778    0.3953        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5479       292
                  macro avg     0.3402    0.3748    0.3507       292
               weighted avg     0.5206    0.5479    0.5258       292

Epoch [7/15]
top-down:TOP: Iter:   2500,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   1.9,  Val Acc: 72.02%, Val F1: 72.28% Time: 9.268841743469238 
top-down:SEC: Iter:   2500,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   1.9,  Val Acc: 57.14%, Val F1: 40.91% Time: 9.268841743469238 
top-down:CONN: Iter:   2500,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   1.9,  Val Acc: 99.40%, Val F1: 49.85% Time: 9.268841743469238 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.6e+01,  Train Acc: 81.25%,Val Loss:   1.8,  Val Acc: 72.02%, Val F1: 72.49% Time: 100.06445837020874 
top-down:SEC: Iter:   2600,  Train Loss: 3.6e+01,  Train Acc: 71.88%,Val Loss:   1.8,  Val Acc: 58.93%, Val F1: 32.63% Time: 100.06445837020874 
top-down:CONN: Iter:   2600,  Train Loss: 3.6e+01,  Train Acc: 34.38%,Val Loss:   1.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 100.06445837020874 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   1.9,  Val Acc: 72.02%, Val F1: 71.90% Time: 191.0408637523651 
top-down:SEC: Iter:   2700,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   1.9,  Val Acc: 58.33%, Val F1: 31.94% Time: 191.0408637523651 
top-down:CONN: Iter:   2700,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 191.0408637523651 
 
 
top-down:TOP: Iter:   2800,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   1.9,  Val Acc: 73.81%, Val F1: 73.99% Time: 278.9297671318054 
top-down:SEC: Iter:   2800,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   1.9,  Val Acc: 58.33%, Val F1: 34.80% Time: 278.9297671318054 
top-down:CONN: Iter:   2800,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 278.9297671318054 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   1.8,  Val Acc: 70.83%, Val F1: 71.64% Time: 369.9209554195404 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   1.8,  Val Acc: 59.52%, Val F1: 36.38% Time: 369.9209554195404 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   1.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 369.9209554195404 
 
 
Train time usage: 376.84048295021057
Test time usage: 0.5832407474517822
TOP: Test Loss:   2.3,  Test Acc: 67.12%, Test F1: 67.51%
SEC: Test Loss:   2.3,  Test Acc: 55.82%, Test F1: 35.62%
CONN: Test Loss:   2.3,  Test Acc: 98.97%, Test F1: 33.16%
consistency_top_sec: 15.21%,  consistency_sec_conn: 15.59%, consistency_top_sec_conn: 15.11%
              precision    recall  f1-score   support

    Temporal     0.5139    0.6981    0.5920        53
 Contingency     0.8000    0.7059    0.7500        85
  Comparison     0.7632    0.6744    0.7160        43
   Expansion     0.6542    0.6306    0.6422       111

    accuracy                         0.6712       292
   macro avg     0.6828    0.6773    0.6751       292
weighted avg     0.6872    0.6712    0.6753       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4521    0.7333    0.5593        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7191    0.7529    0.7356        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6286    0.6875    0.6567        32
      Comparison.Concession     0.5750    0.3898    0.4646        59
      Expansion.Conjunction     0.4038    0.4667    0.4330        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5582       292
                  macro avg     0.3473    0.3788    0.3562       292
               weighted avg     0.5263    0.5582    0.5329       292

Epoch [8/15]
top-down:TOP: Iter:   3000,  Train Loss: 3.3e+01,  Train Acc: 96.88%,Val Loss:   1.9,  Val Acc: 71.43%, Val F1: 72.39% Time: 83.50238227844238 
top-down:SEC: Iter:   3000,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   1.9,  Val Acc: 59.52%, Val F1: 37.01% Time: 83.50238227844238 
top-down:CONN: Iter:   3000,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 83.50238227844238 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   1.8,  Val Acc: 70.83%, Val F1: 70.89% Time: 175.04833054542542 
top-down:SEC: Iter:   3100,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   1.8,  Val Acc: 58.93%, Val F1: 35.94% Time: 175.04833054542542 
top-down:CONN: Iter:   3100,  Train Loss: 3.3e+01,  Train Acc: 56.25%,Val Loss:   1.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 175.04833054542542 
 
 
top-down:TOP: Iter:   3200,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   1.8,  Val Acc: 73.21%, Val F1: 73.97% Time: 270.1330223083496 *
top-down:SEC: Iter:   3200,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   1.8,  Val Acc: 58.33%, Val F1: 46.87% Time: 270.1330223083496 *
top-down:CONN: Iter:   3200,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   1.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 270.1330223083496 *
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   1.8,  Val Acc: 73.21%, Val F1: 73.48% Time: 363.58407616615295 *
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   1.8,  Val Acc: 60.12%, Val F1: 47.12% Time: 363.58407616615295 *
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   1.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 363.58407616615295 *
 
 
Train time usage: 383.00551414489746
Test time usage: 0.5987734794616699
TOP: Test Loss:   2.5,  Test Acc: 66.10%, Test F1: 67.04%
SEC: Test Loss:   2.5,  Test Acc: 56.85%, Test F1: 36.50%
CONN: Test Loss:   2.5,  Test Acc: 99.32%, Test F1: 33.22%
consistency_top_sec: 15.30%,  consistency_sec_conn: 15.88%, consistency_top_sec_conn: 15.21%
              precision    recall  f1-score   support

    Temporal     0.4872    0.7170    0.5802        53
 Contingency     0.8169    0.6824    0.7436        85
  Comparison     0.8056    0.6744    0.7342        43
   Expansion     0.6355    0.6126    0.6239       111

    accuracy                         0.6610       292
   macro avg     0.6863    0.6716    0.6704       292
weighted avg     0.6864    0.6610    0.6670       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4321    0.7778    0.5556        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7792    0.7059    0.7407        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6286    0.6875    0.6567        32
      Comparison.Concession     0.5800    0.4915    0.5321        59
      Expansion.Conjunction     0.4255    0.4444    0.4348        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5685       292
                  macro avg     0.3557    0.3884    0.3650       292
               weighted avg     0.5451    0.5685    0.5477       292

Epoch [9/15]
top-down:TOP: Iter:   3400,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   1.9,  Val Acc: 70.83%, Val F1: 70.97% Time: 73.8430848121643 
top-down:SEC: Iter:   3400,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   1.9,  Val Acc: 58.93%, Val F1: 47.70% Time: 73.8430848121643 
top-down:CONN: Iter:   3400,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 73.8430848121643 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.6e+01,  Train Acc: 93.75%,Val Loss:   1.9,  Val Acc: 73.21%, Val F1: 73.28% Time: 162.7241747379303 
top-down:SEC: Iter:   3500,  Train Loss: 3.6e+01,  Train Acc: 84.38%,Val Loss:   1.9,  Val Acc: 60.12%, Val F1: 37.08% Time: 162.7241747379303 
top-down:CONN: Iter:   3500,  Train Loss: 3.6e+01,  Train Acc: 59.38%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 162.7241747379303 
 
 
top-down:TOP: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   1.8,  Val Acc: 72.02%, Val F1: 72.26% Time: 253.4282991886139 
top-down:SEC: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   1.8,  Val Acc: 59.52%, Val F1: 35.60% Time: 253.4282991886139 
top-down:CONN: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   1.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 253.4282991886139 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   2.0,  Val Acc: 73.21%, Val F1: 73.78% Time: 347.09458208084106 *
top-down:SEC: Iter:   3700,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   2.0,  Val Acc: 61.90%, Val F1: 45.45% Time: 347.09458208084106 *
top-down:CONN: Iter:   3700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 347.09458208084106 *
 
 
Train time usage: 381.6633427143097
Test time usage: 0.5714473724365234
TOP: Test Loss:   2.6,  Test Acc: 66.10%, Test F1: 67.00%
SEC: Test Loss:   2.6,  Test Acc: 56.16%, Test F1: 31.65%
CONN: Test Loss:   2.6,  Test Acc: 98.97%, Test F1: 33.16%
consistency_top_sec: 15.50%,  consistency_sec_conn: 15.69%, consistency_top_sec_conn: 15.40%
              precision    recall  f1-score   support

    Temporal     0.4819    0.7547    0.5882        53
 Contingency     0.8182    0.7412    0.7778        85
  Comparison     0.7500    0.6977    0.7229        43
   Expansion     0.6522    0.5405    0.5911       111

    accuracy                         0.6610       292
   macro avg     0.6756    0.6835    0.6700       292
weighted avg     0.6840    0.6610    0.6643       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4177    0.7333    0.5323        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7529    0.7529    0.7529        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5610    0.7188    0.6301        32
      Comparison.Concession     0.6579    0.4237    0.5155        59
      Expansion.Conjunction     0.4130    0.4222    0.4176        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.5636    0.5616    0.5626       292
                  macro avg     0.3503    0.3814    0.3560       292
               weighted avg     0.5416    0.5616    0.5388       292

Epoch [10/15]
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   1.9,  Val Acc: 74.40%, Val F1: 74.71% Time: 61.563786029815674 *
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   1.9,  Val Acc: 60.71%, Val F1: 48.85% Time: 61.563786029815674 *
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 61.563786029815674 *
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   1.9,  Val Acc: 74.40%, Val F1: 74.64% Time: 151.22130131721497 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   1.9,  Val Acc: 60.12%, Val F1: 49.19% Time: 151.22130131721497 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 151.22130131721497 
 
 
top-down:TOP: Iter:   4000,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   2.0,  Val Acc: 73.21%, Val F1: 73.32% Time: 240.57065725326538 
top-down:SEC: Iter:   4000,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   2.0,  Val Acc: 61.31%, Val F1: 48.08% Time: 240.57065725326538 
top-down:CONN: Iter:   4000,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 240.57065725326538 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   1.9,  Val Acc: 72.02%, Val F1: 73.01% Time: 329.04642367362976 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   1.9,  Val Acc: 59.52%, Val F1: 46.86% Time: 329.04642367362976 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 329.04642367362976 
 
 
Train time usage: 375.98285722732544
Test time usage: 0.5974891185760498
TOP: Test Loss:   2.6,  Test Acc: 66.44%, Test F1: 66.87%
SEC: Test Loss:   2.6,  Test Acc: 57.53%, Test F1: 30.83%
CONN: Test Loss:   2.6,  Test Acc: 98.97%, Test F1: 24.87%
consistency_top_sec: 15.98%,  consistency_sec_conn: 16.07%, consistency_top_sec_conn: 15.88%
              precision    recall  f1-score   support

    Temporal     0.5000    0.6792    0.5760        53
 Contingency     0.7805    0.7529    0.7665        85
  Comparison     0.7632    0.6744    0.7160        43
   Expansion     0.6500    0.5856    0.6161       111

    accuracy                         0.6644       292
   macro avg     0.6734    0.6730    0.6687       292
weighted avg     0.6774    0.6644    0.6673       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4459    0.7333    0.5546        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7647    0.7647    0.7647        85
Contingency.Pragmatic cause     0.3333    0.0909    0.1429        11
        Comparison.Contrast     0.6471    0.6875    0.6667        32
      Comparison.Concession     0.6136    0.4576    0.5243        59
      Expansion.Conjunction     0.4167    0.4444    0.4301        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.5793    0.5753    0.5773       292
                  macro avg     0.4027    0.3973    0.3854       292
               weighted avg     0.5630    0.5753    0.5587       292

Epoch [11/15]
top-down:TOP: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   1.9,  Val Acc: 72.02%, Val F1: 72.62% Time: 43.33629107475281 
top-down:SEC: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   1.9,  Val Acc: 60.71%, Val F1: 42.92% Time: 43.33629107475281 
top-down:CONN: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 43.33629107475281 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   2.0,  Val Acc: 73.81%, Val F1: 74.33% Time: 137.0597951412201 
top-down:SEC: Iter:   4300,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   2.0,  Val Acc: 60.12%, Val F1: 49.38% Time: 137.0597951412201 
top-down:CONN: Iter:   4300,  Train Loss: 2.4e+01,  Train Acc: 53.12%,Val Loss:   2.0,  Val Acc: 99.40%, Val F1: 49.85% Time: 137.0597951412201 
 
 
top-down:TOP: Iter:   4400,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   1.9,  Val Acc: 72.62%, Val F1: 72.86% Time: 225.58035588264465 
top-down:SEC: Iter:   4400,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   1.9,  Val Acc: 60.12%, Val F1: 49.54% Time: 225.58035588264465 
top-down:CONN: Iter:   4400,  Train Loss: 2.7e+01,  Train Acc: 37.50%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 225.58035588264465 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   2.0,  Val Acc: 73.21%, Val F1: 73.60% Time: 316.55141258239746 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   2.0,  Val Acc: 60.12%, Val F1: 49.71% Time: 316.55141258239746 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 316.55141258239746 
 
 
Train time usage: 375.656800031662
Test time usage: 0.5833735466003418
TOP: Test Loss:   2.5,  Test Acc: 66.78%, Test F1: 67.25%
SEC: Test Loss:   2.5,  Test Acc: 57.19%, Test F1: 32.39%
CONN: Test Loss:   2.5,  Test Acc: 99.32%, Test F1: 33.22%
consistency_top_sec: 15.40%,  consistency_sec_conn: 15.98%, consistency_top_sec_conn: 15.30%
              precision    recall  f1-score   support

    Temporal     0.5000    0.7170    0.5891        53
 Contingency     0.7805    0.7529    0.7665        85
  Comparison     0.7632    0.6744    0.7160        43
   Expansion     0.6667    0.5766    0.6184       111

    accuracy                         0.6678       292
   macro avg     0.6776    0.6802    0.6725       292
weighted avg     0.6838    0.6678    0.6706       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4493    0.6889    0.5439        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7386    0.7647    0.7514        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5789    0.6875    0.6286        32
      Comparison.Concession     0.6170    0.4915    0.5472        59
      Expansion.Conjunction     0.4444    0.4444    0.4444        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.5739    0.5719    0.5729       292
                  macro avg     0.3535    0.3846    0.3644       292
               weighted avg     0.5409    0.5719    0.5505       292

Epoch [12/15]
top-down:TOP: Iter:   4600,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   2.0,  Val Acc: 73.21%, Val F1: 73.38% Time: 32.158087730407715 
top-down:SEC: Iter:   4600,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   2.0,  Val Acc: 60.71%, Val F1: 49.97% Time: 32.158087730407715 
top-down:CONN: Iter:   4600,  Train Loss: 2.4e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 32.158087730407715 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   2.0,  Val Acc: 72.62%, Val F1: 72.81% Time: 123.42532277107239 
top-down:SEC: Iter:   4700,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   2.0,  Val Acc: 60.12%, Val F1: 50.00% Time: 123.42532277107239 
top-down:CONN: Iter:   4700,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 123.42532277107239 
 
 
top-down:TOP: Iter:   4800,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   2.0,  Val Acc: 72.02%, Val F1: 72.23% Time: 217.07084846496582 
top-down:SEC: Iter:   4800,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   2.0,  Val Acc: 60.12%, Val F1: 48.85% Time: 217.07084846496582 
top-down:CONN: Iter:   4800,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 217.07084846496582 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   1.9,  Val Acc: 72.62%, Val F1: 72.72% Time: 309.71748781204224 
top-down:SEC: Iter:   4900,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   1.9,  Val Acc: 60.12%, Val F1: 48.37% Time: 309.71748781204224 
top-down:CONN: Iter:   4900,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 309.71748781204224 
 
 
Train time usage: 383.63368558883667
Test time usage: 0.5911571979522705
TOP: Test Loss:   2.6,  Test Acc: 65.07%, Test F1: 65.40%
SEC: Test Loss:   2.6,  Test Acc: 56.16%, Test F1: 28.82%
CONN: Test Loss:   2.6,  Test Acc: 99.32%, Test F1: 33.22%
consistency_top_sec: 15.30%,  consistency_sec_conn: 15.69%, consistency_top_sec_conn: 15.21%
              precision    recall  f1-score   support

    Temporal     0.4868    0.6981    0.5736        53
 Contingency     0.7654    0.7294    0.7470        85
  Comparison     0.7073    0.6744    0.6905        43
   Expansion     0.6596    0.5586    0.6049       111

    accuracy                         0.6507       292
   macro avg     0.6548    0.6651    0.6540       292
weighted avg     0.6661    0.6507    0.6532       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4400    0.7333    0.5500        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7159    0.7412    0.7283        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6000    0.7500    0.6667        32
      Comparison.Concession     0.5814    0.4237    0.4902        59
      Expansion.Conjunction     0.4750    0.4222    0.4471        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.5655    0.5616    0.5636       292
                  macro avg     0.3515    0.3838    0.3603       292
               weighted avg     0.5326    0.5616    0.5378       292

Epoch [13/15]
top-down:TOP: Iter:   5000,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   2.0,  Val Acc: 72.62%, Val F1: 72.85% Time: 18.488882541656494 
top-down:SEC: Iter:   5000,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   2.0,  Val Acc: 58.93%, Val F1: 47.80% Time: 18.488882541656494 
top-down:CONN: Iter:   5000,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 18.488882541656494 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   2.0,  Val Acc: 71.43%, Val F1: 71.80% Time: 108.1990716457367 
top-down:SEC: Iter:   5100,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   2.0,  Val Acc: 58.33%, Val F1: 48.98% Time: 108.1990716457367 
top-down:CONN: Iter:   5100,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 108.1990716457367 
 
 
top-down:TOP: Iter:   5200,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   2.0,  Val Acc: 72.02%, Val F1: 72.30% Time: 199.4294261932373 
top-down:SEC: Iter:   5200,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   2.0,  Val Acc: 58.33%, Val F1: 47.50% Time: 199.4294261932373 
top-down:CONN: Iter:   5200,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 199.4294261932373 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   2.0,  Val Acc: 73.21%, Val F1: 73.56% Time: 292.99324655532837 
top-down:SEC: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   2.0,  Val Acc: 60.71%, Val F1: 49.25% Time: 292.99324655532837 
top-down:CONN: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 292.99324655532837 
 
 
Train time usage: 383.5176610946655
Test time usage: 0.5934545993804932
TOP: Test Loss:   2.6,  Test Acc: 66.44%, Test F1: 66.78%
SEC: Test Loss:   2.6,  Test Acc: 56.85%, Test F1: 34.44%
CONN: Test Loss:   2.6,  Test Acc: 98.97%, Test F1: 33.16%
consistency_top_sec: 15.30%,  consistency_sec_conn: 15.88%, consistency_top_sec_conn: 15.21%
              precision    recall  f1-score   support

    Temporal     0.5000    0.6981    0.5827        53
 Contingency     0.7973    0.6941    0.7421        85
  Comparison     0.7436    0.6744    0.7073        43
   Expansion     0.6571    0.6216    0.6389       111

    accuracy                         0.6644       292
   macro avg     0.6745    0.6721    0.6678       292
weighted avg     0.6821    0.6644    0.6688       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4507    0.7111    0.5517        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7848    0.7294    0.7561        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5789    0.6875    0.6286        32
      Comparison.Concession     0.6042    0.4915    0.5421        59
      Expansion.Conjunction     0.4000    0.4444    0.4211        45
    Expansion.Instantiation     0.3333    0.1429    0.2000         7

                  micro avg     0.5704    0.5685    0.5695       292
                  macro avg     0.3940    0.4009    0.3874       292
               weighted avg     0.5531    0.5685    0.5532       292

Epoch [14/15]
top-down:TOP: Iter:   5400,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   2.0,  Val Acc: 70.24%, Val F1: 70.87% Time: 4.988206386566162 
top-down:SEC: Iter:   5400,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   2.0,  Val Acc: 59.52%, Val F1: 48.25% Time: 4.988206386566162 
top-down:CONN: Iter:   5400,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 4.988206386566162 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   2.0,  Val Acc: 72.62%, Val F1: 73.12% Time: 94.32298755645752 
top-down:SEC: Iter:   5500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   2.0,  Val Acc: 60.71%, Val F1: 49.16% Time: 94.32298755645752 
top-down:CONN: Iter:   5500,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 94.32298755645752 
 
 
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   2.0,  Val Acc: 72.62%, Val F1: 72.82% Time: 185.49568247795105 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   2.0,  Val Acc: 58.93%, Val F1: 47.92% Time: 185.49568247795105 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 185.49568247795105 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   2.0,  Val Acc: 72.62%, Val F1: 72.92% Time: 274.8476777076721 
top-down:SEC: Iter:   5700,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   2.0,  Val Acc: 59.52%, Val F1: 48.97% Time: 274.8476777076721 
top-down:CONN: Iter:   5700,  Train Loss: 2.5e+01,  Train Acc: 34.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 274.8476777076721 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   2.0,  Val Acc: 73.21%, Val F1: 73.88% Time: 366.94450330734253 
top-down:SEC: Iter:   5800,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   2.0,  Val Acc: 60.71%, Val F1: 49.63% Time: 366.94450330734253 
top-down:CONN: Iter:   5800,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 366.94450330734253 
 
 
Train time usage: 378.215491771698
Test time usage: 0.5811278820037842
TOP: Test Loss:   2.7,  Test Acc: 65.41%, Test F1: 65.95%
SEC: Test Loss:   2.7,  Test Acc: 56.85%, Test F1: 32.30%
CONN: Test Loss:   2.7,  Test Acc: 98.97%, Test F1: 33.16%
consistency_top_sec: 15.30%,  consistency_sec_conn: 15.88%, consistency_top_sec_conn: 15.21%
              precision    recall  f1-score   support

    Temporal     0.4805    0.6981    0.5692        53
 Contingency     0.7895    0.7059    0.7453        85
  Comparison     0.7436    0.6744    0.7073        43
   Expansion     0.6500    0.5856    0.6161       111

    accuracy                         0.6541       292
   macro avg     0.6659    0.6660    0.6595       292
weighted avg     0.6736    0.6541    0.6587       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4583    0.7333    0.5641        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7590    0.7412    0.7500        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5789    0.6875    0.6286        32
      Comparison.Concession     0.6222    0.4746    0.5385        59
      Expansion.Conjunction     0.4082    0.4444    0.4255        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.5704    0.5685    0.5695       292
                  macro avg     0.3533    0.3851    0.3633       292
               weighted avg     0.5437    0.5685    0.5485       292

Epoch [15/15]
top-down:TOP: Iter:   5900,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   2.0,  Val Acc: 73.81%, Val F1: 74.38% Time: 83.1525731086731 
top-down:SEC: Iter:   5900,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   2.0,  Val Acc: 60.71%, Val F1: 49.35% Time: 83.1525731086731 
top-down:CONN: Iter:   5900,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 83.1525731086731 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   2.0,  Val Acc: 72.02%, Val F1: 72.52% Time: 172.80803036689758 
top-down:SEC: Iter:   6000,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   2.0,  Val Acc: 61.31%, Val F1: 49.40% Time: 172.80803036689758 
top-down:CONN: Iter:   6000,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 172.80803036689758 
 
 
top-down:TOP: Iter:   6100,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   2.0,  Val Acc: 72.62%, Val F1: 73.19% Time: 262.1541564464569 
top-down:SEC: Iter:   6100,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   2.0,  Val Acc: 61.31%, Val F1: 49.37% Time: 262.1541564464569 
top-down:CONN: Iter:   6100,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 262.1541564464569 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   2.0,  Val Acc: 73.21%, Val F1: 73.79% Time: 350.94784116744995 
top-down:SEC: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   2.0,  Val Acc: 60.71%, Val F1: 48.76% Time: 350.94784116744995 
top-down:CONN: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 40.62%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 350.94784116744995 
 
 
Train time usage: 375.63522601127625
Test time usage: 0.6085677146911621
TOP: Test Loss:   2.7,  Test Acc: 66.78%, Test F1: 67.07%
SEC: Test Loss:   2.7,  Test Acc: 56.85%, Test F1: 32.35%
CONN: Test Loss:   2.7,  Test Acc: 98.63%, Test F1: 33.10%
consistency_top_sec: 15.59%,  consistency_sec_conn: 15.78%, consistency_top_sec_conn: 15.40%
              precision    recall  f1-score   support

    Temporal     0.5135    0.7170    0.5984        53
 Contingency     0.7875    0.7412    0.7636        85
  Comparison     0.7250    0.6744    0.6988        43
   Expansion     0.6633    0.5856    0.6220       111

    accuracy                         0.6678       292
   macro avg     0.6723    0.6795    0.6707       292
weighted avg     0.6813    0.6678    0.6703       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4648    0.7333    0.5690        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7412    0.7412    0.7412        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5946    0.6875    0.6377        32
      Comparison.Concession     0.6222    0.4746    0.5385        59
      Expansion.Conjunction     0.4082    0.4444    0.4255        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.5704    0.5685    0.5695       292
                  macro avg     0.3539    0.3851    0.3640       292
               weighted avg     0.5412    0.5685    0.5477       292

dev_best_acc_top: 74.40%,  dev_best_f1_top: 74.71%, 
dev_best_acc_sec: 60.71%,  dev_best_f1_sec: 48.85%, 
dev_best_acc_conn: 100.00%,  dev_best_f1_conn: 100.00%
