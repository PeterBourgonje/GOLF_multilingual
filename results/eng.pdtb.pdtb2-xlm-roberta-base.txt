nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/PDTB/Ji/data/', 'log_file': 'data/PDTB/Ji/log/', 'save_file': 'data/PDTB/Ji/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 30, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March06-16:47:14', 'log': 'data/PDTB/Ji/log/March06-16:47:14.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]1it [00:00,  6.13it/s]16it [00:00, 66.37it/s]36it [00:00, 116.61it/s]54it [00:00, 127.38it/s]68it [00:00, 113.09it/s]85it [00:00, 126.61it/s]113it [00:00, 151.77it/s]177it [00:01, 223.65it/s]198it [00:01, 196.67it/s]219it [00:01, 190.04it/s]238it [00:01, 186.55it/s]299it [00:01, 289.39it/s]365it [00:01, 321.33it/s]398it [00:02, 236.30it/s]425it [00:02, 234.83it/s]510it [00:02, 360.04it/s]552it [00:02, 371.46it/s]594it [00:02, 293.33it/s]629it [00:02, 296.94it/s]663it [00:02, 287.29it/s]701it [00:02, 308.73it/s]767it [00:03, 395.98it/s]810it [00:03, 397.72it/s]864it [00:03, 435.29it/s]910it [00:03, 388.19it/s]952it [00:03, 381.72it/s]992it [00:03, 362.31it/s]1030it [00:03, 350.83it/s]1093it [00:03, 413.31it/s]1136it [00:03, 394.38it/s]1177it [00:04, 350.30it/s]1220it [00:04, 364.08it/s]1279it [00:04, 422.66it/s]1361it [00:04, 529.60it/s]1445it [00:04, 613.05it/s]1509it [00:04, 528.27it/s]1566it [00:04, 487.19it/s]1650it [00:04, 558.92it/s]1709it [00:05, 389.02it/s]1757it [00:05, 381.15it/s]1824it [00:05, 379.45it/s]1867it [00:05, 294.86it/s]1937it [00:05, 362.70it/s]1981it [00:06, 290.78it/s]2017it [00:06, 277.20it/s]2076it [00:06, 337.11it/s]2125it [00:06, 365.61it/s]2167it [00:06, 281.15it/s]2202it [00:06, 262.81it/s]2271it [00:07, 331.93it/s]2320it [00:07, 356.99it/s]2360it [00:07, 285.41it/s]2394it [00:07, 282.25it/s]2454it [00:07, 314.86it/s]2488it [00:07, 253.65it/s]2517it [00:08, 225.94it/s]2546it [00:08, 235.78it/s]2577it [00:08, 250.87it/s]2604it [00:08, 216.90it/s]2630it [00:08, 221.90it/s]2744it [00:08, 439.34it/s]2795it [00:08, 410.49it/s]2841it [00:09, 307.61it/s]2879it [00:09, 319.74it/s]2953it [00:09, 354.83it/s]2992it [00:09, 280.62it/s]3045it [00:09, 328.40it/s]3120it [00:09, 408.10it/s]3167it [00:09, 377.13it/s]3210it [00:10, 342.61it/s]3288it [00:10, 439.45it/s]3383it [00:10, 561.17it/s]3446it [00:10, 573.62it/s]3509it [00:10, 527.28it/s]3597it [00:10, 616.75it/s]3664it [00:10, 579.08it/s]3726it [00:10, 487.89it/s]3780it [00:11, 447.65it/s]3829it [00:11, 435.08it/s]3919it [00:11, 527.55it/s]3975it [00:11, 419.40it/s]4022it [00:11, 364.79it/s]4092it [00:11, 434.22it/s]4142it [00:12, 342.81it/s]4183it [00:12, 284.63it/s]4218it [00:12, 254.90it/s]4248it [00:12, 208.81it/s]4273it [00:12, 192.59it/s]4312it [00:12, 228.18it/s]4363it [00:13, 280.52it/s]4396it [00:13, 236.17it/s]4424it [00:13, 213.31it/s]4486it [00:13, 295.36it/s]4529it [00:13, 318.59it/s]4566it [00:13, 312.82it/s]4602it [00:13, 321.04it/s]4653it [00:14, 368.23it/s]4693it [00:14, 313.58it/s]4728it [00:14, 274.93it/s]4759it [00:14, 266.05it/s]4798it [00:14, 294.82it/s]4830it [00:14, 297.30it/s]4871it [00:14, 319.96it/s]4905it [00:14, 312.78it/s]4938it [00:15, 309.40it/s]4977it [00:15, 311.88it/s]5009it [00:15, 310.35it/s]5110it [00:15, 502.16it/s]5163it [00:15, 429.86it/s]5210it [00:16, 136.63it/s]5244it [00:16, 146.84it/s]5274it [00:16, 161.33it/s]5303it [00:17, 154.44it/s]5352it [00:17, 195.57it/s]5380it [00:17, 190.51it/s]5405it [00:17, 196.22it/s]5429it [00:17, 194.91it/s]5540it [00:17, 371.02it/s]5583it [00:17, 342.93it/s]5622it [00:17, 352.30it/s]5661it [00:18, 334.13it/s]5700it [00:18, 346.99it/s]5834it [00:18, 560.63it/s]5891it [00:18, 484.18it/s]5942it [00:18, 388.68it/s]6057it [00:18, 550.03it/s]6181it [00:18, 710.21it/s]6288it [00:18, 728.32it/s]6369it [00:19, 532.31it/s]6435it [00:19, 361.31it/s]6495it [00:19, 397.51it/s]6555it [00:19, 431.26it/s]6611it [00:19, 449.99it/s]6665it [00:20, 377.08it/s]6711it [00:20, 321.95it/s]6753it [00:20, 339.31it/s]6793it [00:20, 313.16it/s]6829it [00:20, 245.92it/s]6858it [00:20, 246.52it/s]6887it [00:21, 254.92it/s]6915it [00:21, 220.39it/s]6940it [00:21, 215.65it/s]6964it [00:21, 219.23it/s]7054it [00:21, 384.73it/s]7148it [00:21, 507.54it/s]7203it [00:21, 414.28it/s]7250it [00:22, 371.02it/s]7374it [00:22, 563.20it/s]7494it [00:22, 652.43it/s]7566it [00:22, 446.25it/s]7641it [00:22, 489.64it/s]7701it [00:22, 409.64it/s]7751it [00:23, 388.45it/s]7863it [00:23, 532.63it/s]7942it [00:23, 584.88it/s]8011it [00:23, 532.52it/s]8072it [00:23, 427.50it/s]8182it [00:23, 562.95it/s]8251it [00:23, 504.88it/s]8311it [00:24, 436.27it/s]8362it [00:24, 410.36it/s]8408it [00:24, 413.64it/s]8461it [00:24, 439.56it/s]8515it [00:24, 462.39it/s]8565it [00:24, 425.11it/s]8621it [00:24, 453.65it/s]8669it [00:25, 434.17it/s]8714it [00:25, 416.56it/s]8762it [00:25, 429.70it/s]8806it [00:25, 418.52it/s]8850it [00:25, 424.09it/s]8893it [00:25, 423.25it/s]8938it [00:25, 429.63it/s]8982it [00:25, 299.51it/s]9018it [00:26, 311.96it/s]9114it [00:26, 467.26it/s]9223it [00:26, 623.57it/s]9325it [00:26, 728.97it/s]9405it [00:26, 553.32it/s]9472it [00:26, 466.75it/s]9528it [00:26, 418.98it/s]9613it [00:27, 505.64it/s]9752it [00:27, 702.85it/s]9836it [00:27, 464.23it/s]9902it [00:27, 406.98it/s]10028it [00:27, 524.17it/s]10095it [00:28, 346.87it/s]10147it [00:28, 337.18it/s]10193it [00:28, 263.56it/s]10229it [00:28, 270.80it/s]10264it [00:29, 237.66it/s]10293it [00:29, 239.64it/s]10406it [00:29, 406.06it/s]10469it [00:29, 398.88it/s]10518it [00:29, 320.72it/s]10559it [00:29, 318.90it/s]10628it [00:29, 391.00it/s]10675it [00:30, 283.08it/s]10713it [00:30, 281.20it/s]10766it [00:30, 323.95it/s]10846it [00:30, 416.98it/s]10895it [00:30, 327.04it/s]10936it [00:31, 270.37it/s]10970it [00:31, 253.35it/s]11067it [00:31, 388.47it/s]11117it [00:31, 353.02it/s]11161it [00:31, 333.28it/s]11200it [00:31, 341.89it/s]11301it [00:31, 492.66it/s]11358it [00:32, 356.36it/s]11404it [00:32, 320.39it/s]11455it [00:32, 347.89it/s]11497it [00:32, 294.56it/s]11532it [00:32, 253.47it/s]11654it [00:32, 435.98it/s]11712it [00:33, 439.86it/s]11766it [00:33, 433.63it/s]11816it [00:33, 279.39it/s]11856it [00:33, 283.84it/s]11893it [00:33, 227.03it/s]11923it [00:34, 215.02it/s]12016it [00:34, 334.50it/s]12060it [00:34, 301.57it/s]12098it [00:34, 282.40it/s]12131it [00:34, 274.89it/s]12224it [00:34, 412.27it/s]12274it [00:35, 350.52it/s]12316it [00:35, 324.91it/s]12354it [00:35, 310.71it/s]12436it [00:35, 421.53it/s]12502it [00:35, 459.37it/s]12547it [00:35, 351.45it/s]
0it [00:00, ?it/s]79it [00:00, 783.34it/s]158it [00:00, 360.35it/s]292it [00:00, 630.78it/s]379it [00:00, 554.40it/s]450it [00:00, 501.10it/s]510it [00:01, 483.94it/s]590it [00:01, 555.59it/s]653it [00:01, 537.17it/s]712it [00:01, 419.46it/s]761it [00:01, 412.82it/s]808it [00:01, 417.47it/s]853it [00:01, 423.46it/s]898it [00:01, 377.02it/s]938it [00:02, 342.78it/s]989it [00:02, 380.02it/s]1063it [00:02, 461.02it/s]1112it [00:02, 445.46it/s]1159it [00:02, 434.06it/s]1165it [00:02, 455.94it/s]
0it [00:00, ?it/s]43it [00:00, 413.84it/s]85it [00:00, 392.36it/s]135it [00:00, 420.16it/s]178it [00:00, 418.04it/s]220it [00:00, 415.54it/s]262it [00:00, 406.36it/s]303it [00:00, 406.61it/s]363it [00:00, 464.60it/s]433it [00:00, 517.65it/s]501it [00:01, 565.43it/s]558it [00:01, 495.84it/s]610it [00:01, 377.05it/s]653it [00:01, 361.94it/s]693it [00:01, 368.16it/s]733it [00:02, 229.48it/s]764it [00:02, 213.62it/s]871it [00:02, 368.34it/s]922it [00:02, 302.59it/s]964it [00:02, 259.97it/s]999it [00:02, 251.34it/s]1039it [00:03, 343.45it/s]
Time usage: 66.24606561660767
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/30]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 55.88%, Val F1: 17.92% Time: 290.39780354499817 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   7.5,  Val Acc: 24.21%, Val F1:  3.54% Time: 290.39780354499817 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  0.00%,Val Loss:   7.5,  Val Acc:  4.03%, Val F1:  0.30% Time: 290.39780354499817 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 55.88%, Val F1: 17.92% Time: 568.5878849029541 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.6,  Val Acc: 27.47%, Val F1:  5.78% Time: 568.5878849029541 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.6,  Val Acc: 12.88%, Val F1:  0.34% Time: 568.5878849029541 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 65.62%,Val Loss:   6.3,  Val Acc: 55.62%, Val F1: 25.47% Time: 852.8009150028229 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 25.00%,Val Loss:   6.3,  Val Acc: 32.70%, Val F1: 10.49% Time: 852.8009150028229 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 15.62%,Val Loss:   6.3,  Val Acc: 17.08%, Val F1:  1.09% Time: 852.8009150028229 *
 
 
Train time usage: 1078.721400976181
Test time usage: 1.8748443126678467
TOP: Test Loss:   6.3,  Test Acc: 54.57%, Test F1: 21.44%
SEC: Test Loss:   6.3,  Test Acc: 35.13%, Test F1: 11.79%
CONN: Test Loss:   6.3,  Test Acc: 16.75%, Test F1:  1.24%
consistency_top_sec: 25.60%,  consistency_sec_conn: 12.70%, consistency_top_sec_conn:  7.60%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        68
 Contingency     0.5814    0.0926    0.1597       270
  Comparison     0.0000    0.0000    0.0000       144
   Expansion     0.5442    0.9731    0.6980       557

    accuracy                         0.5457      1039
   macro avg     0.2814    0.2664    0.2144      1039
weighted avg     0.4428    0.5457    0.4157      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5059    0.4776    0.4914       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.0000    0.0000    0.0000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3974    0.4650    0.4286       200
    Expansion.Instantiation     0.0000    0.0000    0.0000       118
      Expansion.Restatement     0.2609    0.6792    0.3770       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.3513      1039
                  macro avg     0.1058    0.1474    0.1179      1039
               weighted avg     0.2602    0.3513    0.2862      1039

Epoch [2/30]
top-down:TOP: Iter:    400,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.3,  Val Acc: 56.05%, Val F1: 31.89% Time: 22.74835777282715 *
top-down:SEC: Iter:    400,  Train Loss: 3e+01,  Train Acc: 31.25%,Val Loss:   6.3,  Val Acc: 33.56%, Val F1: 11.18% Time: 22.74835777282715 *
top-down:CONN: Iter:    400,  Train Loss: 3e+01,  Train Acc:  6.25%,Val Loss:   6.3,  Val Acc: 17.77%, Val F1:  1.49% Time: 22.74835777282715 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 51.85%, Val F1: 29.20% Time: 267.84220242500305 
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.2,  Val Acc: 35.71%, Val F1: 11.70% Time: 267.84220242500305 
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   6.2,  Val Acc: 18.28%, Val F1:  1.62% Time: 267.84220242500305 
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   5.9,  Val Acc: 56.74%, Val F1: 43.24% Time: 527.0104646682739 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   5.9,  Val Acc: 40.26%, Val F1: 20.02% Time: 527.0104646682739 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 21.88%,Val Loss:   5.9,  Val Acc: 22.75%, Val F1:  2.93% Time: 527.0104646682739 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 58.37%, Val F1: 44.20% Time: 788.9093692302704 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 41.46%, Val F1: 19.03% Time: 788.9093692302704 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.7,  Val Acc: 22.83%, Val F1:  3.08% Time: 788.9093692302704 *
 
 
Train time usage: 1009.0168523788452
Test time usage: 1.9251866340637207
TOP: Test Loss:   5.2,  Test Acc: 62.56%, Test F1: 52.13%
SEC: Test Loss:   5.2,  Test Acc: 48.22%, Test F1: 27.05%
CONN: Test Loss:   5.2,  Test Acc: 25.22%, Test F1:  4.62%
consistency_top_sec: 42.54%,  consistency_sec_conn: 21.27%, consistency_top_sec_conn: 19.25%
              precision    recall  f1-score   support

    Temporal     0.4359    0.5000    0.4658        68
 Contingency     0.5818    0.3529    0.4394       272
  Comparison     0.6203    0.3403    0.4395       144
   Expansion     0.6569    0.8486    0.7406       555

    accuracy                         0.6256      1039
   macro avg     0.5737    0.5105    0.5213      1039
weighted avg     0.6177    0.6256    0.6020      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3908    0.6296    0.4823        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5526    0.5485    0.5506       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4800    0.3750    0.4211       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4658    0.5450    0.5023       200
    Expansion.Instantiation     0.6048    0.6356    0.6198       118
      Expansion.Restatement     0.3860    0.4151    0.4000       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4822      1039
                  macro avg     0.2618    0.2863    0.2705      1039
               weighted avg     0.4591    0.4822    0.4677      1039

Epoch [3/30]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   5.4,  Val Acc: 61.20%, Val F1: 51.28% Time: 48.25715208053589 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   5.4,  Val Acc: 47.38%, Val F1: 26.78% Time: 48.25715208053589 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 31.25%,Val Loss:   5.4,  Val Acc: 24.55%, Val F1:  4.05% Time: 48.25715208053589 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 61.89%, Val F1: 48.29% Time: 343.9707224369049 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 49.44%, Val F1: 28.94% Time: 343.9707224369049 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 25.00%,Val Loss:   5.4,  Val Acc: 26.52%, Val F1:  4.25% Time: 343.9707224369049 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   5.3,  Val Acc: 62.83%, Val F1: 42.08% Time: 594.4821004867554 
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.3,  Val Acc: 48.84%, Val F1: 24.89% Time: 594.4821004867554 
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   5.3,  Val Acc: 28.07%, Val F1:  4.64% Time: 594.4821004867554 
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   5.3,  Val Acc: 62.40%, Val F1: 49.60% Time: 834.7167205810547 *
top-down:SEC: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   5.3,  Val Acc: 49.44%, Val F1: 29.05% Time: 834.7167205810547 *
top-down:CONN: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   5.3,  Val Acc: 27.30%, Val F1:  5.08% Time: 834.7167205810547 *
 
 
Train time usage: 1039.5590801239014
Test time usage: 2.229285955429077
TOP: Test Loss:   5.1,  Test Acc: 63.43%, Test F1: 53.32%
SEC: Test Loss:   5.1,  Test Acc: 49.66%, Test F1: 31.31%
CONN: Test Loss:   5.1,  Test Acc: 25.12%, Test F1:  5.92%
consistency_top_sec: 44.85%,  consistency_sec_conn: 19.35%, consistency_top_sec_conn: 17.52%
              precision    recall  f1-score   support

    Temporal     0.5000    0.3529    0.4138        68
 Contingency     0.6689    0.3640    0.4714       272
  Comparison     0.4647    0.5486    0.5032       144
   Expansion     0.6790    0.8234    0.7443       555

    accuracy                         0.6343      1039
   macro avg     0.5782    0.5222    0.5332      1039
weighted avg     0.6350    0.6343    0.6178      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3889    0.3889    0.3889        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6018    0.4981    0.5451       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4154    0.6328    0.5015       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4270    0.6000    0.4990       200
    Expansion.Instantiation     0.6555    0.6555    0.6555       119
      Expansion.Restatement     0.4878    0.3774    0.4255       212
      Expansion.Alternative     0.6000    0.3333    0.4286         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3251    0.3169    0.3131      1039
               weighted avg     0.4880    0.4966    0.4837      1039

Epoch [4/30]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   5.3,  Val Acc: 63.95%, Val F1: 52.74% Time: 64.82073950767517 *
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 68.75%,Val Loss:   5.3,  Val Acc: 50.73%, Val F1: 28.69% Time: 64.82073950767517 *
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 28.50%, Val F1:  5.73% Time: 64.82073950767517 *
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 78.12%,Val Loss:   5.3,  Val Acc: 64.46%, Val F1: 50.42% Time: 325.2195258140564 *
top-down:SEC: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 53.12%,Val Loss:   5.3,  Val Acc: 50.21%, Val F1: 31.77% Time: 325.2195258140564 *
top-down:CONN: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 28.24%, Val F1:  5.48% Time: 325.2195258140564 *
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   5.3,  Val Acc: 62.32%, Val F1: 50.70% Time: 585.3528232574463 
top-down:SEC: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.3,  Val Acc: 49.53%, Val F1: 31.77% Time: 585.3528232574463 
top-down:CONN: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.3,  Val Acc: 27.81%, Val F1:  5.63% Time: 585.3528232574463 
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   5.3,  Val Acc: 60.77%, Val F1: 50.58% Time: 878.0144596099854 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 43.75%,Val Loss:   5.3,  Val Acc: 49.18%, Val F1: 31.51% Time: 878.0144596099854 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 34.38%,Val Loss:   5.3,  Val Acc: 26.70%, Val F1:  6.00% Time: 878.0144596099854 
 
 
Train time usage: 1067.4979894161224
Test time usage: 2.4777278900146484
TOP: Test Loss:   4.9,  Test Acc: 63.91%, Test F1: 55.72%
SEC: Test Loss:   4.9,  Test Acc: 52.26%, Test F1: 34.43%
CONN: Test Loss:   4.9,  Test Acc: 26.18%, Test F1:  7.69%
consistency_top_sec: 48.99%,  consistency_sec_conn: 21.08%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.6667    0.3235    0.4356        68
 Contingency     0.5809    0.5109    0.5437       274
  Comparison     0.4637    0.5764    0.5139       144
   Expansion     0.7150    0.7577    0.7357       553

    accuracy                         0.6391      1039
   macro avg     0.6066    0.5421    0.5572      1039
weighted avg     0.6417    0.6391    0.6347      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6216    0.4259    0.5055        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5433    0.5880    0.5647       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4216    0.6094    0.4984       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5301    0.4850    0.5065       200
    Expansion.Instantiation     0.7426    0.6303    0.6818       119
      Expansion.Restatement     0.4672    0.5047    0.4853       212
      Expansion.Alternative     0.4615    0.6667    0.5455         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5226      1039
                  macro avg     0.3444    0.3555    0.3443      1039
               weighted avg     0.5103    0.5226    0.5121      1039

Epoch [5/30]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 64.12%, Val F1: 52.92% Time: 81.06534051895142 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   5.3,  Val Acc: 50.82%, Val F1: 31.89% Time: 81.06534051895142 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   5.3,  Val Acc: 29.44%, Val F1:  6.52% Time: 81.06534051895142 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   5.3,  Val Acc: 62.58%, Val F1: 52.87% Time: 324.77678513526917 
top-down:SEC: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.3,  Val Acc: 50.13%, Val F1: 32.03% Time: 324.77678513526917 
top-down:CONN: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   5.3,  Val Acc: 27.81%, Val F1:  6.17% Time: 324.77678513526917 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 60.00%, Val F1: 51.97% Time: 574.0331664085388 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 48.84%, Val F1: 31.27% Time: 574.0331664085388 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   5.5,  Val Acc: 27.21%, Val F1:  6.24% Time: 574.0331664085388 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.4,  Val Acc: 62.92%, Val F1: 53.40% Time: 836.1269209384918 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 50.04%, Val F1: 31.91% Time: 836.1269209384918 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 30.04%, Val F1:  7.30% Time: 836.1269209384918 
 
 
Train time usage: 999.204309463501
Test time usage: 3.2980096340179443
TOP: Test Loss:   5.1,  Test Acc: 63.14%, Test F1: 56.28%
SEC: Test Loss:   5.1,  Test Acc: 51.01%, Test F1: 33.36%
CONN: Test Loss:   5.1,  Test Acc: 26.56%, Test F1:  7.93%
consistency_top_sec: 49.09%,  consistency_sec_conn: 22.43%, consistency_top_sec_conn: 22.04%
              precision    recall  f1-score   support

    Temporal     0.5952    0.3676    0.4545        68
 Contingency     0.5636    0.5657    0.5647       274
  Comparison     0.4793    0.5625    0.5176       144
   Expansion     0.7143    0.7143    0.7143       553

    accuracy                         0.6314      1039
   macro avg     0.5881    0.5525    0.5628      1039
weighted avg     0.6342    0.6314    0.6306      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5750    0.4259    0.4894        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5285    0.6231    0.5719       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4211    0.5625    0.4816       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4848    0.4800    0.4824       200
    Expansion.Instantiation     0.7196    0.6471    0.6814       119
      Expansion.Restatement     0.4635    0.4218    0.4417       211
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5101      1039
                  macro avg     0.3292    0.3479    0.3336      1039
               weighted avg     0.4917    0.5101    0.4974      1039

Epoch [6/30]
top-down:TOP: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   5.4,  Val Acc: 63.26%, Val F1: 52.60% Time: 89.25454020500183 *
top-down:SEC: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   5.4,  Val Acc: 50.90%, Val F1: 32.51% Time: 89.25454020500183 *
top-down:CONN: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 30.64%, Val F1:  7.28% Time: 89.25454020500183 *
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 63.26%, Val F1: 51.77% Time: 362.058974981308 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 50.30%, Val F1: 31.39% Time: 362.058974981308 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 28.58%, Val F1:  7.29% Time: 362.058974981308 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 62.32%, Val F1: 51.76% Time: 609.6261358261108 
top-down:SEC: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 51.07%, Val F1: 32.06% Time: 609.6261358261108 
top-down:CONN: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 30.21%, Val F1:  7.38% Time: 609.6261358261108 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 62.92%, Val F1: 53.17% Time: 858.8668570518494 *
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 62.50%,Val Loss:   5.5,  Val Acc: 51.59%, Val F1: 33.49% Time: 858.8668570518494 *
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 43.75%,Val Loss:   5.5,  Val Acc: 29.27%, Val F1:  7.42% Time: 858.8668570518494 *
 
 
Train time usage: 1008.0624446868896
Test time usage: 2.7074790000915527
TOP: Test Loss:   5.2,  Test Acc: 63.62%, Test F1: 55.81%
SEC: Test Loss:   5.2,  Test Acc: 51.59%, Test F1: 33.66%
CONN: Test Loss:   5.2,  Test Acc: 25.89%, Test F1:  7.76%
consistency_top_sec: 49.95%,  consistency_sec_conn: 21.08%, consistency_top_sec_conn: 20.60%
              precision    recall  f1-score   support

    Temporal     0.5652    0.3824    0.4561        68
 Contingency     0.5709    0.5292    0.5492       274
  Comparison     0.5000    0.4966    0.4983       145
   Expansion     0.7025    0.7572    0.7289       552

    accuracy                         0.6362      1039
   macro avg     0.5847    0.5413    0.5581      1039
weighted avg     0.6306    0.6362    0.6315      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6098    0.4630    0.5263        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5468    0.5672    0.5568       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4583    0.5156    0.4853       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4575    0.5650    0.5056       200
    Expansion.Instantiation     0.7238    0.6387    0.6786       119
      Expansion.Restatement     0.4757    0.4645    0.4700       211
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5159      1039
                  macro avg     0.3315    0.3528    0.3366      1039
               weighted avg     0.5000    0.5159    0.5054      1039

Epoch [7/30]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   5.7,  Val Acc: 61.29%, Val F1: 52.10% Time: 108.30563068389893 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 65.62%,Val Loss:   5.7,  Val Acc: 50.47%, Val F1: 32.37% Time: 108.30563068389893 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 25.00%,Val Loss:   5.7,  Val Acc: 28.24%, Val F1:  7.16% Time: 108.30563068389893 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 62.92%, Val F1: 51.95% Time: 357.64584827423096 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   5.6,  Val Acc: 51.07%, Val F1: 32.04% Time: 357.64584827423096 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   5.6,  Val Acc: 29.10%, Val F1:  7.62% Time: 357.64584827423096 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 63.61%, Val F1: 51.29% Time: 617.2486696243286 
top-down:SEC: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 62.50%,Val Loss:   5.8,  Val Acc: 52.19%, Val F1: 31.32% Time: 617.2486696243286 
top-down:CONN: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 31.25%,Val Loss:   5.8,  Val Acc: 29.53%, Val F1:  7.81% Time: 617.2486696243286 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   5.7,  Val Acc: 60.09%, Val F1: 50.19% Time: 883.544469833374 
top-down:SEC: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 49.36%, Val F1: 30.37% Time: 883.544469833374 
top-down:CONN: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 37.50%,Val Loss:   5.7,  Val Acc: 28.58%, Val F1:  7.26% Time: 883.544469833374 
 
 
Train time usage: 1035.0235800743103
Test time usage: 2.449338674545288
TOP: Test Loss:   5.4,  Test Acc: 63.14%, Test F1: 55.36%
SEC: Test Loss:   5.4,  Test Acc: 51.40%, Test F1: 33.26%
CONN: Test Loss:   5.4,  Test Acc: 28.39%, Test F1:  8.59%
consistency_top_sec: 49.66%,  consistency_sec_conn: 23.77%, consistency_top_sec_conn: 22.91%
              precision    recall  f1-score   support

    Temporal     0.5952    0.3676    0.4545        68
 Contingency     0.5644    0.5438    0.5539       274
  Comparison     0.5154    0.4621    0.4873       145
   Expansion     0.6882    0.7518    0.7186       552

    accuracy                         0.6314      1039
   macro avg     0.5908    0.5313    0.5536      1039
weighted avg     0.6254    0.6314    0.6256      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5455    0.4444    0.4898        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5445    0.5933    0.5679       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4884    0.4922    0.4903       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4367    0.5350    0.4809       200
    Expansion.Instantiation     0.7064    0.6471    0.6754       119
      Expansion.Restatement     0.4851    0.4645    0.4746       211
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5140      1039
                  macro avg     0.3256    0.3494    0.3326      1039
               weighted avg     0.4957    0.5140    0.5028      1039

Epoch [8/30]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   5.8,  Val Acc: 61.72%, Val F1: 51.84% Time: 135.10479545593262 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 50.13%, Val F1: 32.01% Time: 135.10479545593262 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   5.8,  Val Acc: 28.58%, Val F1:  7.50% Time: 135.10479545593262 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 62.32%, Val F1: 50.77% Time: 390.9865093231201 
top-down:SEC: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.0,  Val Acc: 49.79%, Val F1: 31.47% Time: 390.9865093231201 
top-down:CONN: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 53.12%,Val Loss:   6.0,  Val Acc: 28.93%, Val F1:  7.71% Time: 390.9865093231201 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 61.89%, Val F1: 50.41% Time: 631.1053161621094 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.0,  Val Acc: 49.70%, Val F1: 30.66% Time: 631.1053161621094 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   6.0,  Val Acc: 27.04%, Val F1:  7.37% Time: 631.1053161621094 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 61.89%, Val F1: 50.24% Time: 885.4222903251648 
top-down:SEC: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   6.0,  Val Acc: 48.93%, Val F1: 30.54% Time: 885.4222903251648 
top-down:CONN: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   6.0,  Val Acc: 28.33%, Val F1:  7.71% Time: 885.4222903251648 
 
 
Train time usage: 1000.8031969070435
Test time usage: 2.6983642578125
TOP: Test Loss:   5.6,  Test Acc: 63.72%, Test F1: 54.79%
SEC: Test Loss:   5.6,  Test Acc: 52.26%, Test F1: 32.56%
CONN: Test Loss:   5.6,  Test Acc: 25.51%, Test F1:  8.51%
consistency_top_sec: 49.57%,  consistency_sec_conn: 21.46%, consistency_top_sec_conn: 20.89%
              precision    recall  f1-score   support

    Temporal     0.5306    0.3768    0.4407        69
 Contingency     0.6068    0.4579    0.5219       273
  Comparison     0.5111    0.4759    0.4929       145
   Expansion     0.6810    0.8007    0.7361       552

    accuracy                         0.6372      1039
   macro avg     0.5824    0.5278    0.5479      1039
weighted avg     0.6278    0.6372    0.6262      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5476    0.4182    0.4742        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5817    0.5489    0.5648       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4692    0.4766    0.4729       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4346    0.6150    0.5093       200
    Expansion.Instantiation     0.6885    0.7059    0.6971       119
      Expansion.Restatement     0.5204    0.4811    0.5000       212
      Expansion.Alternative     0.3077    0.4444    0.3636         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5226      1039
                  macro avg     0.3227    0.3355    0.3256      1039
               weighted avg     0.5071    0.5226    0.5110      1039

Epoch [9/30]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.1,  Val Acc: 62.40%, Val F1: 50.49% Time: 133.46955561637878 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.1,  Val Acc: 49.70%, Val F1: 30.80% Time: 133.46955561637878 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.1,  Val Acc: 27.90%, Val F1:  7.45% Time: 133.46955561637878 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 61.20%, Val F1: 49.64% Time: 388.08349657058716 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 48.93%, Val F1: 30.33% Time: 388.08349657058716 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 28.07%, Val F1:  7.35% Time: 388.08349657058716 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 62.83%, Val F1: 51.45% Time: 661.083105802536 
top-down:SEC: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   6.1,  Val Acc: 49.79%, Val F1: 30.83% Time: 661.083105802536 
top-down:CONN: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 43.75%,Val Loss:   6.1,  Val Acc: 27.98%, Val F1:  7.55% Time: 661.083105802536 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 62.32%, Val F1: 50.65% Time: 918.5987105369568 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 49.44%, Val F1: 28.36% Time: 918.5987105369568 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 50.00%,Val Loss:   6.1,  Val Acc: 27.81%, Val F1:  7.82% Time: 918.5987105369568 
 
 
Train time usage: 1020.8513782024384
Test time usage: 3.0381155014038086
TOP: Test Loss:   5.9,  Test Acc: 63.33%, Test F1: 55.02%
SEC: Test Loss:   5.9,  Test Acc: 50.14%, Test F1: 32.22%
CONN: Test Loss:   5.9,  Test Acc: 26.56%, Test F1:  8.77%
consistency_top_sec: 48.51%,  consistency_sec_conn: 22.04%, consistency_top_sec_conn: 21.66%
              precision    recall  f1-score   support

    Temporal     0.5185    0.4058    0.4553        69
 Contingency     0.6009    0.4926    0.5414       272
  Comparison     0.4605    0.4828    0.4714       145
   Expansion     0.6984    0.7703    0.7326       553

    accuracy                         0.6333      1039
   macro avg     0.5696    0.5379    0.5502      1039
weighted avg     0.6277    0.6333    0.6277      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4630    0.4545    0.4587        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5855    0.5131    0.5469       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4324    0.5000    0.4638       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4357    0.5250    0.4762       200
    Expansion.Instantiation     0.6842    0.6610    0.6724       118
      Expansion.Restatement     0.4649    0.5000    0.4818       212
      Expansion.Alternative     0.3333    0.6667    0.4444         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5014      1039
                  macro avg     0.3090    0.3473    0.3222      1039
               weighted avg     0.4876    0.5014    0.4922      1039

Epoch [10/30]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 61.37%, Val F1: 51.30% Time: 161.2376971244812 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.3,  Val Acc: 48.24%, Val F1: 29.33% Time: 161.2376971244812 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 37.50%,Val Loss:   6.3,  Val Acc: 28.33%, Val F1:  7.53% Time: 161.2376971244812 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   6.3,  Val Acc: 61.46%, Val F1: 51.05% Time: 417.45481991767883 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   6.3,  Val Acc: 48.93%, Val F1: 29.81% Time: 417.45481991767883 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.3,  Val Acc: 28.15%, Val F1:  7.70% Time: 417.45481991767883 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 62.32%, Val F1: 52.27% Time: 661.9003155231476 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.3,  Val Acc: 50.30%, Val F1: 30.91% Time: 661.9003155231476 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   6.3,  Val Acc: 29.01%, Val F1:  7.72% Time: 661.9003155231476 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.4,  Val Acc: 61.37%, Val F1: 49.69% Time: 918.2913372516632 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 47.81%, Val F1: 29.56% Time: 918.2913372516632 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.4,  Val Acc: 27.38%, Val F1:  7.62% Time: 918.2913372516632 
 
 
Train time usage: 992.4923365116119
Test time usage: 2.0647308826446533
TOP: Test Loss:   6.1,  Test Acc: 62.46%, Test F1: 53.45%
SEC: Test Loss:   6.1,  Test Acc: 49.57%, Test F1: 31.59%
CONN: Test Loss:   6.1,  Test Acc: 25.79%, Test F1:  8.54%
consistency_top_sec: 48.03%,  consistency_sec_conn: 21.85%, consistency_top_sec_conn: 21.37%
              precision    recall  f1-score   support

    Temporal     0.5116    0.3188    0.3929        69
 Contingency     0.5950    0.5275    0.5592       273
  Comparison     0.4390    0.4966    0.4660       145
   Expansion     0.6966    0.7446    0.7198       552

    accuracy                         0.6246      1039
   macro avg     0.5606    0.5219    0.5345      1039
weighted avg     0.6217    0.6246    0.6205      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5789    0.4000    0.4731        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5777    0.5431    0.5598       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4207    0.5391    0.4726       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3939    0.5200    0.4483       200
    Expansion.Instantiation     0.6972    0.6441    0.6696       118
      Expansion.Restatement     0.4921    0.4434    0.4665       212
      Expansion.Alternative     0.2941    0.5556    0.3846         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4957      1039
                  macro avg     0.3141    0.3314    0.3159      1039
               weighted avg     0.4889    0.4957    0.4880      1039

Epoch [11/30]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.5,  Val Acc: 62.06%, Val F1: 51.31% Time: 176.72399473190308 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 81.25%,Val Loss:   6.5,  Val Acc: 48.41%, Val F1: 30.68% Time: 176.72399473190308 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 40.62%,Val Loss:   6.5,  Val Acc: 28.33%, Val F1:  7.83% Time: 176.72399473190308 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.6,  Val Acc: 60.34%, Val F1: 48.28% Time: 423.8409934043884 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   6.6,  Val Acc: 45.92%, Val F1: 29.38% Time: 423.8409934043884 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   6.6,  Val Acc: 28.07%, Val F1:  7.83% Time: 423.8409934043884 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.4,  Val Acc: 63.69%, Val F1: 52.29% Time: 685.716157913208 
top-down:SEC: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   6.4,  Val Acc: 49.53%, Val F1: 32.00% Time: 685.716157913208 
top-down:CONN: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   6.4,  Val Acc: 28.33%, Val F1:  8.18% Time: 685.716157913208 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.5,  Val Acc: 62.58%, Val F1: 51.34% Time: 931.3341331481934 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.5,  Val Acc: 48.76%, Val F1: 30.89% Time: 931.3341331481934 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   6.5,  Val Acc: 28.41%, Val F1:  7.40% Time: 931.3341331481934 
 
 
Train time usage: 988.0427150726318
Test time usage: 2.559645891189575
TOP: Test Loss:   6.3,  Test Acc: 63.04%, Test F1: 54.87%
SEC: Test Loss:   6.3,  Test Acc: 50.05%, Test F1: 31.47%
CONN: Test Loss:   6.3,  Test Acc: 26.85%, Test F1:  9.20%
consistency_top_sec: 48.51%,  consistency_sec_conn: 22.43%, consistency_top_sec_conn: 21.94%
              precision    recall  f1-score   support

    Temporal     0.5200    0.3768    0.4370        69
 Contingency     0.5949    0.5165    0.5529       273
  Comparison     0.4765    0.4897    0.4830       145
   Expansion     0.6915    0.7554    0.7221       552

    accuracy                         0.6304      1039
   macro avg     0.5707    0.5346    0.5487      1039
weighted avg     0.6248    0.6304    0.6253      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5217    0.4364    0.4752        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5833    0.5506    0.5665       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4345    0.4922    0.4615       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4223    0.5300    0.4701       200
    Expansion.Instantiation     0.7009    0.6356    0.6667       118
      Expansion.Restatement     0.4720    0.4764    0.4742       212
      Expansion.Alternative     0.2857    0.4444    0.3478         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5005      1039
                  macro avg     0.3110    0.3241    0.3147      1039
               weighted avg     0.4907    0.5005    0.4936      1039

Epoch [12/30]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 62.40%, Val F1: 53.32% Time: 194.4545238018036 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 50.39%, Val F1: 31.60% Time: 194.4545238018036 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   6.7,  Val Acc: 28.07%, Val F1:  7.96% Time: 194.4545238018036 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.8,  Val Acc: 60.09%, Val F1: 50.54% Time: 453.88642406463623 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 48.84%, Val F1: 31.66% Time: 453.88642406463623 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   6.8,  Val Acc: 28.07%, Val F1:  7.56% Time: 453.88642406463623 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 62.23%, Val F1: 51.42% Time: 706.4062550067902 
top-down:SEC: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 48.58%, Val F1: 30.50% Time: 706.4062550067902 
top-down:CONN: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   6.8,  Val Acc: 27.21%, Val F1:  7.84% Time: 706.4062550067902 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.6,  Val Acc: 61.80%, Val F1: 51.58% Time: 946.5831835269928 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.6,  Val Acc: 50.39%, Val F1: 32.55% Time: 946.5831835269928 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 28.24%, Val F1:  7.91% Time: 946.5831835269928 
 
 
Train time usage: 992.4033205509186
Test time usage: 3.6088709831237793
TOP: Test Loss:   6.4,  Test Acc: 63.33%, Test F1: 56.36%
SEC: Test Loss:   6.4,  Test Acc: 50.43%, Test F1: 32.51%
CONN: Test Loss:   6.4,  Test Acc: 25.89%, Test F1:  9.18%
consistency_top_sec: 49.86%,  consistency_sec_conn: 21.27%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.5254    0.4493    0.4844        69
 Contingency     0.5744    0.5092    0.5398       273
  Comparison     0.5000    0.5103    0.5051       145
   Expansion     0.7017    0.7500    0.7250       552

    accuracy                         0.6333      1039
   macro avg     0.5754    0.5547    0.5636      1039
weighted avg     0.6284    0.6333    0.6297      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4815    0.4727    0.4771        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5578    0.5243    0.5405       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4476    0.5000    0.4723       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4973    0.4600    0.4779       200
    Expansion.Instantiation     0.6610    0.6610    0.6610       118
      Expansion.Restatement     0.4491    0.5613    0.4990       212
      Expansion.Alternative     0.2857    0.4444    0.3478         9
             Expansion.List     0.1250    0.0833    0.1000        12

                   accuracy                         0.5043      1039
                  macro avg     0.3186    0.3370    0.3251      1039
               weighted avg     0.4903    0.5043    0.4954      1039

Epoch [13/30]
top-down:TOP: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   6.8,  Val Acc: 62.15%, Val F1: 51.46% Time: 209.31159353256226 
top-down:SEC: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 49.44%, Val F1: 29.50% Time: 209.31159353256226 
top-down:CONN: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   6.8,  Val Acc: 27.55%, Val F1:  8.04% Time: 209.31159353256226 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   6.9,  Val Acc: 62.40%, Val F1: 51.42% Time: 467.37631464004517 
top-down:SEC: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   6.9,  Val Acc: 49.27%, Val F1: 31.00% Time: 467.37631464004517 
top-down:CONN: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 53.12%,Val Loss:   6.9,  Val Acc: 28.50%, Val F1:  8.39% Time: 467.37631464004517 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.9,  Val Acc: 62.06%, Val F1: 51.91% Time: 711.381511926651 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   6.9,  Val Acc: 49.27%, Val F1: 32.58% Time: 711.381511926651 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.9,  Val Acc: 27.90%, Val F1:  7.89% Time: 711.381511926651 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 62.49%, Val F1: 51.80% Time: 960.7175772190094 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 50.39%, Val F1: 30.68% Time: 960.7175772190094 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   6.8,  Val Acc: 29.01%, Val F1:  8.30% Time: 960.7175772190094 
 
 
Train time usage: 989.3767955303192
Test time usage: 2.444408655166626
TOP: Test Loss:   6.6,  Test Acc: 61.12%, Test F1: 53.88%
SEC: Test Loss:   6.6,  Test Acc: 50.82%, Test F1: 33.36%
CONN: Test Loss:   6.6,  Test Acc: 27.24%, Test F1:  9.35%
consistency_top_sec: 48.70%,  consistency_sec_conn: 22.43%, consistency_top_sec_conn: 21.94%
              precision    recall  f1-score   support

    Temporal     0.4762    0.4348    0.4545        69
 Contingency     0.5542    0.4872    0.5185       273
  Comparison     0.4698    0.4828    0.4762       145
   Expansion     0.6848    0.7283    0.7059       552

    accuracy                         0.6112      1039
   macro avg     0.5462    0.5332    0.5388      1039
weighted avg     0.6066    0.6112    0.6079      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5625    0.4909    0.5243        55
         Temporal.Synchrony     0.2500    0.0714    0.1111        14
          Contingency.Cause     0.5652    0.5356    0.5500       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4444    0.5000    0.4706       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4286    0.5400    0.4779       200
    Expansion.Instantiation     0.7117    0.6695    0.6900       118
      Expansion.Restatement     0.5025    0.4764    0.4891       212
      Expansion.Alternative     0.2632    0.5556    0.3571         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5082      1039
                  macro avg     0.3389    0.3490    0.3336      1039
               weighted avg     0.5013    0.5082    0.5018      1039

Epoch [14/30]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.9,  Val Acc: 62.83%, Val F1: 52.35% Time: 228.60859632492065 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.9,  Val Acc: 49.87%, Val F1: 32.93% Time: 228.60859632492065 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   6.9,  Val Acc: 28.33%, Val F1:  8.37% Time: 228.60859632492065 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.0,  Val Acc: 61.72%, Val F1: 51.22% Time: 484.5428144931793 
top-down:SEC: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 48.76%, Val F1: 31.59% Time: 484.5428144931793 
top-down:CONN: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   7.0,  Val Acc: 27.55%, Val F1:  8.27% Time: 484.5428144931793 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 100.00%,Val Loss:   7.1,  Val Acc: 61.20%, Val F1: 50.88% Time: 735.6115348339081 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 48.84%, Val F1: 31.10% Time: 735.6115348339081 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 28.33%, Val F1:  8.49% Time: 735.6115348339081 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   7.1,  Val Acc: 60.43%, Val F1: 48.84% Time: 990.9312884807587 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 48.24%, Val F1: 31.49% Time: 990.9312884807587 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 53.12%,Val Loss:   7.1,  Val Acc: 27.64%, Val F1:  8.36% Time: 990.9312884807587 
 
 
Train time usage: 1001.0273277759552
Test time usage: 2.60516357421875
TOP: Test Loss:   6.9,  Test Acc: 62.37%, Test F1: 53.21%
SEC: Test Loss:   6.9,  Test Acc: 49.76%, Test F1: 33.16%
CONN: Test Loss:   6.9,  Test Acc: 27.53%, Test F1: 10.00%
consistency_top_sec: 48.12%,  consistency_sec_conn: 22.43%, consistency_top_sec_conn: 21.94%
              precision    recall  f1-score   support

    Temporal     0.4746    0.4058    0.4375        69
 Contingency     0.6082    0.3824    0.4695       272
  Comparison     0.5111    0.4759    0.4929       145
   Expansion     0.6632    0.8083    0.7286       553

    accuracy                         0.6237      1039
   macro avg     0.5643    0.5181    0.5321      1039
weighted avg     0.6150    0.6237    0.6085      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4355    0.4909    0.4615        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6053    0.4291    0.5022       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4697    0.4844    0.4769       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4211    0.5600    0.4807       200
    Expansion.Instantiation     0.7255    0.6325    0.6758       117
      Expansion.Restatement     0.4651    0.5660    0.5106       212
      Expansion.Alternative     0.3158    0.6667    0.4286         9
             Expansion.List     0.1667    0.0833    0.1111        12

                   accuracy                         0.4976      1039
                  macro avg     0.3277    0.3557    0.3316      1039
               weighted avg     0.4993    0.4976    0.4905      1039

Epoch [15/30]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 61.89%, Val F1: 52.82% Time: 243.0487186908722 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.1,  Val Acc: 49.44%, Val F1: 30.78% Time: 243.0487186908722 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 28.24%, Val F1:  8.35% Time: 243.0487186908722 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 61.20%, Val F1: 51.22% Time: 494.2400255203247 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 48.41%, Val F1: 31.49% Time: 494.2400255203247 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   7.2,  Val Acc: 27.04%, Val F1:  8.35% Time: 494.2400255203247 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 61.63%, Val F1: 50.42% Time: 736.6736376285553 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 48.76%, Val F1: 30.89% Time: 736.6736376285553 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 59.38%,Val Loss:   7.2,  Val Acc: 27.38%, Val F1:  8.15% Time: 736.6736376285553 
 
 
Train time usage: 996.8824861049652
Test time usage: 2.7036960124969482
TOP: Test Loss:   6.9,  Test Acc: 61.50%, Test F1: 53.41%
SEC: Test Loss:   6.9,  Test Acc: 50.34%, Test F1: 31.64%
CONN: Test Loss:   6.9,  Test Acc: 26.28%, Test F1:  8.69%
consistency_top_sec: 47.93%,  consistency_sec_conn: 21.27%, consistency_top_sec_conn: 20.40%
              precision    recall  f1-score   support

    Temporal     0.4355    0.3913    0.4122        69
 Contingency     0.5659    0.5328    0.5489       274
  Comparison     0.4962    0.4483    0.4710       145
   Expansion     0.6820    0.7278    0.7041       551

    accuracy                         0.6150      1039
   macro avg     0.5449    0.5250    0.5341      1039
weighted avg     0.6091    0.6150    0.6113      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5200    0.4727    0.4952        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5613    0.5634    0.5624       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4762    0.4688    0.4724       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4187    0.5150    0.4619       200
    Expansion.Instantiation     0.7212    0.6410    0.6787       117
      Expansion.Restatement     0.4882    0.4858    0.4870       212
      Expansion.Alternative     0.2273    0.5556    0.3226         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5034      1039
                  macro avg     0.3103    0.3366    0.3164      1039
               weighted avg     0.4944    0.5034    0.4970      1039

Epoch [16/30]
top-down:TOP: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 61.37%, Val F1: 50.56% Time: 14.614229440689087 
top-down:SEC: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.3,  Val Acc: 49.27%, Val F1: 30.66% Time: 14.614229440689087 
top-down:CONN: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   7.3,  Val Acc: 27.47%, Val F1:  8.09% Time: 14.614229440689087 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 61.80%, Val F1: 50.93% Time: 272.0282690525055 
top-down:SEC: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 48.58%, Val F1: 29.70% Time: 272.0282690525055 
top-down:CONN: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   7.3,  Val Acc: 27.47%, Val F1:  8.11% Time: 272.0282690525055 
 
 
top-down:TOP: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.2,  Val Acc: 62.06%, Val F1: 51.56% Time: 539.8435935974121 
top-down:SEC: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 48.76%, Val F1: 31.42% Time: 539.8435935974121 
top-down:CONN: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.2,  Val Acc: 28.41%, Val F1:  8.55% Time: 539.8435935974121 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 61.37%, Val F1: 50.51% Time: 784.1746079921722 
top-down:SEC: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   7.2,  Val Acc: 49.10%, Val F1: 32.55% Time: 784.1746079921722 
top-down:CONN: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   7.2,  Val Acc: 27.04%, Val F1:  8.20% Time: 784.1746079921722 
 
 
Train time usage: 1012.6650333404541
Test time usage: 1.8961691856384277
TOP: Test Loss:   7.0,  Test Acc: 61.98%, Test F1: 54.05%
SEC: Test Loss:   7.0,  Test Acc: 49.66%, Test F1: 30.85%
CONN: Test Loss:   7.0,  Test Acc: 27.72%, Test F1:  9.66%
consistency_top_sec: 47.93%,  consistency_sec_conn: 22.62%, consistency_top_sec_conn: 22.33%
              precision    recall  f1-score   support

    Temporal     0.4308    0.4058    0.4179        69
 Contingency     0.5875    0.5146    0.5486       274
  Comparison     0.4797    0.4897    0.4846       145
   Expansion     0.6894    0.7332    0.7106       551

    accuracy                         0.6198      1039
   macro avg     0.5469    0.5358    0.5405      1039
weighted avg     0.6161    0.6198    0.6169      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4528    0.4364    0.4444        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5709    0.5410    0.5556       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4397    0.4844    0.4610       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4400    0.4950    0.4659       200
    Expansion.Instantiation     0.7283    0.5726    0.6411       117
      Expansion.Restatement     0.4733    0.5425    0.5055       212
      Expansion.Alternative     0.2500    0.4444    0.3200         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3050    0.3197    0.3085      1039
               weighted avg     0.4908    0.4966    0.4914      1039

Epoch [17/30]
top-down:TOP: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 62.40%, Val F1: 51.77% Time: 30.75309658050537 
top-down:SEC: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 49.79%, Val F1: 32.78% Time: 30.75309658050537 
top-down:CONN: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   7.3,  Val Acc: 28.58%, Val F1:  8.31% Time: 30.75309658050537 
 
 
top-down:TOP: Iter:   6400,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 60.94%, Val F1: 50.20% Time: 264.26819586753845 
top-down:SEC: Iter:   6400,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 48.24%, Val F1: 30.94% Time: 264.26819586753845 
top-down:CONN: Iter:   6400,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.3,  Val Acc: 26.61%, Val F1:  8.45% Time: 264.26819586753845 
 
 
top-down:TOP: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 60.94%, Val F1: 50.62% Time: 536.6702332496643 
top-down:SEC: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 47.98%, Val F1: 32.26% Time: 536.6702332496643 
top-down:CONN: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   7.3,  Val Acc: 27.12%, Val F1:  7.89% Time: 536.6702332496643 
 
 
top-down:TOP: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 62.15%, Val F1: 52.31% Time: 814.2004187107086 
top-down:SEC: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 50.39%, Val F1: 33.18% Time: 814.2004187107086 
top-down:CONN: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   7.3,  Val Acc: 27.98%, Val F1:  8.35% Time: 814.2004187107086 
 
 
Train time usage: 1032.5750682353973
Test time usage: 3.8852927684783936
TOP: Test Loss:   7.0,  Test Acc: 62.37%, Test F1: 54.66%
SEC: Test Loss:   7.0,  Test Acc: 51.01%, Test F1: 33.01%
CONN: Test Loss:   7.0,  Test Acc: 26.18%, Test F1:  9.53%
consistency_top_sec: 48.80%,  consistency_sec_conn: 21.75%, consistency_top_sec_conn: 21.08%
              precision    recall  f1-score   support

    Temporal     0.5208    0.3623    0.4274        69
 Contingency     0.5596    0.6168    0.5868       274
  Comparison     0.4626    0.4690    0.4658       145
   Expansion     0.7122    0.7005    0.7063       551

    accuracy                         0.6237      1039
   macro avg     0.5638    0.5372    0.5466      1039
weighted avg     0.6244    0.6237    0.6227      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5814    0.4545    0.5102        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5493    0.6231    0.5839       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4545    0.4688    0.4615       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4498    0.5150    0.4802       200
    Expansion.Instantiation     0.6852    0.6325    0.6578       117
      Expansion.Restatement     0.5027    0.4434    0.4712       212
      Expansion.Alternative     0.3333    0.7778    0.4667         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5101      1039
                  macro avg     0.3233    0.3559    0.3301      1039
               weighted avg     0.4977    0.5101    0.5012      1039

Epoch [18/30]
top-down:TOP: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.2,  Val Acc: 61.63%, Val F1: 50.81% Time: 46.88186764717102 
top-down:SEC: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 49.61%, Val F1: 32.29% Time: 46.88186764717102 
top-down:CONN: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 62.50%,Val Loss:   7.2,  Val Acc: 26.70%, Val F1:  8.00% Time: 46.88186764717102 
 
 
top-down:TOP: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 62.32%, Val F1: 52.41% Time: 292.32916355133057 
top-down:SEC: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 49.61%, Val F1: 32.82% Time: 292.32916355133057 
top-down:CONN: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   7.3,  Val Acc: 26.95%, Val F1:  8.44% Time: 292.32916355133057 
 
 
top-down:TOP: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 61.80%, Val F1: 51.17% Time: 548.2775464057922 
top-down:SEC: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 48.84%, Val F1: 31.08% Time: 548.2775464057922 
top-down:CONN: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 71.88%,Val Loss:   7.3,  Val Acc: 27.30%, Val F1:  8.77% Time: 548.2775464057922 
 
 
top-down:TOP: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 62.75%, Val F1: 51.75% Time: 785.6057441234589 
top-down:SEC: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.3,  Val Acc: 49.18%, Val F1: 31.81% Time: 785.6057441234589 
top-down:CONN: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   7.3,  Val Acc: 27.21%, Val F1:  8.34% Time: 785.6057441234589 
 
 
Train time usage: 950.6300711631775
Test time usage: 2.1056435108184814
TOP: Test Loss:   7.2,  Test Acc: 62.95%, Test F1: 54.58%
SEC: Test Loss:   7.2,  Test Acc: 49.37%, Test F1: 32.11%
CONN: Test Loss:   7.2,  Test Acc: 24.83%, Test F1:  9.30%
consistency_top_sec: 48.12%,  consistency_sec_conn: 19.73%, consistency_top_sec_conn: 19.54%
              precision    recall  f1-score   support

    Temporal     0.4561    0.3768    0.4127        69
 Contingency     0.6009    0.4891    0.5392       274
  Comparison     0.5068    0.5139    0.5103       144
   Expansion     0.6852    0.7609    0.7210       552

    accuracy                         0.6295      1039
   macro avg     0.5623    0.5352    0.5458      1039
weighted avg     0.6230    0.6295    0.6234      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4364    0.4660        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5776    0.5000    0.5360       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4643    0.5078    0.4851       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4262    0.5050    0.4622       200
    Expansion.Instantiation     0.6667    0.6496    0.6580       117
      Expansion.Restatement     0.4573    0.5047    0.4798       212
      Expansion.Alternative     0.3333    0.6667    0.4444         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4937      1039
                  macro avg     0.3114    0.3427    0.3211      1039
               weighted avg     0.4859    0.4937    0.4875      1039

Epoch [19/30]
top-down:TOP: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.4,  Val Acc: 62.32%, Val F1: 51.62% Time: 48.20349884033203 
top-down:SEC: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 49.70%, Val F1: 33.40% Time: 48.20349884033203 
top-down:CONN: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   7.4,  Val Acc: 27.81%, Val F1:  8.31% Time: 48.20349884033203 
 
 
top-down:TOP: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 63.18%, Val F1: 53.27% Time: 288.91252994537354 
top-down:SEC: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 84.38%,Val Loss:   7.4,  Val Acc: 49.87%, Val F1: 32.56% Time: 288.91252994537354 
top-down:CONN: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 62.50%,Val Loss:   7.4,  Val Acc: 28.07%, Val F1:  8.78% Time: 288.91252994537354 
 
 
top-down:TOP: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.5,  Val Acc: 61.63%, Val F1: 51.26% Time: 527.2487058639526 
top-down:SEC: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 49.53%, Val F1: 31.57% Time: 527.2487058639526 
top-down:CONN: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   7.5,  Val Acc: 28.07%, Val F1:  8.49% Time: 527.2487058639526 
 
 
top-down:TOP: Iter:   7400,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.4,  Val Acc: 61.97%, Val F1: 51.73% Time: 742.3160820007324 
top-down:SEC: Iter:   7400,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 49.79%, Val F1: 32.87% Time: 742.3160820007324 
top-down:CONN: Iter:   7400,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   7.4,  Val Acc: 27.55%, Val F1:  8.53% Time: 742.3160820007324 
 
 
Train time usage: 886.8647077083588
Test time usage: 2.1192543506622314
TOP: Test Loss:   7.3,  Test Acc: 63.43%, Test F1: 56.20%
SEC: Test Loss:   7.3,  Test Acc: 50.53%, Test F1: 32.68%
CONN: Test Loss:   7.3,  Test Acc: 27.24%, Test F1: 10.40%
consistency_top_sec: 48.70%,  consistency_sec_conn: 21.94%, consistency_top_sec_conn: 21.66%
              precision    recall  f1-score   support

    Temporal     0.5854    0.3529    0.4404        68
 Contingency     0.5578    0.5985    0.5775       274
  Comparison     0.5172    0.5172    0.5172       145
   Expansion     0.7084    0.7174    0.7129       552

    accuracy                         0.6343      1039
   macro avg     0.5922    0.5465    0.5620      1039
weighted avg     0.6340    0.6343    0.6320      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5750    0.4259    0.4894        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5524    0.5896    0.5704       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4885    0.5000    0.4942       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4310    0.5150    0.4692       200
    Expansion.Instantiation     0.6435    0.6271    0.6352       118
      Expansion.Restatement     0.4924    0.4575    0.4743       212
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5053      1039
                  macro avg     0.3214    0.3438    0.3268      1039
               weighted avg     0.4921    0.5053    0.4967      1039

Epoch [20/30]
top-down:TOP: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 62.66%, Val F1: 52.19% Time: 71.86333775520325 
top-down:SEC: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.5,  Val Acc: 48.84%, Val F1: 31.27% Time: 71.86333775520325 
top-down:CONN: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 27.64%, Val F1:  8.36% Time: 71.86333775520325 
 
 
top-down:TOP: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   7.4,  Val Acc: 62.66%, Val F1: 50.79% Time: 289.63489508628845 
top-down:SEC: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.4,  Val Acc: 49.61%, Val F1: 32.29% Time: 289.63489508628845 
top-down:CONN: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.4,  Val Acc: 27.21%, Val F1:  8.57% Time: 289.63489508628845 
 
 
top-down:TOP: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 62.40%, Val F1: 51.43% Time: 496.1036217212677 
top-down:SEC: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   7.5,  Val Acc: 48.93%, Val F1: 32.16% Time: 496.1036217212677 
top-down:CONN: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   7.5,  Val Acc: 27.64%, Val F1:  8.77% Time: 496.1036217212677 
 
 
top-down:TOP: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 61.97%, Val F1: 51.54% Time: 676.4627976417542 
top-down:SEC: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   7.5,  Val Acc: 50.04%, Val F1: 32.04% Time: 676.4627976417542 
top-down:CONN: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 62.50%,Val Loss:   7.5,  Val Acc: 26.27%, Val F1:  8.59% Time: 676.4627976417542 
 
 
Train time usage: 790.6195070743561
Test time usage: 2.2739484310150146
TOP: Test Loss:   7.4,  Test Acc: 62.95%, Test F1: 55.05%
SEC: Test Loss:   7.4,  Test Acc: 49.47%, Test F1: 31.18%
CONN: Test Loss:   7.4,  Test Acc: 25.99%, Test F1:  9.72%
consistency_top_sec: 47.93%,  consistency_sec_conn: 20.60%, consistency_top_sec_conn: 20.12%
              precision    recall  f1-score   support

    Temporal     0.5098    0.3768    0.4333        69
 Contingency     0.5738    0.4964    0.5323       274
  Comparison     0.5033    0.5241    0.5135       145
   Expansion     0.6933    0.7550    0.7228       551

    accuracy                         0.6295      1039
   macro avg     0.5701    0.5381    0.5505      1039
weighted avg     0.6231    0.6295    0.6242      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5319    0.4545    0.4902        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5774    0.5149    0.5444       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4552    0.5156    0.4835       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4191    0.5050    0.4580       200
    Expansion.Instantiation     0.7292    0.5983    0.6573       117
      Expansion.Restatement     0.4739    0.5142    0.4932       212
      Expansion.Alternative     0.2083    0.5556    0.3030         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4947      1039
                  macro avg     0.3086    0.3326    0.3118      1039
               weighted avg     0.4945    0.4947    0.4914      1039

Epoch [21/30]
top-down:TOP: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 61.72%, Val F1: 51.32% Time: 82.07288002967834 
top-down:SEC: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 48.07%, Val F1: 32.44% Time: 82.07288002967834 
top-down:CONN: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.6,  Val Acc: 26.87%, Val F1:  8.95% Time: 82.07288002967834 
 
 
top-down:TOP: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 62.23%, Val F1: 52.16% Time: 277.13289880752563 
top-down:SEC: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   7.6,  Val Acc: 49.01%, Val F1: 31.69% Time: 277.13289880752563 
top-down:CONN: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   7.6,  Val Acc: 27.47%, Val F1:  8.44% Time: 277.13289880752563 
 
 
top-down:TOP: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 61.89%, Val F1: 52.66% Time: 455.55019998550415 
top-down:SEC: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 50.30%, Val F1: 33.19% Time: 455.55019998550415 
top-down:CONN: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   7.6,  Val Acc: 27.98%, Val F1:  8.78% Time: 455.55019998550415 
 
 
top-down:TOP: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   7.5,  Val Acc: 62.06%, Val F1: 52.20% Time: 630.5539901256561 
top-down:SEC: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 49.61%, Val F1: 32.28% Time: 630.5539901256561 
top-down:CONN: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   7.5,  Val Acc: 26.52%, Val F1:  8.58% Time: 630.5539901256561 
 
 
Train time usage: 726.5573348999023
Test time usage: 1.9166970252990723
TOP: Test Loss:   7.4,  Test Acc: 63.23%, Test F1: 55.12%
SEC: Test Loss:   7.4,  Test Acc: 50.05%, Test F1: 31.08%
CONN: Test Loss:   7.4,  Test Acc: 27.33%, Test F1: 10.73%
consistency_top_sec: 48.41%,  consistency_sec_conn: 22.14%, consistency_top_sec_conn: 21.94%
              precision    recall  f1-score   support

    Temporal     0.5952    0.3623    0.4505        69
 Contingency     0.5726    0.5182    0.5441       274
  Comparison     0.5037    0.4690    0.4857       145
   Expansion     0.6873    0.7659    0.7245       551

    accuracy                         0.6323      1039
   macro avg     0.5897    0.5289    0.5512      1039
weighted avg     0.6253    0.6323    0.6254      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5750    0.4182    0.4842        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5543    0.5356    0.5448       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4634    0.4453    0.4542       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4372    0.5400    0.4832       200
    Expansion.Instantiation     0.7143    0.6356    0.6726       118
      Expansion.Restatement     0.4721    0.5189    0.4944       212
      Expansion.Alternative     0.2105    0.4444    0.2857         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5005      1039
                  macro avg     0.3115    0.3216    0.3108      1039
               weighted avg     0.4934    0.5005    0.4943      1039

Epoch [22/30]
top-down:TOP: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.20%, Val F1: 51.86% Time: 86.60305213928223 
top-down:SEC: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 48.58%, Val F1: 31.88% Time: 86.60305213928223 
top-down:CONN: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 25.84%, Val F1:  7.88% Time: 86.60305213928223 
 
 
top-down:TOP: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 62.32%, Val F1: 52.88% Time: 267.8258044719696 
top-down:SEC: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 49.18%, Val F1: 32.24% Time: 267.8258044719696 
top-down:CONN: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   7.6,  Val Acc: 26.27%, Val F1:  8.49% Time: 267.8258044719696 
 
 
top-down:TOP: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 61.46%, Val F1: 50.46% Time: 431.6350214481354 
top-down:SEC: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 48.24%, Val F1: 32.13% Time: 431.6350214481354 
top-down:CONN: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 26.78%, Val F1:  8.98% Time: 431.6350214481354 
 
 
top-down:TOP: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 62.06%, Val F1: 52.08% Time: 604.0192704200745 
top-down:SEC: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   7.7,  Val Acc: 49.61%, Val F1: 32.27% Time: 604.0192704200745 
top-down:CONN: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   7.7,  Val Acc: 26.01%, Val F1:  8.55% Time: 604.0192704200745 
 
 
Train time usage: 683.5800523757935
Test time usage: 1.9218385219573975
TOP: Test Loss:   7.4,  Test Acc: 62.85%, Test F1: 54.16%
SEC: Test Loss:   7.4,  Test Acc: 49.37%, Test F1: 32.68%
CONN: Test Loss:   7.4,  Test Acc: 25.70%, Test F1: 10.05%
consistency_top_sec: 47.93%,  consistency_sec_conn: 20.02%, consistency_top_sec_conn: 19.63%
              precision    recall  f1-score   support

    Temporal     0.4906    0.3768    0.4262        69
 Contingency     0.5738    0.4964    0.5323       274
  Comparison     0.5242    0.4483    0.4833       145
   Expansion     0.6816    0.7731    0.7245       551

    accuracy                         0.6285      1039
   macro avg     0.5675    0.5236    0.5416      1039
weighted avg     0.6185    0.6285    0.6203      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5532    0.4727    0.5098        55
         Temporal.Synchrony     0.1250    0.0714    0.0909        14
          Contingency.Cause     0.5551    0.5075    0.5302       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5000    0.4375    0.4667       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4274    0.5150    0.4671       200
    Expansion.Instantiation     0.6887    0.6239    0.6547       117
      Expansion.Restatement     0.4556    0.5330    0.4913       212
      Expansion.Alternative     0.2941    0.5556    0.3846         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4937      1039
                  macro avg     0.3272    0.3379    0.3268      1039
               weighted avg     0.4911    0.4937    0.4897      1039

Epoch [23/30]
top-down:TOP: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 61.72%, Val F1: 51.31% Time: 77.33458542823792 
top-down:SEC: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 49.36%, Val F1: 32.66% Time: 77.33458542823792 
top-down:CONN: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   7.7,  Val Acc: 25.58%, Val F1:  8.20% Time: 77.33458542823792 
 
 
top-down:TOP: Iter:   8800,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 62.06%, Val F1: 51.45% Time: 237.07548093795776 
top-down:SEC: Iter:   8800,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 49.01%, Val F1: 30.66% Time: 237.07548093795776 
top-down:CONN: Iter:   8800,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   7.7,  Val Acc: 26.95%, Val F1:  8.54% Time: 237.07548093795776 
 
 
top-down:TOP: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 60.94%, Val F1: 50.18% Time: 401.33230924606323 
top-down:SEC: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.7,  Val Acc: 48.41%, Val F1: 30.93% Time: 401.33230924606323 
top-down:CONN: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 26.61%, Val F1:  8.37% Time: 401.33230924606323 
 
 
top-down:TOP: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.89%, Val F1: 51.27% Time: 561.5280575752258 
top-down:SEC: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 47.90%, Val F1: 29.92% Time: 561.5280575752258 
top-down:CONN: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   7.7,  Val Acc: 26.95%, Val F1:  8.23% Time: 561.5280575752258 
 
 
Train time usage: 622.1323835849762
Test time usage: 1.8566036224365234
TOP: Test Loss:   7.5,  Test Acc: 62.56%, Test F1: 53.54%
SEC: Test Loss:   7.5,  Test Acc: 48.32%, Test F1: 30.54%
CONN: Test Loss:   7.5,  Test Acc: 27.14%, Test F1: 10.34%
consistency_top_sec: 47.06%,  consistency_sec_conn: 21.37%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.4902    0.3623    0.4167        69
 Contingency     0.5783    0.4854    0.5278       274
  Comparison     0.5120    0.4414    0.4741       145
   Expansion     0.6761    0.7768    0.7230       551

    accuracy                         0.6256      1039
   macro avg     0.5642    0.5165    0.5354      1039
weighted avg     0.6151    0.6256    0.6164      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5306    0.4727    0.5000        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5455    0.4925    0.5176       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4667    0.4375    0.4516       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4250    0.5100    0.4636       200
    Expansion.Instantiation     0.6822    0.6239    0.6518       117
      Expansion.Restatement     0.4467    0.5142    0.4781       212
      Expansion.Alternative     0.2222    0.4444    0.2963         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4832      1039
                  macro avg     0.3017    0.3178    0.3054      1039
               weighted avg     0.4780    0.4832    0.4784      1039

Epoch [24/30]
top-down:TOP: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.72%, Val F1: 50.63% Time: 92.72469925880432 
top-down:SEC: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 47.47%, Val F1: 29.86% Time: 92.72469925880432 
top-down:CONN: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 62.50%,Val Loss:   7.7,  Val Acc: 26.27%, Val F1:  8.50% Time: 92.72469925880432 
 
 
top-down:TOP: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 61.12%, Val F1: 50.53% Time: 260.81021523475647 
top-down:SEC: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 47.21%, Val F1: 30.17% Time: 260.81021523475647 
top-down:CONN: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 71.88%,Val Loss:   7.8,  Val Acc: 26.44%, Val F1:  8.75% Time: 260.81021523475647 
 
 
top-down:TOP: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 60.94%, Val F1: 49.73% Time: 421.1413221359253 
top-down:SEC: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 46.78%, Val F1: 29.33% Time: 421.1413221359253 
top-down:CONN: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 78.12%,Val Loss:   7.8,  Val Acc: 26.78%, Val F1:  8.52% Time: 421.1413221359253 
 
 
top-down:TOP: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 60.94%, Val F1: 50.17% Time: 587.276930809021 
top-down:SEC: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 48.84%, Val F1: 31.59% Time: 587.276930809021 
top-down:CONN: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 53.12%,Val Loss:   7.8,  Val Acc: 26.87%, Val F1:  8.26% Time: 587.276930809021 
 
 
Train time usage: 639.6021885871887
Test time usage: 1.9595060348510742
TOP: Test Loss:   7.6,  Test Acc: 63.14%, Test F1: 55.40%
SEC: Test Loss:   7.6,  Test Acc: 49.28%, Test F1: 31.68%
CONN: Test Loss:   7.6,  Test Acc: 27.14%, Test F1: 11.10%
consistency_top_sec: 48.12%,  consistency_sec_conn: 21.46%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.4655    0.3913    0.4252        69
 Contingency     0.5813    0.5219    0.5500       274
  Comparison     0.5099    0.5310    0.5203       145
   Expansion     0.7003    0.7423    0.7207       551

    accuracy                         0.6314      1039
   macro avg     0.5643    0.5466    0.5540      1039
weighted avg     0.6268    0.6314    0.6281      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4545    0.4762        55
         Temporal.Synchrony     0.1667    0.0714    0.1000        14
          Contingency.Cause     0.5650    0.5187    0.5409       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4718    0.5234    0.4963       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4244    0.5050    0.4612       200
    Expansion.Instantiation     0.7087    0.6239    0.6636       117
      Expansion.Restatement     0.4615    0.4811    0.4711       212
      Expansion.Alternative     0.2000    0.4444    0.2759         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4928      1039
                  macro avg     0.3180    0.3293    0.3168      1039
               weighted avg     0.4900    0.4928    0.4892      1039

Epoch [25/30]
top-down:TOP: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 61.80%, Val F1: 51.45% Time: 109.47372889518738 
top-down:SEC: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 48.93%, Val F1: 31.19% Time: 109.47372889518738 
top-down:CONN: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   7.9,  Val Acc: 26.52%, Val F1:  8.34% Time: 109.47372889518738 
 
 
top-down:TOP: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 61.20%, Val F1: 51.20% Time: 277.0277988910675 
top-down:SEC: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 48.84%, Val F1: 31.88% Time: 277.0277988910675 
top-down:CONN: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 27.21%, Val F1:  8.85% Time: 277.0277988910675 
 
 
top-down:TOP: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 61.63%, Val F1: 50.85% Time: 427.5496938228607 
top-down:SEC: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.9,  Val Acc: 47.73%, Val F1: 30.48% Time: 427.5496938228607 
top-down:CONN: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   7.9,  Val Acc: 26.87%, Val F1:  8.57% Time: 427.5496938228607 
 
 
top-down:TOP: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.63%, Val F1: 51.04% Time: 586.704038143158 
top-down:SEC: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 48.41%, Val F1: 30.53% Time: 586.704038143158 
top-down:CONN: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 78.12%,Val Loss:   7.9,  Val Acc: 26.44%, Val F1:  8.47% Time: 586.704038143158 
 
 
Train time usage: 631.0043127536774
Test time usage: 1.8191726207733154
TOP: Test Loss:   7.6,  Test Acc: 63.23%, Test F1: 55.01%
SEC: Test Loss:   7.6,  Test Acc: 49.66%, Test F1: 32.22%
CONN: Test Loss:   7.6,  Test Acc: 26.56%, Test F1: 10.67%
consistency_top_sec: 48.41%,  consistency_sec_conn: 21.08%, consistency_top_sec_conn: 20.69%
              precision    recall  f1-score   support

    Temporal     0.4576    0.3913    0.4219        69
 Contingency     0.5957    0.5109    0.5501       274
  Comparison     0.5143    0.4966    0.5053       145
   Expansion     0.6909    0.7586    0.7232       551

    accuracy                         0.6323      1039
   macro avg     0.5646    0.5394    0.5501      1039
weighted avg     0.6257    0.6323    0.6271      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5294    0.4909    0.5094        55
         Temporal.Synchrony     0.1667    0.0714    0.1000        14
          Contingency.Cause     0.5656    0.5149    0.5391       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4846    0.4922    0.4884       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4228    0.5200    0.4664       200
    Expansion.Instantiation     0.6981    0.6325    0.6637       117
      Expansion.Restatement     0.4667    0.4953    0.4805       212
      Expansion.Alternative     0.2222    0.4444    0.2963         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3233    0.3329    0.3222      1039
               weighted avg     0.4930    0.4966    0.4927      1039

Epoch [26/30]
top-down:TOP: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.12%, Val F1: 51.26% Time: 119.6663670539856 
top-down:SEC: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 47.55%, Val F1: 30.71% Time: 119.6663670539856 
top-down:CONN: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   7.9,  Val Acc: 26.52%, Val F1:  8.85% Time: 119.6663670539856 
 
 
top-down:TOP: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 61.37%, Val F1: 50.07% Time: 277.1382694244385 
top-down:SEC: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 47.64%, Val F1: 30.56% Time: 277.1382694244385 
top-down:CONN: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 27.64%, Val F1:  9.09% Time: 277.1382694244385 
 
 
top-down:TOP: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.55%, Val F1: 51.55% Time: 447.3023748397827 
top-down:SEC: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 48.07%, Val F1: 30.39% Time: 447.3023748397827 
top-down:CONN: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   7.9,  Val Acc: 26.87%, Val F1:  8.57% Time: 447.3023748397827 
 
 
top-down:TOP: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.55%, Val F1: 51.93% Time: 606.099351644516 
top-down:SEC: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 48.67%, Val F1: 31.74% Time: 606.099351644516 
top-down:CONN: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   7.9,  Val Acc: 26.44%, Val F1:  8.74% Time: 606.099351644516 
 
 
Train time usage: 633.4006197452545
Test time usage: 1.8650176525115967
TOP: Test Loss:   7.7,  Test Acc: 63.52%, Test F1: 54.90%
SEC: Test Loss:   7.7,  Test Acc: 49.47%, Test F1: 31.43%
CONN: Test Loss:   7.7,  Test Acc: 26.56%, Test F1: 10.76%
consistency_top_sec: 48.41%,  consistency_sec_conn: 20.79%, consistency_top_sec_conn: 20.50%
              precision    recall  f1-score   support

    Temporal     0.5000    0.3768    0.4298        69
 Contingency     0.5944    0.5401    0.5660       274
  Comparison     0.4925    0.4552    0.4731       145
   Expansion     0.6954    0.7623    0.7273       551

    accuracy                         0.6352      1039
   macro avg     0.5706    0.5336    0.5490      1039
weighted avg     0.6275    0.6352    0.6295      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5682    0.4545    0.5051        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5587    0.5149    0.5359       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4754    0.4531    0.4640       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4280    0.5200    0.4695       200
    Expansion.Instantiation     0.7172    0.6068    0.6574       117
      Expansion.Restatement     0.4542    0.5377    0.4924       212
      Expansion.Alternative     0.2667    0.4444    0.3333         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4947      1039
                  macro avg     0.3153    0.3211    0.3143      1039
               weighted avg     0.4909    0.4947    0.4899      1039

Epoch [27/30]
top-down:TOP: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.46%, Val F1: 51.24% Time: 129.65304112434387 
top-down:SEC: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 48.67%, Val F1: 31.71% Time: 129.65304112434387 
top-down:CONN: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   7.9,  Val Acc: 26.87%, Val F1:  8.96% Time: 129.65304112434387 
 
 
top-down:TOP: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 60.94%, Val F1: 51.06% Time: 294.7888877391815 
top-down:SEC: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 48.15%, Val F1: 30.14% Time: 294.7888877391815 
top-down:CONN: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   7.9,  Val Acc: 25.75%, Val F1:  8.51% Time: 294.7888877391815 
 
 
top-down:TOP: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.29%, Val F1: 51.03% Time: 458.4080274105072 
top-down:SEC: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 48.58%, Val F1: 30.56% Time: 458.4080274105072 
top-down:CONN: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 81.25%,Val Loss:   7.9,  Val Acc: 26.61%, Val F1:  8.78% Time: 458.4080274105072 
 
 
top-down:TOP: Iter:  10600,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.12%, Val F1: 50.41% Time: 625.5049288272858 
top-down:SEC: Iter:  10600,  Train Loss: 3.2e+01,  Train Acc: 87.50%,Val Loss:   7.9,  Val Acc: 48.84%, Val F1: 31.18% Time: 625.5049288272858 
top-down:CONN: Iter:  10600,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   7.9,  Val Acc: 26.01%, Val F1:  8.63% Time: 625.5049288272858 
 
 
Train time usage: 645.2988843917847
Test time usage: 2.203843355178833
TOP: Test Loss:   7.7,  Test Acc: 63.33%, Test F1: 54.90%
SEC: Test Loss:   7.7,  Test Acc: 49.57%, Test F1: 31.87%
CONN: Test Loss:   7.7,  Test Acc: 25.89%, Test F1: 10.53%
consistency_top_sec: 48.12%,  consistency_sec_conn: 20.21%, consistency_top_sec_conn: 19.83%
              precision    recall  f1-score   support

    Temporal     0.4727    0.3768    0.4194        69
 Contingency     0.6068    0.5182    0.5591       274
  Comparison     0.4898    0.4966    0.4932       145
   Expansion     0.6932    0.7586    0.7244       551

    accuracy                         0.6333      1039
   macro avg     0.5656    0.5376    0.5490      1039
weighted avg     0.6274    0.6333    0.6283      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5333    0.4364    0.4800        55
         Temporal.Synchrony     0.1250    0.0714    0.0909        14
          Contingency.Cause     0.5732    0.5131    0.5415       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4737    0.4922    0.4828       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4315    0.5200    0.4717       200
    Expansion.Instantiation     0.6759    0.6186    0.6460       118
      Expansion.Restatement     0.4599    0.5142    0.4855       212
      Expansion.Alternative     0.2353    0.4444    0.3077         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4957      1039
                  macro avg     0.3189    0.3282    0.3187      1039
               weighted avg     0.4913    0.4957    0.4912      1039

Epoch [28/30]
top-down:TOP: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.63%, Val F1: 50.64% Time: 146.69576168060303 
top-down:SEC: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 48.33%, Val F1: 30.42% Time: 146.69576168060303 
top-down:CONN: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   7.9,  Val Acc: 26.35%, Val F1:  8.90% Time: 146.69576168060303 
 
 
top-down:TOP: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 62.06%, Val F1: 52.20% Time: 302.9127140045166 
top-down:SEC: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 48.41%, Val F1: 30.91% Time: 302.9127140045166 
top-down:CONN: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   7.9,  Val Acc: 26.52%, Val F1:  8.90% Time: 302.9127140045166 
 
 
top-down:TOP: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.72%, Val F1: 50.96% Time: 466.8085722923279 
top-down:SEC: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 47.98%, Val F1: 31.07% Time: 466.8085722923279 
top-down:CONN: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   7.9,  Val Acc: 26.52%, Val F1:  8.81% Time: 466.8085722923279 
 
 
top-down:TOP: Iter:  11000,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.46%, Val F1: 51.26% Time: 624.2425587177277 
top-down:SEC: Iter:  11000,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 48.33%, Val F1: 31.11% Time: 624.2425587177277 
top-down:CONN: Iter:  11000,  Train Loss: 2.3e+01,  Train Acc: 78.12%,Val Loss:   7.9,  Val Acc: 26.18%, Val F1:  8.62% Time: 624.2425587177277 
 
 
Train time usage: 635.0435938835144
Test time usage: 1.875201940536499
TOP: Test Loss:   7.7,  Test Acc: 63.14%, Test F1: 54.72%
SEC: Test Loss:   7.7,  Test Acc: 48.89%, Test F1: 30.58%
CONN: Test Loss:   7.7,  Test Acc: 25.99%, Test F1: 10.61%
consistency_top_sec: 48.03%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 20.69%
              precision    recall  f1-score   support

    Temporal     0.4655    0.3913    0.4252        69
 Contingency     0.6044    0.4964    0.5451       274
  Comparison     0.5072    0.4828    0.4947       145
   Expansion     0.6845    0.7677    0.7237       551

    accuracy                         0.6314      1039
   macro avg     0.5654    0.5345    0.5472      1039
weighted avg     0.6241    0.6314    0.6248      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4808    0.4545    0.4673        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5877    0.5000    0.5403       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4662    0.4844    0.4751       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4080    0.5100    0.4533       200
    Expansion.Instantiation     0.6952    0.6239    0.6577       117
      Expansion.Restatement     0.4615    0.5094    0.4843       212
      Expansion.Alternative     0.2105    0.4444    0.2857         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4889      1039
                  macro avg     0.3009    0.3206    0.3058      1039
               weighted avg     0.4873    0.4889    0.4853      1039

Epoch [29/30]
top-down:TOP: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 62.06%, Val F1: 50.99% Time: 166.05218291282654 
top-down:SEC: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 48.50%, Val F1: 30.43% Time: 166.05218291282654 
top-down:CONN: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 27.04%, Val F1:  8.99% Time: 166.05218291282654 
 
 
top-down:TOP: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.29%, Val F1: 50.57% Time: 337.3624060153961 
top-down:SEC: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 48.58%, Val F1: 31.06% Time: 337.3624060153961 
top-down:CONN: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   7.9,  Val Acc: 26.78%, Val F1:  9.02% Time: 337.3624060153961 
 
 
top-down:TOP: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.63%, Val F1: 50.13% Time: 503.1665184497833 
top-down:SEC: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 48.24%, Val F1: 30.30% Time: 503.1665184497833 
top-down:CONN: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   7.9,  Val Acc: 26.35%, Val F1:  8.76% Time: 503.1665184497833 
 
 
Train time usage: 660.5656177997589
Test time usage: 2.1576144695281982
TOP: Test Loss:   7.8,  Test Acc: 62.95%, Test F1: 54.69%
SEC: Test Loss:   7.8,  Test Acc: 49.28%, Test F1: 30.78%
CONN: Test Loss:   7.8,  Test Acc: 26.47%, Test F1: 10.92%
consistency_top_sec: 47.45%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.4821    0.3913    0.4320        69
 Contingency     0.5948    0.5036    0.5455       274
  Comparison     0.4930    0.4828    0.4878       145
   Expansion     0.6880    0.7604    0.7224       551

    accuracy                         0.6295      1039
   macro avg     0.5645    0.5345    0.5469      1039
weighted avg     0.6225    0.6295    0.6237      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5455    0.4364    0.4848        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5789    0.4925    0.5323       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4885    0.5000    0.4942       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4206    0.5300    0.4690       200
    Expansion.Instantiation     0.6822    0.6239    0.6518       117
      Expansion.Restatement     0.4619    0.5142    0.4866       212
      Expansion.Alternative     0.1905    0.4444    0.2667         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4928      1039
                  macro avg     0.3062    0.3219    0.3078      1039
               weighted avg     0.4921    0.4928    0.4891      1039

Epoch [30/30]
top-down:TOP: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 61.37%, Val F1: 49.93% Time: 6.447099685668945 
top-down:SEC: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.9,  Val Acc: 48.50%, Val F1: 30.51% Time: 6.447099685668945 
top-down:CONN: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.9,  Val Acc: 26.35%, Val F1:  8.86% Time: 6.447099685668945 
 
 
top-down:TOP: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.97%, Val F1: 51.07% Time: 165.58005452156067 
top-down:SEC: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 48.33%, Val F1: 30.23% Time: 165.58005452156067 
top-down:CONN: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   7.9,  Val Acc: 26.61%, Val F1:  8.93% Time: 165.58005452156067 
 
 
top-down:TOP: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.80%, Val F1: 50.65% Time: 331.62826013565063 
top-down:SEC: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 48.24%, Val F1: 30.25% Time: 331.62826013565063 
top-down:CONN: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 26.70%, Val F1:  9.03% Time: 331.62826013565063 
 
 
top-down:TOP: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.89%, Val F1: 51.05% Time: 492.83839654922485 
top-down:SEC: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 48.33%, Val F1: 30.19% Time: 492.83839654922485 
top-down:CONN: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   7.9,  Val Acc: 26.78%, Val F1:  9.00% Time: 492.83839654922485 
 
 
Train time usage: 642.5784718990326
Test time usage: 2.187915802001953
TOP: Test Loss:   7.8,  Test Acc: 62.95%, Test F1: 54.57%
SEC: Test Loss:   7.8,  Test Acc: 48.99%, Test F1: 30.61%
CONN: Test Loss:   7.8,  Test Acc: 26.47%, Test F1: 10.78%
consistency_top_sec: 47.64%,  consistency_sec_conn: 20.98%, consistency_top_sec_conn: 20.69%
              precision    recall  f1-score   support

    Temporal     0.4737    0.3913    0.4286        69
 Contingency     0.5957    0.5109    0.5501       274
  Comparison     0.4964    0.4690    0.4823       145
   Expansion     0.6869    0.7604    0.7218       551

    accuracy                         0.6295      1039
   macro avg     0.5632    0.5329    0.5457      1039
weighted avg     0.6221    0.6295    0.6236      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4364    0.4660        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5672    0.5037    0.5336       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4880    0.4766    0.4822       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4257    0.5300    0.4722       200
    Expansion.Instantiation     0.6857    0.6154    0.6486       117
      Expansion.Restatement     0.4553    0.5047    0.4787       212
      Expansion.Alternative     0.2105    0.4444    0.2857         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4899      1039
                  macro avg     0.3030    0.3192    0.3061      1039
               weighted avg     0.4868    0.4899    0.4858      1039

dev_best_acc_top: 62.92%,  dev_best_f1_top: 53.17%, 
dev_best_acc_sec: 51.59%,  dev_best_f1_sec: 33.49%, 
dev_best_acc_conn: 29.27%,  dev_best_f1_conn:  7.42%
