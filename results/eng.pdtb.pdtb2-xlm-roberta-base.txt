nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'PDTB/Ji/data/', 'log_file': 'PDTB/Ji/log/', 'save_file': 'PDTB/Ji/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'February28-09:19:42', 'log': 'PDTB/Ji/log/February28-09:19:42.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]97it [00:00, 967.90it/s]300it [00:00, 1588.90it/s]499it [00:00, 1771.57it/s]706it [00:00, 1887.69it/s]904it [00:00, 1917.90it/s]1107it [00:00, 1953.11it/s]1338it [00:00, 2068.29it/s]1558it [00:00, 2109.35it/s]1769it [00:00, 2079.95it/s]1978it [00:01, 2053.59it/s]2184it [00:01, 2037.26it/s]2388it [00:01, 2026.24it/s]2596it [00:01, 2041.59it/s]2808it [00:01, 2063.79it/s]3015it [00:01, 2034.52it/s]3221it [00:01, 2041.36it/s]3431it [00:01, 2056.89it/s]3637it [00:01, 2003.84it/s]3838it [00:01, 1963.48it/s]4035it [00:02, 1962.54it/s]4232it [00:02, 1749.78it/s]4415it [00:02, 1771.52it/s]4615it [00:02, 1831.93it/s]4834it [00:02, 1931.52it/s]5030it [00:02, 1938.41it/s]5226it [00:02, 1385.94it/s]5405it [00:02, 1477.48it/s]5623it [00:03, 1650.46it/s]5852it [00:03, 1816.35it/s]6048it [00:03, 1807.10it/s]6239it [00:03, 1831.03it/s]6442it [00:03, 1885.28it/s]6643it [00:03, 1919.45it/s]6849it [00:03, 1957.65it/s]7051it [00:03, 1975.61it/s]7255it [00:03, 1992.09it/s]7456it [00:03, 1984.95it/s]7664it [00:04, 2010.89it/s]7866it [00:04, 1884.34it/s]8057it [00:04, 1891.23it/s]8262it [00:04, 1934.28it/s]8459it [00:04, 1942.14it/s]8664it [00:04, 1972.51it/s]8862it [00:04, 1936.82it/s]9059it [00:04, 1944.25it/s]9254it [00:04, 1935.87it/s]9448it [00:04, 1935.03it/s]9664it [00:05, 2001.00it/s]9881it [00:05, 2049.78it/s]10089it [00:05, 2058.32it/s]10295it [00:05, 2028.01it/s]10507it [00:05, 2055.13it/s]10713it [00:05, 2029.80it/s]10926it [00:05, 2059.21it/s]11137it [00:05, 2072.26it/s]11349it [00:05, 2084.15it/s]11560it [00:05, 2090.50it/s]11773it [00:06, 2099.37it/s]11983it [00:06, 2074.30it/s]12191it [00:06, 1996.61it/s]12392it [00:06, 1988.41it/s]12547it [00:06, 1935.37it/s]
0it [00:00, ?it/s]181it [00:00, 1808.79it/s]362it [00:00, 1786.40it/s]544it [00:00, 1800.08it/s]725it [00:00, 1653.23it/s]906it [00:00, 1703.56it/s]1078it [00:00, 1705.39it/s]1165it [00:00, 1713.66it/s]
0it [00:00, ?it/s]174it [00:00, 1733.49it/s]362it [00:00, 1817.07it/s]553it [00:00, 1858.60it/s]739it [00:00, 1838.56it/s]923it [00:00, 1815.42it/s]1039it [00:00, 1835.46it/s]
Time usage: 16.990752935409546
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.8,  Val Acc: 55.88%, Val F1: 17.92% Time: 85.96544027328491 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 18.75%,Val Loss:   6.8,  Val Acc: 24.21%, Val F1:  3.54% Time: 85.96544027328491 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  3.12%,Val Loss:   6.8,  Val Acc: 12.88%, Val F1:  0.34% Time: 85.96544027328491 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   6.4,  Val Acc: 53.39%, Val F1: 25.58% Time: 165.4693570137024 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   6.4,  Val Acc: 31.33%, Val F1:  7.56% Time: 165.4693570137024 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.4,  Val Acc: 14.94%, Val F1:  0.87% Time: 165.4693570137024 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 55.45%, Val F1: 20.75% Time: 244.79519176483154 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 34.38%,Val Loss:   6.2,  Val Acc: 33.82%, Val F1: 11.14% Time: 244.79519176483154 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 18.75%,Val Loss:   6.2,  Val Acc: 16.48%, Val F1:  1.18% Time: 244.79519176483154 *
 
 
Train time usage: 317.1776373386383
Test time usage: 1.7551946640014648
TOP: Test Loss:   5.8,  Test Acc: 56.40%, Test F1: 27.00%
SEC: Test Loss:   5.8,  Test Acc: 37.92%, Test F1: 16.67%
CONN: Test Loss:   5.8,  Test Acc: 18.58%, Test F1:  1.99%
consistency_top_sec: 29.36%,  consistency_sec_conn: 14.34%, consistency_top_sec_conn: 10.01%
              precision    recall  f1-score   support

    Temporal     0.3333    0.0441    0.0779        68
 Contingency     0.6167    0.1365    0.2236       271
  Comparison     0.8333    0.0347    0.0667       144
   Expansion     0.5612    0.9730    0.7118       556

    accuracy                         0.5640      1039
   macro avg     0.5861    0.2971    0.2700      1039
weighted avg     0.5985    0.5640    0.4536      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3750    0.1111    0.1714        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5104    0.4572    0.4824       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4286    0.0469    0.0845       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4484    0.5000    0.4728       200
    Expansion.Instantiation     0.8889    0.1356    0.2353       118
      Expansion.Restatement     0.2713    0.6777    0.3875       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.3792      1039
                  macro avg     0.2657    0.1753    0.1667      1039
               weighted avg     0.4468    0.3792    0.3406      1039

Epoch [2/15]
top-down:TOP: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   6.0,  Val Acc: 56.14%, Val F1: 40.52% Time: 9.103943109512329 *
top-down:SEC: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   6.0,  Val Acc: 38.97%, Val F1: 17.62% Time: 9.103943109512329 *
top-down:CONN: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 12.50%,Val Loss:   6.0,  Val Acc: 19.91%, Val F1:  2.38% Time: 9.103943109512329 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.8,  Val Acc: 57.17%, Val F1: 40.99% Time: 88.89709687232971 *
top-down:SEC: Iter:    500,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   5.8,  Val Acc: 42.06%, Val F1: 20.39% Time: 88.89709687232971 *
top-down:CONN: Iter:    500,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   5.8,  Val Acc: 23.61%, Val F1:  3.44% Time: 88.89709687232971 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 75.00%,Val Loss:   5.6,  Val Acc: 57.85%, Val F1: 46.44% Time: 168.74602794647217 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   5.6,  Val Acc: 43.78%, Val F1: 24.55% Time: 168.74602794647217 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   5.6,  Val Acc: 24.03%, Val F1:  3.68% Time: 168.74602794647217 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 60.77%, Val F1: 49.69% Time: 248.2258265018463 *
top-down:SEC: Iter:    700,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 44.64%, Val F1: 24.34% Time: 248.2258265018463 *
top-down:CONN: Iter:    700,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   5.5,  Val Acc: 25.92%, Val F1:  4.12% Time: 248.2258265018463 *
 
 
Train time usage: 315.30509638786316
Test time usage: 1.7622840404510498
TOP: Test Loss:   5.1,  Test Acc: 61.31%, Test F1: 50.10%
SEC: Test Loss:   5.1,  Test Acc: 49.47%, Test F1: 27.80%
CONN: Test Loss:   5.1,  Test Acc: 27.82%, Test F1:  5.83%
consistency_top_sec: 42.25%,  consistency_sec_conn: 21.75%, consistency_top_sec_conn: 18.96%
              precision    recall  f1-score   support

    Temporal     0.4839    0.4412    0.4615        68
 Contingency     0.5442    0.2952    0.3828       271
  Comparison     0.5376    0.3472    0.4219       144
   Expansion     0.6472    0.8579    0.7378       556

    accuracy                         0.6131      1039
   macro avg     0.5532    0.4854    0.5010      1039
weighted avg     0.5945    0.6131    0.5834      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4366    0.5741    0.4960        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5399    0.5318    0.5358       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5106    0.3750    0.4324       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4781    0.6000    0.5322       200
    Expansion.Instantiation     0.6124    0.6639    0.6371       119
      Expansion.Restatement     0.4069    0.4434    0.4244       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4947      1039
                  macro avg     0.2713    0.2898    0.2780      1039
               weighted avg     0.4695    0.4947    0.4787      1039

Epoch [3/15]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   5.4,  Val Acc: 59.48%, Val F1: 51.84% Time: 14.425464630126953 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 46.35%, Val F1: 28.66% Time: 14.425464630126953 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 26.01%, Val F1:  4.92% Time: 14.425464630126953 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   5.4,  Val Acc: 61.29%, Val F1: 50.04% Time: 94.17434906959534 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 47.47%, Val F1: 27.60% Time: 94.17434906959534 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 18.75%,Val Loss:   5.4,  Val Acc: 27.30%, Val F1:  4.98% Time: 94.17434906959534 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   5.3,  Val Acc: 63.86%, Val F1: 50.56% Time: 174.44697785377502 *
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   5.3,  Val Acc: 50.64%, Val F1: 30.12% Time: 174.44697785377502 *
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   5.3,  Val Acc: 27.38%, Val F1:  5.11% Time: 174.44697785377502 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   5.3,  Val Acc: 64.46%, Val F1: 52.33% Time: 253.61344933509827 *
top-down:SEC: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.3,  Val Acc: 49.79%, Val F1: 31.04% Time: 253.61344933509827 *
top-down:CONN: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 27.81%, Val F1:  5.50% Time: 253.61344933509827 *
 
 
Train time usage: 314.56256675720215
Test time usage: 1.7223799228668213
TOP: Test Loss:   5.1,  Test Acc: 63.33%, Test F1: 53.95%
SEC: Test Loss:   5.1,  Test Acc: 49.28%, Test F1: 29.30%
CONN: Test Loss:   5.1,  Test Acc: 25.89%, Test F1:  7.13%
consistency_top_sec: 46.01%,  consistency_sec_conn: 20.79%, consistency_top_sec_conn: 18.86%
              precision    recall  f1-score   support

    Temporal     0.5581    0.3529    0.4324        68
 Contingency     0.6369    0.3919    0.4853       273
  Comparison     0.4520    0.5556    0.4984       144
   Expansion     0.6866    0.8069    0.7419       554

    accuracy                         0.6333      1039
   macro avg     0.5834    0.5268    0.5395      1039
weighted avg     0.6326    0.6333    0.6205      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4615    0.4444    0.4528        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5885    0.4963    0.5385       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.3892    0.5078    0.4407       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4135    0.6450    0.5039       200
    Expansion.Instantiation     0.7379    0.6441    0.6878       118
      Expansion.Restatement     0.4773    0.3962    0.4330       212
      Expansion.Alternative     0.3333    0.1111    0.1667         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4928      1039
                  macro avg     0.3092    0.2950    0.2930      1039
               weighted avg     0.4874    0.4928    0.4816      1039

Epoch [4/15]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.3,  Val Acc: 64.64%, Val F1: 52.96% Time: 19.841545343399048 *
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 50.47%, Val F1: 30.43% Time: 19.841545343399048 *
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   5.3,  Val Acc: 29.27%, Val F1:  6.56% Time: 19.841545343399048 *
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 64.55%, Val F1: 53.59% Time: 97.35536909103394 
top-down:SEC: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 65.62%,Val Loss:   5.3,  Val Acc: 49.61%, Val F1: 31.19% Time: 97.35536909103394 
top-down:CONN: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 27.21%, Val F1:  5.48% Time: 97.35536909103394 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   5.4,  Val Acc: 61.37%, Val F1: 51.19% Time: 174.90955877304077 
top-down:SEC: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   5.4,  Val Acc: 48.84%, Val F1: 31.35% Time: 174.90955877304077 
top-down:CONN: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 29.27%, Val F1:  6.76% Time: 174.90955877304077 
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 61.89%, Val F1: 50.49% Time: 252.4006609916687 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   5.4,  Val Acc: 48.24%, Val F1: 31.29% Time: 252.4006609916687 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 31.25%,Val Loss:   5.4,  Val Acc: 28.67%, Val F1:  7.13% Time: 252.4006609916687 
 
 
Train time usage: 308.95388674736023
Test time usage: 1.7607862949371338
TOP: Test Loss:   5.0,  Test Acc: 62.95%, Test F1: 54.98%
SEC: Test Loss:   5.0,  Test Acc: 50.24%, Test F1: 33.53%
CONN: Test Loss:   5.0,  Test Acc: 26.47%, Test F1:  8.39%
consistency_top_sec: 47.06%,  consistency_sec_conn: 20.50%, consistency_top_sec_conn: 20.12%
              precision    recall  f1-score   support

    Temporal     0.7000    0.3088    0.4286        68
 Contingency     0.5628    0.4762    0.5159       273
  Comparison     0.4778    0.5972    0.5309       144
   Expansion     0.6973    0.7527    0.7240       554

    accuracy                         0.6295      1039
   macro avg     0.6095    0.5337    0.5498      1039
weighted avg     0.6317    0.6295    0.6232      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7188    0.4259    0.5349        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5216    0.5431    0.5321       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4181    0.5781    0.4852       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4798    0.4750    0.4774       200
    Expansion.Instantiation     0.6864    0.6807    0.6835       119
      Expansion.Restatement     0.4455    0.4623    0.4537       212
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5024      1039
                  macro avg     0.3362    0.3483    0.3353      1039
               weighted avg     0.4885    0.5024    0.4916      1039

Epoch [5/15]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.4,  Val Acc: 64.46%, Val F1: 53.85% Time: 24.854089498519897 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 49.96%, Val F1: 32.54% Time: 24.854089498519897 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 29.18%, Val F1:  6.46% Time: 24.854089498519897 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 63.09%, Val F1: 53.05% Time: 102.17651963233948 
top-down:SEC: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   5.5,  Val Acc: 48.93%, Val F1: 31.73% Time: 102.17651963233948 
top-down:CONN: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   5.5,  Val Acc: 27.90%, Val F1:  6.48% Time: 102.17651963233948 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   5.6,  Val Acc: 62.23%, Val F1: 53.45% Time: 179.52017879486084 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   5.6,  Val Acc: 49.27%, Val F1: 31.71% Time: 179.52017879486084 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 27.47%, Val F1:  6.09% Time: 179.52017879486084 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 61.97%, Val F1: 53.03% Time: 257.3834743499756 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 50.04%, Val F1: 31.83% Time: 257.3834743499756 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 31.25%,Val Loss:   5.5,  Val Acc: 28.50%, Val F1:  6.52% Time: 257.3834743499756 
 
 
Train time usage: 307.7795901298523
Test time usage: 1.7410142421722412
TOP: Test Loss:   5.2,  Test Acc: 62.66%, Test F1: 54.06%
SEC: Test Loss:   5.2,  Test Acc: 50.63%, Test F1: 33.11%
CONN: Test Loss:   5.2,  Test Acc: 28.10%, Test F1:  8.63%
consistency_top_sec: 48.51%,  consistency_sec_conn: 22.43%, consistency_top_sec_conn: 21.75%
              precision    recall  f1-score   support

    Temporal     0.5227    0.3382    0.4107        68
 Contingency     0.5860    0.4615    0.5164       273
  Comparison     0.4655    0.5625    0.5094       144
   Expansion     0.6947    0.7599    0.7259       554

    accuracy                         0.6266      1039
   macro avg     0.5673    0.5306    0.5406      1039
weighted avg     0.6231    0.6266    0.6202      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5106    0.4444    0.4752        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5533    0.5075    0.5294       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4162    0.5625    0.4784       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4900    0.4900    0.4900       200
    Expansion.Instantiation     0.6917    0.6917    0.6917       120
      Expansion.Restatement     0.4500    0.5094    0.4779       212
      Expansion.Alternative     0.4000    0.6667    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5063      1039
                  macro avg     0.3193    0.3520    0.3311      1039
               weighted avg     0.4889    0.5063    0.4952      1039

Epoch [6/15]
top-down:TOP: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   5.5,  Val Acc: 62.40%, Val F1: 53.50% Time: 28.44401979446411 
top-down:SEC: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 50.64%, Val F1: 32.11% Time: 28.44401979446411 
top-down:CONN: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 29.18%, Val F1:  7.08% Time: 28.44401979446411 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 62.32%, Val F1: 52.22% Time: 106.3383584022522 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 48.84%, Val F1: 30.85% Time: 106.3383584022522 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 27.64%, Val F1:  6.87% Time: 106.3383584022522 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   5.6,  Val Acc: 62.75%, Val F1: 53.25% Time: 184.39516592025757 
top-down:SEC: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 49.36%, Val F1: 30.70% Time: 184.39516592025757 
top-down:CONN: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   5.6,  Val Acc: 28.50%, Val F1:  6.80% Time: 184.39516592025757 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.5e+01,  Train Acc: 84.38%,Val Loss:   5.6,  Val Acc: 63.00%, Val F1: 51.58% Time: 262.5213017463684 
top-down:SEC: Iter:   2300,  Train Loss: 3.5e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 49.18%, Val F1: 30.65% Time: 262.5213017463684 
top-down:CONN: Iter:   2300,  Train Loss: 3.5e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 28.24%, Val F1:  6.57% Time: 262.5213017463684 
 
 
Train time usage: 307.61681938171387
Test time usage: 1.7334158420562744
TOP: Test Loss:   5.3,  Test Acc: 62.56%, Test F1: 54.90%
SEC: Test Loss:   5.3,  Test Acc: 50.72%, Test F1: 33.85%
CONN: Test Loss:   5.3,  Test Acc: 26.56%, Test F1:  8.62%
consistency_top_sec: 48.60%,  consistency_sec_conn: 22.52%, consistency_top_sec_conn: 21.85%
              precision    recall  f1-score   support

    Temporal     0.4909    0.3971    0.4390        68
 Contingency     0.5618    0.5146    0.5371       274
  Comparison     0.4667    0.5310    0.4968       145
   Expansion     0.7130    0.7337    0.7232       552

    accuracy                         0.6256      1039
   macro avg     0.5581    0.5441    0.5490      1039
weighted avg     0.6242    0.6256    0.6239      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5094    0.5000    0.5047        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5352    0.5672    0.5507       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4268    0.5234    0.4702       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4545    0.5500    0.4977       200
    Expansion.Instantiation     0.7115    0.6271    0.6667       118
      Expansion.Restatement     0.5000    0.4292    0.4619       212
      Expansion.Alternative     0.5000    0.6667    0.5714         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5072      1039
                  macro avg     0.3307    0.3512    0.3385      1039
               weighted avg     0.4918    0.5072    0.4969      1039

Epoch [7/15]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   5.8,  Val Acc: 62.83%, Val F1: 53.19% Time: 33.58605909347534 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   5.8,  Val Acc: 49.61%, Val F1: 31.67% Time: 33.58605909347534 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 28.12%,Val Loss:   5.8,  Val Acc: 27.30%, Val F1:  6.59% Time: 33.58605909347534 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 62.75%, Val F1: 51.78% Time: 112.02921748161316 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 49.10%, Val F1: 31.08% Time: 112.02921748161316 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   5.8,  Val Acc: 27.38%, Val F1:  6.89% Time: 112.02921748161316 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 62.92%, Val F1: 53.82% Time: 189.82168674468994 
top-down:SEC: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 65.62%,Val Loss:   5.8,  Val Acc: 49.53%, Val F1: 32.88% Time: 189.82168674468994 
top-down:CONN: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 28.12%,Val Loss:   5.8,  Val Acc: 28.41%, Val F1:  6.97% Time: 189.82168674468994 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 62.83%, Val F1: 53.37% Time: 267.75204253196716 
top-down:SEC: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   5.8,  Val Acc: 49.01%, Val F1: 31.39% Time: 267.75204253196716 
top-down:CONN: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 25.00%,Val Loss:   5.8,  Val Acc: 28.67%, Val F1:  7.72% Time: 267.75204253196716 
 
 
Train time usage: 307.46444392204285
Test time usage: 1.7496144771575928
TOP: Test Loss:   5.5,  Test Acc: 63.62%, Test F1: 56.55%
SEC: Test Loss:   5.5,  Test Acc: 51.40%, Test F1: 34.05%
CONN: Test Loss:   5.5,  Test Acc: 27.05%, Test F1:  9.43%
consistency_top_sec: 48.99%,  consistency_sec_conn: 22.52%, consistency_top_sec_conn: 21.94%
              precision    recall  f1-score   support

    Temporal     0.5472    0.4203    0.4754        69
 Contingency     0.5678    0.5678    0.5678       273
  Comparison     0.5035    0.4897    0.4965       145
   Expansion     0.7098    0.7355    0.7224       552

    accuracy                         0.6362      1039
   macro avg     0.5821    0.5533    0.5655      1039
weighted avg     0.6329    0.6362    0.6339      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5091    0.5091    0.5091        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5464    0.5977    0.5709       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4519    0.4766    0.4639       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4506    0.5700    0.5033       200
    Expansion.Instantiation     0.7320    0.6017    0.6605       118
      Expansion.Restatement     0.4897    0.4460    0.4668       213
      Expansion.Alternative     0.5000    0.6667    0.5714         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5140      1039
                  macro avg     0.3345    0.3516    0.3405      1039
               weighted avg     0.4971    0.5140    0.5028      1039

Epoch [8/15]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   5.9,  Val Acc: 62.58%, Val F1: 53.83% Time: 39.06399607658386 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   5.9,  Val Acc: 49.79%, Val F1: 32.28% Time: 39.06399607658386 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   5.9,  Val Acc: 27.98%, Val F1:  7.22% Time: 39.06399607658386 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 62.58%, Val F1: 52.65% Time: 116.8822283744812 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   6.0,  Val Acc: 48.33%, Val F1: 31.09% Time: 116.8822283744812 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   6.0,  Val Acc: 27.81%, Val F1:  7.37% Time: 116.8822283744812 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 61.80%, Val F1: 51.92% Time: 195.05665349960327 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.1,  Val Acc: 48.50%, Val F1: 30.90% Time: 195.05665349960327 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   6.1,  Val Acc: 27.38%, Val F1:  7.37% Time: 195.05665349960327 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 62.15%, Val F1: 51.77% Time: 272.6829397678375 
top-down:SEC: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   6.1,  Val Acc: 48.93%, Val F1: 30.31% Time: 272.6829397678375 
top-down:CONN: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   6.1,  Val Acc: 27.30%, Val F1:  6.72% Time: 272.6829397678375 
 
 
Train time usage: 307.2118320465088
Test time usage: 1.7489333152770996
TOP: Test Loss:   5.8,  Test Acc: 64.49%, Test F1: 55.08%
SEC: Test Loss:   5.8,  Test Acc: 50.82%, Test F1: 33.64%
CONN: Test Loss:   5.8,  Test Acc: 25.89%, Test F1:  8.89%
consistency_top_sec: 49.57%,  consistency_sec_conn: 22.14%, consistency_top_sec_conn: 21.66%
              precision    recall  f1-score   support

    Temporal     0.5909    0.3768    0.4602        69
 Contingency     0.5845    0.4449    0.5052       272
  Comparison     0.5688    0.4306    0.4901       144
   Expansion     0.6789    0.8321    0.7478       554

    accuracy                         0.6449      1039
   macro avg     0.6058    0.5211    0.5508      1039
weighted avg     0.6331    0.6449    0.6295      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6047    0.4727    0.5306        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5639    0.4812    0.5193       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5143    0.4219    0.4635       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4237    0.6250    0.5051       200
    Expansion.Instantiation     0.6343    0.7143    0.6719       119
      Expansion.Restatement     0.4860    0.4906    0.4883       212
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5082      1039
                  macro avg     0.3323    0.3520    0.3364      1039
               weighted avg     0.4968    0.5082    0.4965      1039

Epoch [9/15]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.1,  Val Acc: 62.23%, Val F1: 53.35% Time: 44.424084424972534 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 48.93%, Val F1: 30.97% Time: 44.424084424972534 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   6.1,  Val Acc: 26.78%, Val F1:  6.64% Time: 44.424084424972534 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.3,  Val Acc: 61.63%, Val F1: 51.48% Time: 121.76427555084229 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   6.3,  Val Acc: 48.33%, Val F1: 31.25% Time: 121.76427555084229 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.3,  Val Acc: 27.12%, Val F1:  7.06% Time: 121.76427555084229 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 81.25%,Val Loss:   6.2,  Val Acc: 61.89%, Val F1: 52.30% Time: 199.48906302452087 
top-down:SEC: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 48.41%, Val F1: 30.77% Time: 199.48906302452087 
top-down:CONN: Iter:   3400,  Train Loss: 2.4e+01,  Train Acc: 50.00%,Val Loss:   6.2,  Val Acc: 28.76%, Val F1:  7.72% Time: 199.48906302452087 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 62.49%, Val F1: 52.87% Time: 278.2870650291443 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   6.2,  Val Acc: 48.93%, Val F1: 31.36% Time: 278.2870650291443 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   6.2,  Val Acc: 28.07%, Val F1:  7.72% Time: 278.2870650291443 
 
 
Train time usage: 307.4839117527008
Test time usage: 1.735450267791748
TOP: Test Loss:   6.0,  Test Acc: 61.79%, Test F1: 53.54%
SEC: Test Loss:   6.0,  Test Acc: 49.86%, Test F1: 32.68%
CONN: Test Loss:   6.0,  Test Acc: 25.99%, Test F1:  9.61%
consistency_top_sec: 47.26%,  consistency_sec_conn: 21.75%, consistency_top_sec_conn: 21.17%
              precision    recall  f1-score   support

    Temporal     0.5306    0.3768    0.4407        69
 Contingency     0.5561    0.4191    0.4780       272
  Comparison     0.4540    0.5486    0.4969       144
   Expansion     0.6923    0.7635    0.7262       554

    accuracy                         0.6179      1039
   macro avg     0.5583    0.5270    0.5354      1039
weighted avg     0.6129    0.6179    0.6105      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5200    0.4727    0.4952        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5535    0.4474    0.4948       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4161    0.5234    0.4637       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4664    0.5550    0.5068       200
    Expansion.Instantiation     0.6560    0.6891    0.6721       119
      Expansion.Restatement     0.4612    0.5047    0.4820       212
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4986      1039
                  macro avg     0.3135    0.3508    0.3268      1039
               weighted avg     0.4828    0.4986    0.4871      1039

Epoch [10/15]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   6.3,  Val Acc: 61.55%, Val F1: 52.07% Time: 49.49640512466431 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 48.84%, Val F1: 30.74% Time: 49.49640512466431 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 43.75%,Val Loss:   6.3,  Val Acc: 27.90%, Val F1:  7.15% Time: 49.49640512466431 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.3,  Val Acc: 62.83%, Val F1: 52.49% Time: 127.23031616210938 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   6.3,  Val Acc: 48.84%, Val F1: 30.91% Time: 127.23031616210938 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   6.3,  Val Acc: 27.98%, Val F1:  7.68% Time: 127.23031616210938 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.3,  Val Acc: 61.46%, Val F1: 51.76% Time: 204.60775446891785 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   6.3,  Val Acc: 48.93%, Val F1: 30.85% Time: 204.60775446891785 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   6.3,  Val Acc: 28.24%, Val F1:  7.07% Time: 204.60775446891785 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.4,  Val Acc: 61.97%, Val F1: 52.73% Time: 282.48154377937317 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 48.07%, Val F1: 29.87% Time: 282.48154377937317 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   6.4,  Val Acc: 27.73%, Val F1:  7.18% Time: 282.48154377937317 
 
 
Train time usage: 306.8213620185852
Test time usage: 1.7689847946166992
TOP: Test Loss:   6.0,  Test Acc: 63.33%, Test F1: 55.56%
SEC: Test Loss:   6.0,  Test Acc: 51.49%, Test F1: 34.03%
CONN: Test Loss:   6.0,  Test Acc: 26.95%, Test F1:  9.60%
consistency_top_sec: 50.14%,  consistency_sec_conn: 22.52%, consistency_top_sec_conn: 22.23%
              precision    recall  f1-score   support

    Temporal     0.6154    0.3529    0.4486        68
 Contingency     0.5579    0.5803    0.5689       274
  Comparison     0.4615    0.5000    0.4800       144
   Expansion     0.7209    0.7288    0.7248       553

    accuracy                         0.6333      1039
   macro avg     0.5889    0.5405    0.5556      1039
weighted avg     0.6351    0.6333    0.6317      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6098    0.4630    0.5263        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5392    0.5896    0.5633       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4183    0.5000    0.4555       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4553    0.5600    0.5022       200
    Expansion.Instantiation     0.7292    0.5932    0.6542       118
      Expansion.Restatement     0.5236    0.4717    0.4963       212
      Expansion.Alternative     0.4615    0.6667    0.5455         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5149      1039
                  macro avg     0.3397    0.3495    0.3403      1039
               weighted avg     0.5036    0.5149    0.5057      1039

Epoch [11/15]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.5,  Val Acc: 62.58%, Val F1: 53.39% Time: 54.8581440448761 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 78.12%,Val Loss:   6.5,  Val Acc: 48.50%, Val F1: 30.57% Time: 54.8581440448761 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 40.62%,Val Loss:   6.5,  Val Acc: 28.07%, Val F1:  7.47% Time: 54.8581440448761 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.5,  Val Acc: 62.32%, Val F1: 51.90% Time: 133.43984007835388 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   6.5,  Val Acc: 48.84%, Val F1: 30.87% Time: 133.43984007835388 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   6.5,  Val Acc: 28.33%, Val F1:  7.72% Time: 133.43984007835388 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.5,  Val Acc: 61.89%, Val F1: 51.85% Time: 211.64543104171753 
top-down:SEC: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   6.5,  Val Acc: 48.58%, Val F1: 30.66% Time: 211.64543104171753 
top-down:CONN: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.5,  Val Acc: 27.38%, Val F1:  7.31% Time: 211.64543104171753 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.5,  Val Acc: 61.97%, Val F1: 51.78% Time: 288.7577359676361 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.5,  Val Acc: 48.33%, Val F1: 29.88% Time: 288.7577359676361 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   6.5,  Val Acc: 27.38%, Val F1:  6.98% Time: 288.7577359676361 
 
 
Train time usage: 307.25861048698425
Test time usage: 1.7583725452423096
TOP: Test Loss:   6.2,  Test Acc: 63.04%, Test F1: 55.86%
SEC: Test Loss:   6.2,  Test Acc: 50.72%, Test F1: 33.14%
CONN: Test Loss:   6.2,  Test Acc: 26.08%, Test F1:  9.48%
consistency_top_sec: 49.28%,  consistency_sec_conn: 21.85%, consistency_top_sec_conn: 21.56%
              precision    recall  f1-score   support

    Temporal     0.5283    0.4058    0.4590        69
 Contingency     0.5669    0.5275    0.5465       273
  Comparison     0.4675    0.5486    0.5048       144
   Expansion     0.7176    0.7306    0.7240       553

    accuracy                         0.6304      1039
   macro avg     0.5701    0.5531    0.5586      1039
weighted avg     0.6308    0.6304    0.6294      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4545    0.4762        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5603    0.5414    0.5507       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.3988    0.5078    0.4467       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4693    0.5350    0.5000       200
    Expansion.Instantiation     0.7117    0.6695    0.6900       118
      Expansion.Restatement     0.4903    0.4742    0.4821       213
      Expansion.Alternative     0.4000    0.6667    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5072      1039
                  macro avg     0.3209    0.3499    0.3314      1039
               weighted avg     0.4942    0.5072    0.4990      1039

Epoch [12/15]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.5,  Val Acc: 62.06%, Val F1: 53.06% Time: 60.66212606430054 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.5,  Val Acc: 48.67%, Val F1: 29.98% Time: 60.66212606430054 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.5,  Val Acc: 27.81%, Val F1:  7.63% Time: 60.66212606430054 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 62.40%, Val F1: 52.64% Time: 138.09384298324585 
top-down:SEC: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.6,  Val Acc: 48.50%, Val F1: 30.33% Time: 138.09384298324585 
top-down:CONN: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   6.6,  Val Acc: 27.98%, Val F1:  7.57% Time: 138.09384298324585 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   6.7,  Val Acc: 61.63%, Val F1: 51.85% Time: 216.68918466567993 
top-down:SEC: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 47.90%, Val F1: 30.08% Time: 216.68918466567993 
top-down:CONN: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 28.07%, Val F1:  7.67% Time: 216.68918466567993 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.6,  Val Acc: 62.83%, Val F1: 52.95% Time: 294.6983404159546 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 49.18%, Val F1: 31.48% Time: 294.6983404159546 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   6.6,  Val Acc: 27.64%, Val F1:  7.30% Time: 294.6983404159546 
 
 
Train time usage: 307.78691959381104
Test time usage: 1.7744171619415283
TOP: Test Loss:   6.3,  Test Acc: 62.85%, Test F1: 55.47%
SEC: Test Loss:   6.3,  Test Acc: 51.20%, Test F1: 33.15%
CONN: Test Loss:   6.3,  Test Acc: 26.08%, Test F1:  9.58%
consistency_top_sec: 48.89%,  consistency_sec_conn: 21.66%, consistency_top_sec_conn: 20.98%
              precision    recall  f1-score   support

    Temporal     0.5600    0.4058    0.4706        69
 Contingency     0.5542    0.5074    0.5298       272
  Comparison     0.4898    0.5000    0.4948       144
   Expansion     0.6998    0.7491    0.7236       554

    accuracy                         0.6285      1039
   macro avg     0.5760    0.5406    0.5547      1039
weighted avg     0.6233    0.6285    0.6244      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5319    0.4545    0.4902        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5521    0.5396    0.5458       265
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4406    0.4922    0.4649       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4773    0.5250    0.5000       200
    Expansion.Instantiation     0.7168    0.6807    0.6983       119
      Expansion.Restatement     0.4619    0.5117    0.4855       213
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5120      1039
                  macro avg     0.3212    0.3519    0.3315      1039
               weighted avg     0.4950    0.5120    0.5022      1039

Epoch [13/15]
top-down:TOP: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 61.72%, Val F1: 50.86% Time: 65.76511907577515 
top-down:SEC: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.7,  Val Acc: 47.81%, Val F1: 30.47% Time: 65.76511907577515 
top-down:CONN: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 27.98%, Val F1:  7.77% Time: 65.76511907577515 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   6.6,  Val Acc: 63.09%, Val F1: 53.04% Time: 143.5421278476715 
top-down:SEC: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 49.44%, Val F1: 31.09% Time: 143.5421278476715 
top-down:CONN: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   6.6,  Val Acc: 28.33%, Val F1:  8.01% Time: 143.5421278476715 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 62.58%, Val F1: 52.44% Time: 221.36465692520142 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   6.7,  Val Acc: 48.67%, Val F1: 30.24% Time: 221.36465692520142 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.7,  Val Acc: 27.38%, Val F1:  7.51% Time: 221.36465692520142 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 62.40%, Val F1: 51.84% Time: 299.5486023426056 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   6.7,  Val Acc: 47.73%, Val F1: 30.27% Time: 299.5486023426056 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   6.7,  Val Acc: 27.21%, Val F1:  7.23% Time: 299.5486023426056 
 
 
Train time usage: 307.69899725914
Test time usage: 1.7303807735443115
TOP: Test Loss:   6.4,  Test Acc: 62.95%, Test F1: 55.62%
SEC: Test Loss:   6.4,  Test Acc: 51.20%, Test F1: 33.67%
CONN: Test Loss:   6.4,  Test Acc: 26.18%, Test F1:  9.62%
consistency_top_sec: 49.28%,  consistency_sec_conn: 21.56%, consistency_top_sec_conn: 20.98%
              precision    recall  f1-score   support

    Temporal     0.5686    0.4203    0.4833        69
 Contingency     0.5447    0.4908    0.5164       273
  Comparison     0.5000    0.4931    0.4965       144
   Expansion     0.7000    0.7595    0.7285       553

    accuracy                         0.6295      1039
   macro avg     0.5783    0.5409    0.5562      1039
weighted avg     0.6228    0.6295    0.6243      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5918    0.5273    0.5577        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5363    0.4981    0.5165       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4255    0.4688    0.4461       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4733    0.5750    0.5192       200
    Expansion.Instantiation     0.7248    0.6752    0.6991       117
      Expansion.Restatement     0.4911    0.5164    0.5034       213
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5120      1039
                  macro avg     0.3269    0.3570    0.3367      1039
               weighted avg     0.4980    0.5120    0.5031      1039

Epoch [14/15]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.7,  Val Acc: 62.49%, Val F1: 52.37% Time: 71.52703309059143 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 48.58%, Val F1: 31.00% Time: 71.52703309059143 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.7,  Val Acc: 27.30%, Val F1:  7.41% Time: 71.52703309059143 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 62.66%, Val F1: 52.24% Time: 149.48285102844238 
top-down:SEC: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 48.24%, Val F1: 30.74% Time: 149.48285102844238 
top-down:CONN: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   6.7,  Val Acc: 27.55%, Val F1:  7.77% Time: 149.48285102844238 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 100.00%,Val Loss:   6.8,  Val Acc: 62.66%, Val F1: 52.08% Time: 227.98150515556335 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 81.25%,Val Loss:   6.8,  Val Acc: 48.50%, Val F1: 30.72% Time: 227.98150515556335 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   6.8,  Val Acc: 27.30%, Val F1:  7.61% Time: 227.98150515556335 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   6.8,  Val Acc: 61.55%, Val F1: 51.52% Time: 305.95530891418457 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 48.15%, Val F1: 30.75% Time: 305.95530891418457 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 53.12%,Val Loss:   6.8,  Val Acc: 27.73%, Val F1:  7.61% Time: 305.95530891418457 
 
 
Train time usage: 308.640830039978
Test time usage: 1.72957444190979
TOP: Test Loss:   6.5,  Test Acc: 62.56%, Test F1: 55.47%
SEC: Test Loss:   6.5,  Test Acc: 51.78%, Test F1: 34.15%
CONN: Test Loss:   6.5,  Test Acc: 26.37%, Test F1:  9.84%
consistency_top_sec: 49.47%,  consistency_sec_conn: 21.66%, consistency_top_sec_conn: 20.98%
              precision    recall  f1-score   support

    Temporal     0.5263    0.4348    0.4762        69
 Contingency     0.5394    0.5018    0.5199       273
  Comparison     0.4899    0.5069    0.4983       144
   Expansion     0.7081    0.7414    0.7244       553

    accuracy                         0.6256      1039
   macro avg     0.5659    0.5462    0.5547      1039
weighted avg     0.6215    0.6256    0.6228      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5385    0.5091    0.5234        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5564    0.5356    0.5458       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4267    0.5000    0.4604       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4829    0.5650    0.5207       200
    Expansion.Instantiation     0.7290    0.6667    0.6964       117
      Expansion.Restatement     0.4796    0.4977    0.4885       213
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5178      1039
                  macro avg     0.3311    0.3582    0.3415      1039
               weighted avg     0.5011    0.5178    0.5080      1039

Epoch [15/15]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.8,  Val Acc: 62.23%, Val F1: 51.84% Time: 75.72911977767944 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   6.8,  Val Acc: 48.33%, Val F1: 30.71% Time: 75.72911977767944 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   6.8,  Val Acc: 27.30%, Val F1:  7.48% Time: 75.72911977767944 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 62.66%, Val F1: 52.64% Time: 153.98130083084106 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   6.8,  Val Acc: 48.41%, Val F1: 30.60% Time: 153.98130083084106 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   6.8,  Val Acc: 27.90%, Val F1:  7.65% Time: 153.98130083084106 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 62.15%, Val F1: 51.89% Time: 232.79315781593323 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 48.50%, Val F1: 30.64% Time: 232.79315781593323 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 56.25%,Val Loss:   6.8,  Val Acc: 27.64%, Val F1:  7.53% Time: 232.79315781593323 
 
 
Train time usage: 306.8511621952057
Test time usage: 1.7226958274841309
TOP: Test Loss:   6.5,  Test Acc: 62.66%, Test F1: 55.40%
SEC: Test Loss:   6.5,  Test Acc: 51.49%, Test F1: 33.99%
CONN: Test Loss:   6.5,  Test Acc: 26.37%, Test F1:  9.70%
consistency_top_sec: 49.09%,  consistency_sec_conn: 21.85%, consistency_top_sec_conn: 20.98%
              precision    recall  f1-score   support

    Temporal     0.5577    0.4203    0.4793        69
 Contingency     0.5469    0.4908    0.5174       273
  Comparison     0.4803    0.5069    0.4932       144
   Expansion     0.7034    0.7505    0.7262       553

    accuracy                         0.6266      1039
   macro avg     0.5721    0.5421    0.5540      1039
weighted avg     0.6217    0.6266    0.6226      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5510    0.4909    0.5192        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5504    0.5318    0.5410       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4257    0.4922    0.4565       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4768    0.5650    0.5172       200
    Expansion.Instantiation     0.7222    0.6667    0.6933       117
      Expansion.Restatement     0.4818    0.4977    0.4896       213
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5149      1039
                  macro avg     0.3306    0.3555    0.3399      1039
               weighted avg     0.4986    0.5149    0.5053      1039

dev_best_acc_top: 64.46%,  dev_best_f1_top: 53.85%, 
dev_best_acc_sec: 49.96%,  dev_best_f1_sec: 32.54%, 
dev_best_acc_conn: 29.18%,  dev_best_f1_conn:  6.46%
