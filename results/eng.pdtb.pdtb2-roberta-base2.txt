nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/PDTB/Ji/data/', 'log_file': 'data/PDTB/Ji//log/', 'save_file': 'data/PDTB/Ji//saved_dict/', 'model_name_or_path': 'roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March04-17:24:12', 'log': 'data/PDTB/Ji//log/March04-17:24:12.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]66it [00:00, 651.80it/s]171it [00:00, 883.17it/s]265it [00:00, 906.15it/s]362it [00:00, 929.78it/s]464it [00:00, 956.37it/s]564it [00:00, 970.45it/s]685it [00:00, 1047.79it/s]798it [00:00, 1070.86it/s]915it [00:00, 1100.21it/s]1034it [00:01, 1126.97it/s]1153it [00:01, 1145.91it/s]1287it [00:01, 1204.03it/s]1416it [00:01, 1227.40it/s]1539it [00:01, 1216.81it/s]1661it [00:01, 1156.16it/s]1789it [00:01, 1190.29it/s]1913it [00:01, 1201.60it/s]2042it [00:01, 1226.99it/s]2166it [00:01, 1208.91it/s]2288it [00:02, 1201.32it/s]2425it [00:02, 1248.33it/s]2553it [00:02, 1256.16it/s]2680it [00:02, 1260.06it/s]2809it [00:02, 1266.55it/s]2940it [00:02, 1278.17it/s]3068it [00:02, 1259.81it/s]3195it [00:02, 1262.01it/s]3322it [00:02, 1259.94it/s]3460it [00:02, 1291.91it/s]3590it [00:03, 1271.00it/s]3719it [00:03, 1272.61it/s]3847it [00:03, 1243.08it/s]3982it [00:03, 1273.73it/s]4110it [00:03, 1267.20it/s]4237it [00:03, 1246.27it/s]4362it [00:03, 1224.92it/s]4485it [00:03, 1218.01it/s]4610it [00:03, 1226.27it/s]4744it [00:03, 1256.94it/s]4882it [00:04, 1291.27it/s]5016it [00:04, 1305.65it/s]5149it [00:04, 1308.88it/s]5293it [00:04, 1345.14it/s]5428it [00:04, 1318.11it/s]5572it [00:04, 1353.15it/s]5708it [00:04, 1345.62it/s]5869it [00:04, 1421.00it/s]6012it [00:04, 1375.55it/s]6150it [00:05, 1367.66it/s]6293it [00:05, 1384.10it/s]6432it [00:05, 1368.04it/s]6572it [00:05, 1373.30it/s]6711it [00:05, 1376.00it/s]6850it [00:05, 1377.19it/s]6988it [00:05, 1374.64it/s]7126it [00:05, 1341.00it/s]7274it [00:05, 1378.68it/s]7413it [00:05, 1359.72it/s]7550it [00:06, 1361.56it/s]7687it [00:06, 1352.23it/s]7828it [00:06, 1368.38it/s]7978it [00:06, 1404.79it/s]8119it [00:06, 1381.99it/s]8260it [00:06, 1388.43it/s]8399it [00:06, 1347.25it/s]8535it [00:06, 1332.60it/s]8669it [00:07, 849.61it/s] 8813it [00:07, 971.70it/s]8949it [00:07, 1058.48it/s]9094it [00:07, 1154.48it/s]9234it [00:07, 1216.55it/s]9380it [00:07, 1281.26it/s]9528it [00:07, 1336.11it/s]9675it [00:07, 1372.23it/s]9826it [00:07, 1411.91it/s]9971it [00:07, 1386.05it/s]10119it [00:08, 1412.28it/s]10262it [00:08, 1376.06it/s]10409it [00:08, 1401.52it/s]10567it [00:08, 1453.04it/s]10714it [00:08, 1443.20it/s]10863it [00:08, 1455.13it/s]11009it [00:08, 1450.74it/s]11157it [00:08, 1456.34it/s]11304it [00:08, 1460.24it/s]11451it [00:08, 1446.01it/s]11596it [00:09, 1438.87it/s]11740it [00:09, 1426.96it/s]11883it [00:09, 1395.16it/s]12023it [00:09, 1366.41it/s]12160it [00:09, 1360.67it/s]12307it [00:09, 1389.13it/s]12447it [00:09, 1360.58it/s]12547it [00:09, 1281.27it/s]
0it [00:00, ?it/s]140it [00:00, 1394.28it/s]289it [00:00, 1449.66it/s]434it [00:00, 1445.31it/s]579it [00:00, 1417.31it/s]721it [00:00, 1384.74it/s]862it [00:00, 1392.46it/s]1002it [00:00, 1389.35it/s]1149it [00:00, 1411.77it/s]1165it [00:00, 1410.84it/s]
0it [00:00, ?it/s]133it [00:00, 1329.61it/s]280it [00:00, 1411.03it/s]432it [00:00, 1458.99it/s]580it [00:00, 1467.16it/s]727it [00:00, 1463.86it/s]874it [00:00, 1446.60it/s]1028it [00:00, 1475.75it/s]1039it [00:00, 1456.86it/s]
Time usage: 21.54438805580139
https://huggingface.co:443 "HEAD /roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   6.6,  Val Acc: 55.88%, Val F1: 17.92% Time: 86.26428055763245 *
top-down:SEC: Iter:    100,  Train Loss: 3e+01,  Train Acc: 15.62%,Val Loss:   6.6,  Val Acc: 28.76%, Val F1:  9.20% Time: 86.26428055763245 *
top-down:CONN: Iter:    100,  Train Loss: 3e+01,  Train Acc: 25.00%,Val Loss:   6.6,  Val Acc: 15.36%, Val F1:  0.77% Time: 86.26428055763245 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.5e+01,  Train Acc: 65.62%,Val Loss:   6.1,  Val Acc: 59.14%, Val F1: 36.00% Time: 162.35335230827332 *
top-down:SEC: Iter:    200,  Train Loss: 3.5e+01,  Train Acc: 56.25%,Val Loss:   6.1,  Val Acc: 37.08%, Val F1: 12.61% Time: 162.35335230827332 *
top-down:CONN: Iter:    200,  Train Loss: 3.5e+01,  Train Acc: 21.88%,Val Loss:   6.1,  Val Acc: 18.37%, Val F1:  1.72% Time: 162.35335230827332 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 65.06%, Val F1: 52.41% Time: 238.59370350837708 *
top-down:SEC: Iter:    300,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   5.3,  Val Acc: 50.99%, Val F1: 25.68% Time: 238.59370350837708 *
top-down:CONN: Iter:    300,  Train Loss: 2.9e+01,  Train Acc: 31.25%,Val Loss:   5.3,  Val Acc: 26.78%, Val F1:  4.06% Time: 238.59370350837708 *
 
 
Train time usage: 310.1112837791443
Test time usage: 1.7595763206481934
TOP: Test Loss:   4.7,  Test Acc: 67.85%, Test F1: 60.27%
SEC: Test Loss:   4.7,  Test Acc: 56.11%, Test F1: 31.16%
CONN: Test Loss:   4.7,  Test Acc: 28.20%, Test F1:  6.29%
consistency_top_sec: 53.03%,  consistency_sec_conn: 23.68%, consistency_top_sec_conn: 23.10%
              precision    recall  f1-score   support

    Temporal     0.6486    0.3529    0.4571        68
 Contingency     0.5784    0.6460    0.6103       274
  Comparison     0.5695    0.5931    0.5811       145
   Expansion     0.7670    0.7572    0.7621       552

    accuracy                         0.6785      1039
   macro avg     0.6409    0.5873    0.6027      1039
weighted avg     0.6820    0.6785    0.6768      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5750    0.4259    0.4894        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5606    0.6877    0.6177       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5250    0.6562    0.5833       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5812    0.5550    0.5678       200
    Expansion.Instantiation     0.8841    0.5169    0.6524       118
      Expansion.Restatement     0.4779    0.5640    0.5174       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5611      1039
                  macro avg     0.3276    0.3096    0.3116      1039
               weighted avg     0.5490    0.5611    0.5457      1039

Epoch [2/15]
top-down:TOP: Iter:    400,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   5.1,  Val Acc: 66.61%, Val F1: 53.80% Time: 12.035727977752686 *
top-down:SEC: Iter:    400,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   5.1,  Val Acc: 51.67%, Val F1: 27.06% Time: 12.035727977752686 *
top-down:CONN: Iter:    400,  Train Loss: 3.2e+01,  Train Acc: 15.62%,Val Loss:   5.1,  Val Acc: 28.84%, Val F1:  5.99% Time: 12.035727977752686 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   4.9,  Val Acc: 67.38%, Val F1: 56.06% Time: 88.43436765670776 *
top-down:SEC: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   4.9,  Val Acc: 52.62%, Val F1: 28.41% Time: 88.43436765670776 *
top-down:CONN: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   4.9,  Val Acc: 29.61%, Val F1:  6.64% Time: 88.43436765670776 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   5.0,  Val Acc: 66.27%, Val F1: 56.40% Time: 165.4225137233734 *
top-down:SEC: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   5.0,  Val Acc: 51.33%, Val F1: 31.55% Time: 165.4225137233734 *
top-down:CONN: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   5.0,  Val Acc: 31.50%, Val F1:  7.97% Time: 165.4225137233734 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   4.8,  Val Acc: 67.73%, Val F1: 55.94% Time: 241.77276730537415 *
top-down:SEC: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   4.8,  Val Acc: 53.39%, Val F1: 33.63% Time: 241.77276730537415 *
top-down:CONN: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   4.8,  Val Acc: 30.56%, Val F1:  7.35% Time: 241.77276730537415 *
 
 
Train time usage: 305.57127046585083
Test time usage: 1.6728169918060303
TOP: Test Loss:   4.4,  Test Acc: 69.49%, Test F1: 58.89%
SEC: Test Loss:   4.4,  Test Acc: 58.71%, Test F1: 37.86%
CONN: Test Loss:   4.4,  Test Acc: 30.51%, Test F1: 10.23%
consistency_top_sec: 53.32%,  consistency_sec_conn: 24.54%, consistency_top_sec_conn: 22.14%
              precision    recall  f1-score   support

    Temporal     0.7037    0.2794    0.4000        68
 Contingency     0.6881    0.5092    0.5853       273
  Comparison     0.5929    0.5764    0.5845       144
   Expansion     0.7179    0.8682    0.7859       554

    accuracy                         0.6949      1039
   macro avg     0.6756    0.5583    0.5889      1039
weighted avg     0.6918    0.6949    0.6800      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7317    0.5556    0.6316        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6219    0.6567    0.6388       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5745    0.6328    0.6022       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5378    0.6050    0.5694       200
    Expansion.Instantiation     0.6395    0.7899    0.7068       119
      Expansion.Restatement     0.5580    0.4787    0.5153       211
      Expansion.Alternative     0.3684    0.7778    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5871      1039
                  macro avg     0.3665    0.4088    0.3786      1039
               weighted avg     0.5625    0.5871    0.5713      1039

Epoch [3/15]
top-down:TOP: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   4.8,  Val Acc: 68.24%, Val F1: 59.48% Time: 12.911417961120605 *
top-down:SEC: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   4.8,  Val Acc: 52.10%, Val F1: 32.48% Time: 12.911417961120605 *
top-down:CONN: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   4.8,  Val Acc: 33.39%, Val F1:  8.15% Time: 12.911417961120605 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   4.8,  Val Acc: 68.07%, Val F1: 59.06% Time: 89.34686064720154 *
top-down:SEC: Iter:    900,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   4.8,  Val Acc: 55.11%, Val F1: 35.01% Time: 89.34686064720154 *
top-down:CONN: Iter:    900,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   4.8,  Val Acc: 32.02%, Val F1:  9.26% Time: 89.34686064720154 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   4.8,  Val Acc: 68.07%, Val F1: 59.23% Time: 166.6637442111969 *
top-down:SEC: Iter:   1000,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   4.8,  Val Acc: 55.11%, Val F1: 35.31% Time: 166.6637442111969 *
top-down:CONN: Iter:   1000,  Train Loss: 2.8e+01,  Train Acc: 37.50%,Val Loss:   4.8,  Val Acc: 33.39%, Val F1:  8.83% Time: 166.6637442111969 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   4.9,  Val Acc: 68.67%, Val F1: 59.08% Time: 242.6082489490509 
top-down:SEC: Iter:   1100,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   4.9,  Val Acc: 53.73%, Val F1: 33.94% Time: 242.6082489490509 
top-down:CONN: Iter:   1100,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   4.9,  Val Acc: 32.53%, Val F1:  8.72% Time: 242.6082489490509 
 
 
Train time usage: 301.4020254611969
Test time usage: 1.6670124530792236
TOP: Test Loss:   4.4,  Test Acc: 70.93%, Test F1: 63.92%
SEC: Test Loss:   4.4,  Test Acc: 60.35%, Test F1: 39.14%
CONN: Test Loss:   4.4,  Test Acc: 31.76%, Test F1: 10.71%
consistency_top_sec: 59.00%,  consistency_sec_conn: 26.18%, consistency_top_sec_conn: 25.89%
              precision    recall  f1-score   support

    Temporal     0.6829    0.4118    0.5138        68
 Contingency     0.7123    0.5714    0.6341       273
  Comparison     0.5568    0.7103    0.6242       145
   Expansion     0.7576    0.8137    0.7847       553

    accuracy                         0.7093      1039
   macro avg     0.6774    0.6268    0.6392      1039
weighted avg     0.7128    0.7093    0.7050      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6444    0.5370    0.5859        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6573    0.6082    0.6318       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5027    0.7188    0.5916       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5594    0.5650    0.5622       200
    Expansion.Instantiation     0.8037    0.7227    0.7611       119
      Expansion.Restatement     0.5805    0.6493    0.6130       211
      Expansion.Alternative     0.4375    0.7778    0.5600         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.6035      1039
                  macro avg     0.3805    0.4163    0.3914      1039
               weighted avg     0.5864    0.6035    0.5910      1039

Epoch [4/15]
top-down:TOP: Iter:   1200,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   5.0,  Val Acc: 67.73%, Val F1: 58.88% Time: 17.26898956298828 
top-down:SEC: Iter:   1200,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.0,  Val Acc: 53.30%, Val F1: 33.90% Time: 17.26898956298828 
top-down:CONN: Iter:   1200,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   5.0,  Val Acc: 31.16%, Val F1:  8.54% Time: 17.26898956298828 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   5.0,  Val Acc: 68.58%, Val F1: 59.28% Time: 92.56284928321838 
top-down:SEC: Iter:   1300,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   5.0,  Val Acc: 54.33%, Val F1: 34.48% Time: 92.56284928321838 
top-down:CONN: Iter:   1300,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   5.0,  Val Acc: 32.62%, Val F1:  9.58% Time: 92.56284928321838 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   4.9,  Val Acc: 69.70%, Val F1: 62.07% Time: 170.4025101661682 *
top-down:SEC: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   4.9,  Val Acc: 54.94%, Val F1: 34.82% Time: 170.4025101661682 *
top-down:CONN: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   4.9,  Val Acc: 33.39%, Val F1:  9.78% Time: 170.4025101661682 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.0,  Val Acc: 67.81%, Val F1: 58.95% Time: 252.08069252967834 
top-down:SEC: Iter:   1500,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   5.0,  Val Acc: 54.59%, Val F1: 34.17% Time: 252.08069252967834 
top-down:CONN: Iter:   1500,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   5.0,  Val Acc: 32.88%, Val F1:  9.18% Time: 252.08069252967834 
 
 
Train time usage: 306.34716296195984
Test time usage: 1.6698415279388428
TOP: Test Loss:   4.5,  Test Acc: 71.41%, Test F1: 64.45%
SEC: Test Loss:   4.5,  Test Acc: 61.60%, Test F1: 42.38%
CONN: Test Loss:   4.5,  Test Acc: 32.82%, Test F1: 11.42%
consistency_top_sec: 59.67%,  consistency_sec_conn: 27.91%, consistency_top_sec_conn: 27.33%
              precision    recall  f1-score   support

    Temporal     0.5556    0.4412    0.4918        68
 Contingency     0.6761    0.6095    0.6411       274
  Comparison     0.6510    0.6690    0.6599       145
   Expansion     0.7606    0.8116    0.7853       552

    accuracy                         0.7141      1039
   macro avg     0.6608    0.6328    0.6445      1039
weighted avg     0.7096    0.7141    0.7105      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6182    0.6296    0.6239        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6543    0.6543    0.6543       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6054    0.6953    0.6473       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5700    0.5700    0.5700       200
    Expansion.Instantiation     0.8039    0.6949    0.7455       118
      Expansion.Restatement     0.5649    0.6398    0.6000       211
      Expansion.Alternative     0.4375    0.7778    0.5600         9
             Expansion.List     0.2727    0.2500    0.2609        12

                   accuracy                         0.6160      1039
                  macro avg     0.4115    0.4465    0.4238      1039
               weighted avg     0.5988    0.6160    0.6057      1039

Epoch [5/15]
top-down:TOP: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   5.0,  Val Acc: 68.93%, Val F1: 60.70% Time: 23.247545957565308 *
top-down:SEC: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.0,  Val Acc: 56.05%, Val F1: 36.06% Time: 23.247545957565308 *
top-down:CONN: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   5.0,  Val Acc: 33.99%, Val F1:  9.70% Time: 23.247545957565308 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   5.2,  Val Acc: 69.10%, Val F1: 61.41% Time: 98.71320939064026 
top-down:SEC: Iter:   1700,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   5.2,  Val Acc: 54.33%, Val F1: 34.72% Time: 98.71320939064026 
top-down:CONN: Iter:   1700,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   5.2,  Val Acc: 32.79%, Val F1:  9.27% Time: 98.71320939064026 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   5.3,  Val Acc: 68.15%, Val F1: 59.45% Time: 174.30642247200012 
top-down:SEC: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.3,  Val Acc: 54.76%, Val F1: 35.96% Time: 174.30642247200012 
top-down:CONN: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   5.3,  Val Acc: 31.67%, Val F1:  9.57% Time: 174.30642247200012 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   5.2,  Val Acc: 68.67%, Val F1: 62.75% Time: 252.07867980003357 *
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.2,  Val Acc: 55.71%, Val F1: 36.41% Time: 252.07867980003357 *
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   5.2,  Val Acc: 33.65%, Val F1:  9.81% Time: 252.07867980003357 *
 
 
Train time usage: 299.9720346927643
Test time usage: 1.6669344902038574
TOP: Test Loss:   4.6,  Test Acc: 70.93%, Test F1: 64.00%
SEC: Test Loss:   4.6,  Test Acc: 60.25%, Test F1: 39.86%
CONN: Test Loss:   4.6,  Test Acc: 31.86%, Test F1: 11.50%
consistency_top_sec: 58.42%,  consistency_sec_conn: 27.05%, consistency_top_sec_conn: 26.85%
              precision    recall  f1-score   support

    Temporal     0.5246    0.4706    0.4961        68
 Contingency     0.6936    0.5949    0.6405       274
  Comparison     0.6291    0.6552    0.6419       145
   Expansion     0.7551    0.8098    0.7815       552

    accuracy                         0.7093      1039
   macro avg     0.6506    0.6326    0.6400      1039
weighted avg     0.7062    0.7093    0.7061      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5660    0.5556    0.5607        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6586    0.6097    0.6332       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5986    0.6875    0.6400       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5079    0.6400    0.5664       200
    Expansion.Instantiation     0.8587    0.6695    0.7524       118
      Expansion.Restatement     0.5864    0.6114    0.5986       211
      Expansion.Alternative     0.4118    0.7778    0.5385         9
             Expansion.List     0.1111    0.0833    0.0952        12

                   accuracy                         0.6025      1039
                  macro avg     0.3908    0.4213    0.3986      1039
               weighted avg     0.5929    0.6025    0.5937      1039

Epoch [6/15]
top-down:TOP: Iter:   2000,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   5.3,  Val Acc: 68.24%, Val F1: 60.84% Time: 27.536534070968628 
top-down:SEC: Iter:   2000,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   5.3,  Val Acc: 55.02%, Val F1: 35.36% Time: 27.536534070968628 
top-down:CONN: Iter:   2000,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   5.3,  Val Acc: 32.53%, Val F1:  9.76% Time: 27.536534070968628 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   5.3,  Val Acc: 67.64%, Val F1: 59.40% Time: 102.79769134521484 
top-down:SEC: Iter:   2100,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   5.3,  Val Acc: 54.33%, Val F1: 34.45% Time: 102.79769134521484 
top-down:CONN: Iter:   2100,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.3,  Val Acc: 33.13%, Val F1:  9.70% Time: 102.79769134521484 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   5.3,  Val Acc: 68.41%, Val F1: 62.79% Time: 179.64588236808777 
top-down:SEC: Iter:   2200,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.3,  Val Acc: 55.11%, Val F1: 36.05% Time: 179.64588236808777 
top-down:CONN: Iter:   2200,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   5.3,  Val Acc: 32.36%, Val F1:  9.37% Time: 179.64588236808777 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   5.4,  Val Acc: 68.84%, Val F1: 61.95% Time: 259.78931975364685 
top-down:SEC: Iter:   2300,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   5.4,  Val Acc: 54.08%, Val F1: 35.87% Time: 259.78931975364685 
top-down:CONN: Iter:   2300,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   5.4,  Val Acc: 31.76%, Val F1:  9.60% Time: 259.78931975364685 
 
 
Train time usage: 305.0933437347412
Test time usage: 1.6754300594329834
TOP: Test Loss:   4.7,  Test Acc: 71.61%, Test F1: 64.79%
SEC: Test Loss:   4.7,  Test Acc: 61.89%, Test F1: 42.19%
CONN: Test Loss:   4.7,  Test Acc: 33.40%, Test F1: 12.48%
consistency_top_sec: 60.54%,  consistency_sec_conn: 27.82%, consistency_top_sec_conn: 27.43%
              precision    recall  f1-score   support

    Temporal     0.6122    0.4412    0.5128        68
 Contingency     0.6617    0.6423    0.6519       274
  Comparison     0.7143    0.5862    0.6439       145
   Expansion     0.7488    0.8207    0.7831       552

    accuracy                         0.7161      1039
   macro avg     0.6842    0.6226    0.6479      1039
weighted avg     0.7120    0.7161    0.7114      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6275    0.5926    0.6095        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6303    0.6679    0.6486       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6754    0.6016    0.6364       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5060    0.6300    0.5612       200
    Expansion.Instantiation     0.8241    0.7479    0.7841       119
      Expansion.Restatement     0.6329    0.6209    0.6268       211
      Expansion.Alternative     0.4667    0.7778    0.5833         9
             Expansion.List     0.2222    0.1667    0.1905        12

                   accuracy                         0.6189      1039
                  macro avg     0.4168    0.4368    0.4219      1039
               weighted avg     0.6053    0.6189    0.6098      1039

Epoch [7/15]
top-down:TOP: Iter:   2400,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   5.4,  Val Acc: 68.07%, Val F1: 60.53% Time: 34.69139575958252 
top-down:SEC: Iter:   2400,  Train Loss: 2.2e+01,  Train Acc: 87.50%,Val Loss:   5.4,  Val Acc: 56.22%, Val F1: 36.73% Time: 34.69139575958252 
top-down:CONN: Iter:   2400,  Train Loss: 2.2e+01,  Train Acc: 62.50%,Val Loss:   5.4,  Val Acc: 33.39%, Val F1: 10.36% Time: 34.69139575958252 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   5.5,  Val Acc: 67.64%, Val F1: 60.79% Time: 135.37225222587585 
top-down:SEC: Iter:   2500,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   5.5,  Val Acc: 53.91%, Val F1: 36.46% Time: 135.37225222587585 
top-down:CONN: Iter:   2500,  Train Loss: 2.3e+01,  Train Acc: 75.00%,Val Loss:   5.5,  Val Acc: 30.90%, Val F1:  9.78% Time: 135.37225222587585 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   5.5,  Val Acc: 69.27%, Val F1: 62.07% Time: 236.64636278152466 
top-down:SEC: Iter:   2600,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 55.36%, Val F1: 35.87% Time: 236.64636278152466 
top-down:CONN: Iter:   2600,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 33.30%, Val F1: 10.22% Time: 236.64636278152466 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   5.5,  Val Acc: 69.27%, Val F1: 61.83% Time: 344.29632210731506 
top-down:SEC: Iter:   2700,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 55.45%, Val F1: 36.75% Time: 344.29632210731506 
top-down:CONN: Iter:   2700,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 32.70%, Val F1:  9.72% Time: 344.29632210731506 
 
 
Train time usage: 399.77329111099243
Test time usage: 1.747976303100586
TOP: Test Loss:   4.9,  Test Acc: 70.93%, Test F1: 64.49%
SEC: Test Loss:   4.9,  Test Acc: 60.25%, Test F1: 41.20%
CONN: Test Loss:   4.9,  Test Acc: 33.49%, Test F1: 12.34%
consistency_top_sec: 58.13%,  consistency_sec_conn: 28.10%, consistency_top_sec_conn: 27.24%
              precision    recall  f1-score   support

    Temporal     0.5962    0.4559    0.5167        68
 Contingency     0.6722    0.5934    0.6304       273
  Comparison     0.6691    0.6414    0.6549       145
   Expansion     0.7430    0.8156    0.7776       553

    accuracy                         0.7093      1039
   macro avg     0.6701    0.6266    0.6449      1039
weighted avg     0.7045    0.7093    0.7047      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6200    0.5741    0.5962        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6409    0.6171    0.6288       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6364    0.6562    0.6462       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5085    0.6000    0.5505       200
    Expansion.Instantiation     0.7679    0.7288    0.7478       118
      Expansion.Restatement     0.5909    0.6161    0.6032       211
      Expansion.Alternative     0.4615    0.6667    0.5455         9
             Expansion.List     0.1875    0.2500    0.2143        12

                   accuracy                         0.6025      1039
                  macro avg     0.4012    0.4281    0.4120      1039
               weighted avg     0.5878    0.6025    0.5940      1039

Epoch [8/15]
top-down:TOP: Iter:   2800,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   5.6,  Val Acc: 68.58%, Val F1: 61.69% Time: 57.0249559879303 
top-down:SEC: Iter:   2800,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   5.6,  Val Acc: 53.91%, Val F1: 35.16% Time: 57.0249559879303 
top-down:CONN: Iter:   2800,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   5.6,  Val Acc: 32.36%, Val F1:  9.92% Time: 57.0249559879303 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   5.7,  Val Acc: 68.41%, Val F1: 61.53% Time: 174.25082182884216 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   5.7,  Val Acc: 54.33%, Val F1: 35.09% Time: 174.25082182884216 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 31.59%, Val F1: 10.05% Time: 174.25082182884216 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   5.7,  Val Acc: 68.84%, Val F1: 61.10% Time: 281.30350494384766 
top-down:SEC: Iter:   3000,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   5.7,  Val Acc: 53.99%, Val F1: 34.90% Time: 281.30350494384766 
top-down:CONN: Iter:   3000,  Train Loss: 2.4e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 32.96%, Val F1: 11.25% Time: 281.30350494384766 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   5.7,  Val Acc: 69.01%, Val F1: 60.82% Time: 395.2056920528412 
top-down:SEC: Iter:   3100,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   5.7,  Val Acc: 53.91%, Val F1: 33.72% Time: 395.2056920528412 
top-down:CONN: Iter:   3100,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 32.19%, Val F1:  9.86% Time: 395.2056920528412 
 
 
Train time usage: 441.2527139186859
Test time usage: 1.8125767707824707
TOP: Test Loss:   5.1,  Test Acc: 70.26%, Test F1: 62.62%
SEC: Test Loss:   5.1,  Test Acc: 59.48%, Test F1: 41.14%
CONN: Test Loss:   5.1,  Test Acc: 30.41%, Test F1: 11.56%
consistency_top_sec: 58.04%,  consistency_sec_conn: 25.60%, consistency_top_sec_conn: 24.93%
              precision    recall  f1-score   support

    Temporal     0.5870    0.3913    0.4696        69
 Contingency     0.6583    0.5788    0.6160       273
  Comparison     0.6370    0.6414    0.6392       145
   Expansion     0.7446    0.8188    0.7800       552

    accuracy                         0.7026      1039
   macro avg     0.6567    0.6076    0.6262      1039
weighted avg     0.6965    0.7026    0.6966      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6098    0.4630    0.5263        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6400    0.5970    0.6178       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6159    0.6641    0.6391       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5283    0.5600    0.5437       200
    Expansion.Instantiation     0.7458    0.7395    0.7426       119
      Expansion.Restatement     0.5685    0.6493    0.6062       211
      Expansion.Alternative     0.4667    0.7778    0.5833         9
             Expansion.List     0.2222    0.3333    0.2667        12

                   accuracy                         0.5948      1039
                  macro avg     0.3997    0.4349    0.4114      1039
               weighted avg     0.5818    0.5948    0.5864      1039

Epoch [9/15]
top-down:TOP: Iter:   3200,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   5.8,  Val Acc: 68.93%, Val F1: 61.11% Time: 62.34468340873718 
top-down:SEC: Iter:   3200,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 55.11%, Val F1: 35.85% Time: 62.34468340873718 
top-down:CONN: Iter:   3200,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   5.8,  Val Acc: 33.39%, Val F1: 10.29% Time: 62.34468340873718 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   5.8,  Val Acc: 69.10%, Val F1: 61.23% Time: 170.69328331947327 
top-down:SEC: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 55.11%, Val F1: 34.40% Time: 170.69328331947327 
top-down:CONN: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.8,  Val Acc: 33.13%, Val F1: 10.17% Time: 170.69328331947327 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 3.2e+01,  Train Acc: 100.00%,Val Loss:   5.8,  Val Acc: 68.41%, Val F1: 60.69% Time: 273.7579777240753 
top-down:SEC: Iter:   3400,  Train Loss: 3.2e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 53.91%, Val F1: 35.08% Time: 273.7579777240753 
top-down:CONN: Iter:   3400,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   5.8,  Val Acc: 32.70%, Val F1: 10.07% Time: 273.7579777240753 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   5.8,  Val Acc: 68.58%, Val F1: 61.73% Time: 379.38128542900085 
top-down:SEC: Iter:   3500,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 53.91%, Val F1: 34.65% Time: 379.38128542900085 
top-down:CONN: Iter:   3500,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   5.8,  Val Acc: 33.13%, Val F1: 10.56% Time: 379.38128542900085 
 
 
Train time usage: 417.9439392089844
Test time usage: 1.844219446182251
TOP: Test Loss:   5.1,  Test Acc: 71.70%, Test F1: 64.61%
SEC: Test Loss:   5.1,  Test Acc: 59.48%, Test F1: 39.48%
CONN: Test Loss:   5.1,  Test Acc: 32.15%, Test F1: 12.12%
consistency_top_sec: 58.42%,  consistency_sec_conn: 26.95%, consistency_top_sec_conn: 26.56%
              precision    recall  f1-score   support

    Temporal     0.5800    0.4203    0.4874        69
 Contingency     0.6707    0.6044    0.6358       273
  Comparison     0.7015    0.6483    0.6738       145
   Expansion     0.7504    0.8279    0.7873       552

    accuracy                         0.7170      1039
   macro avg     0.6757    0.6252    0.6461      1039
weighted avg     0.7113    0.7170    0.7117      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6279    0.4909    0.5510        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6475    0.6283    0.6377       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6336    0.6484    0.6409       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5183    0.5650    0.5407       200
    Expansion.Instantiation     0.7586    0.7521    0.7554       117
      Expansion.Restatement     0.5792    0.6066    0.5926       211
      Expansion.Alternative     0.3000    0.6667    0.4138         9
             Expansion.List     0.1538    0.3333    0.2105        12

                   accuracy                         0.5948      1039
                  macro avg     0.3835    0.4265    0.3948      1039
               weighted avg     0.5861    0.5948    0.5887      1039

Epoch [10/15]
top-down:TOP: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   5.9,  Val Acc: 68.76%, Val F1: 60.79% Time: 70.28778052330017 
top-down:SEC: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   5.9,  Val Acc: 54.25%, Val F1: 35.22% Time: 70.28778052330017 
top-down:CONN: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   5.9,  Val Acc: 32.19%, Val F1: 10.38% Time: 70.28778052330017 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 4e+01,  Train Acc: 90.62%,Val Loss:   5.9,  Val Acc: 69.18%, Val F1: 61.25% Time: 173.01604175567627 
top-down:SEC: Iter:   3700,  Train Loss: 4e+01,  Train Acc: 87.50%,Val Loss:   5.9,  Val Acc: 53.91%, Val F1: 35.12% Time: 173.01604175567627 
top-down:CONN: Iter:   3700,  Train Loss: 4e+01,  Train Acc: 46.88%,Val Loss:   5.9,  Val Acc: 33.05%, Val F1: 10.63% Time: 173.01604175567627 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   6.0,  Val Acc: 69.10%, Val F1: 61.34% Time: 261.66739153862 
top-down:SEC: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   6.0,  Val Acc: 53.48%, Val F1: 33.90% Time: 261.66739153862 
top-down:CONN: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   6.0,  Val Acc: 32.88%, Val F1: 10.61% Time: 261.66739153862 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.0,  Val Acc: 68.84%, Val F1: 61.05% Time: 337.6575417518616 
top-down:SEC: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 54.42%, Val F1: 35.94% Time: 337.6575417518616 
top-down:CONN: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 32.88%, Val F1: 10.66% Time: 337.6575417518616 
 
 
Train time usage: 360.13519978523254
Test time usage: 1.665419101715088
TOP: Test Loss:   5.3,  Test Acc: 71.32%, Test F1: 63.83%
SEC: Test Loss:   5.3,  Test Acc: 60.15%, Test F1: 40.39%
CONN: Test Loss:   5.3,  Test Acc: 32.63%, Test F1: 12.66%
consistency_top_sec: 58.71%,  consistency_sec_conn: 26.37%, consistency_top_sec_conn: 25.99%
              precision    recall  f1-score   support

    Temporal     0.5714    0.4058    0.4746        69
 Contingency     0.6749    0.6007    0.6357       273
  Comparison     0.6739    0.6414    0.6572       145
   Expansion     0.7488    0.8261    0.7855       552

    accuracy                         0.7132      1039
   macro avg     0.6673    0.6185    0.6383      1039
weighted avg     0.7071    0.7132    0.7076      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6250    0.5455    0.5825        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6560    0.6119    0.6332       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6288    0.6484    0.6385       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5043    0.5850    0.5417       200
    Expansion.Instantiation     0.8019    0.7203    0.7589       118
      Expansion.Restatement     0.5633    0.6540    0.6053       211
      Expansion.Alternative     0.4667    0.7778    0.5833         9
             Expansion.List     0.1250    0.0833    0.1000        12

                   accuracy                         0.6015      1039
                  macro avg     0.3974    0.4206    0.4039      1039
               weighted avg     0.5878    0.6015    0.5924      1039

Epoch [11/15]
top-down:TOP: Iter:   4000,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   6.0,  Val Acc: 69.18%, Val F1: 61.02% Time: 53.35431957244873 
top-down:SEC: Iter:   4000,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 54.25%, Val F1: 35.57% Time: 53.35431957244873 
top-down:CONN: Iter:   4000,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.0,  Val Acc: 32.27%, Val F1: 10.58% Time: 53.35431957244873 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   6.0,  Val Acc: 69.27%, Val F1: 62.22% Time: 128.75665760040283 
top-down:SEC: Iter:   4100,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   6.0,  Val Acc: 54.42%, Val F1: 34.38% Time: 128.75665760040283 
top-down:CONN: Iter:   4100,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 32.45%, Val F1: 11.38% Time: 128.75665760040283 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   6.0,  Val Acc: 69.87%, Val F1: 62.36% Time: 204.11764931678772 
top-down:SEC: Iter:   4200,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 53.39%, Val F1: 34.72% Time: 204.11764931678772 
top-down:CONN: Iter:   4200,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   6.0,  Val Acc: 33.30%, Val F1: 10.87% Time: 204.11764931678772 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 69.53%, Val F1: 62.12% Time: 284.3772566318512 
top-down:SEC: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.1,  Val Acc: 54.08%, Val F1: 34.47% Time: 284.3772566318512 
top-down:CONN: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 43.75%,Val Loss:   6.1,  Val Acc: 32.62%, Val F1: 10.81% Time: 284.3772566318512 
 
 
Train time usage: 306.09755849838257
Test time usage: 1.7657043933868408
TOP: Test Loss:   5.3,  Test Acc: 71.13%, Test F1: 63.69%
SEC: Test Loss:   5.3,  Test Acc: 59.19%, Test F1: 38.60%
CONN: Test Loss:   5.3,  Test Acc: 32.72%, Test F1: 12.38%
consistency_top_sec: 58.33%,  consistency_sec_conn: 26.18%, consistency_top_sec_conn: 25.89%
              precision    recall  f1-score   support

    Temporal     0.5870    0.3913    0.4696        69
 Contingency     0.6628    0.6337    0.6479       273
  Comparison     0.6376    0.6597    0.6485       144
   Expansion     0.7616    0.8029    0.7817       553

    accuracy                         0.7113      1039
   macro avg     0.6622    0.6219    0.6369      1039
weighted avg     0.7069    0.7113    0.7074      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6136    0.4909    0.5455        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6330    0.6283    0.6306       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5915    0.6562    0.6222       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4978    0.5600    0.5271       200
    Expansion.Instantiation     0.8269    0.7350    0.7783       117
      Expansion.Restatement     0.5856    0.6161    0.6005       211
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0769    0.0833    0.0800        12

                   accuracy                         0.5919      1039
                  macro avg     0.3798    0.4033    0.3860      1039
               weighted avg     0.5810    0.5919    0.5848      1039

Epoch [12/15]
top-down:TOP: Iter:   4400,  Train Loss: 3.2e+01,  Train Acc: 93.75%,Val Loss:   6.1,  Val Acc: 68.93%, Val F1: 62.00% Time: 75.29900407791138 
top-down:SEC: Iter:   4400,  Train Loss: 3.2e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 53.65%, Val F1: 35.63% Time: 75.29900407791138 
top-down:CONN: Iter:   4400,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   6.1,  Val Acc: 32.88%, Val F1: 10.34% Time: 75.29900407791138 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 68.67%, Val F1: 60.66% Time: 168.57748937606812 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 53.39%, Val F1: 34.67% Time: 168.57748937606812 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   6.1,  Val Acc: 32.62%, Val F1: 10.49% Time: 168.57748937606812 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 69.36%, Val F1: 61.42% Time: 259.0989775657654 
top-down:SEC: Iter:   4600,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   6.1,  Val Acc: 55.19%, Val F1: 36.58% Time: 259.0989775657654 
top-down:CONN: Iter:   4600,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   6.1,  Val Acc: 32.79%, Val F1: 10.93% Time: 259.0989775657654 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   6.1,  Val Acc: 68.67%, Val F1: 61.65% Time: 350.4499719142914 
top-down:SEC: Iter:   4700,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 54.16%, Val F1: 34.72% Time: 350.4499719142914 
top-down:CONN: Iter:   4700,  Train Loss: 2.7e+01,  Train Acc: 40.62%,Val Loss:   6.1,  Val Acc: 32.27%, Val F1: 10.91% Time: 350.4499719142914 
 
 
Train time usage: 365.1145598888397
Test time usage: 1.6903491020202637
TOP: Test Loss:   5.4,  Test Acc: 70.36%, Test F1: 62.97%
SEC: Test Loss:   5.4,  Test Acc: 58.71%, Test F1: 39.28%
CONN: Test Loss:   5.4,  Test Acc: 33.40%, Test F1: 12.75%
consistency_top_sec: 57.56%,  consistency_sec_conn: 27.33%, consistency_top_sec_conn: 27.14%
              precision    recall  f1-score   support

    Temporal     0.5185    0.4058    0.4553        69
 Contingency     0.6600    0.6044    0.6310       273
  Comparison     0.6597    0.6552    0.6574       145
   Expansion     0.7496    0.8025    0.7752       552

    accuracy                         0.7036      1039
   macro avg     0.6470    0.6170    0.6297      1039
weighted avg     0.6982    0.7036    0.6996      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5952    0.4630    0.5208        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6312    0.6171    0.6241       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6087    0.6562    0.6316       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4957    0.5750    0.5324       200
    Expansion.Instantiation     0.7748    0.7288    0.7511       118
      Expansion.Restatement     0.5924    0.5924    0.5924       211
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.1765    0.2500    0.2069        12

                   accuracy                         0.5871      1039
                  macro avg     0.3843    0.4136    0.3928      1039
               weighted avg     0.5782    0.5871    0.5809      1039

Epoch [13/15]
top-down:TOP: Iter:   4800,  Train Loss: 3.6e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 68.58%, Val F1: 60.75% Time: 79.27846240997314 
top-down:SEC: Iter:   4800,  Train Loss: 3.6e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 55.45%, Val F1: 35.99% Time: 79.27846240997314 
top-down:CONN: Iter:   4800,  Train Loss: 3.6e+01,  Train Acc: 53.12%,Val Loss:   6.1,  Val Acc: 32.88%, Val F1: 10.74% Time: 79.27846240997314 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 68.50%, Val F1: 61.27% Time: 172.53092432022095 
top-down:SEC: Iter:   4900,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   6.2,  Val Acc: 54.51%, Val F1: 35.89% Time: 172.53092432022095 
top-down:CONN: Iter:   4900,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   6.2,  Val Acc: 32.27%, Val F1: 10.63% Time: 172.53092432022095 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 68.50%, Val F1: 61.19% Time: 264.88203859329224 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 54.33%, Val F1: 35.52% Time: 264.88203859329224 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 31.93%, Val F1: 11.04% Time: 264.88203859329224 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 68.41%, Val F1: 60.87% Time: 356.0748541355133 
top-down:SEC: Iter:   5100,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 53.82%, Val F1: 35.48% Time: 356.0748541355133 
top-down:CONN: Iter:   5100,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   6.2,  Val Acc: 32.19%, Val F1: 10.82% Time: 356.0748541355133 
 
 
Train time usage: 364.47716546058655
Test time usage: 1.7024967670440674
TOP: Test Loss:   5.5,  Test Acc: 70.26%, Test F1: 62.51%
SEC: Test Loss:   5.5,  Test Acc: 58.71%, Test F1: 38.93%
CONN: Test Loss:   5.5,  Test Acc: 32.82%, Test F1: 12.48%
consistency_top_sec: 57.65%,  consistency_sec_conn: 26.76%, consistency_top_sec_conn: 26.56%
              precision    recall  f1-score   support

    Temporal     0.5417    0.3768    0.4444        69
 Contingency     0.6523    0.6095    0.6302       274
  Comparison     0.6596    0.6414    0.6503       145
   Expansion     0.7475    0.8058    0.7755       551

    accuracy                         0.7026      1039
   macro avg     0.6503    0.6084    0.6251      1039
weighted avg     0.6965    0.7026    0.6978      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.4444    0.5106        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6312    0.6171    0.6241       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6043    0.6562    0.6292       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4933    0.5500    0.5201       200
    Expansion.Instantiation     0.8131    0.7373    0.7733       118
      Expansion.Restatement     0.5746    0.6209    0.5968       211
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.1333    0.1667    0.1481        12

                   accuracy                         0.5871      1039
                  macro avg     0.3841    0.4054    0.3893      1039
               weighted avg     0.5778    0.5871    0.5806      1039

Epoch [14/15]
top-down:TOP: Iter:   5200,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 68.67%, Val F1: 61.64% Time: 85.70684957504272 
top-down:SEC: Iter:   5200,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 53.99%, Val F1: 35.54% Time: 85.70684957504272 
top-down:CONN: Iter:   5200,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 32.27%, Val F1: 10.42% Time: 85.70684957504272 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.2,  Val Acc: 68.76%, Val F1: 61.61% Time: 176.95273280143738 
top-down:SEC: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.2,  Val Acc: 54.51%, Val F1: 36.38% Time: 176.95273280143738 
top-down:CONN: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.2,  Val Acc: 33.13%, Val F1: 10.80% Time: 176.95273280143738 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 68.67%, Val F1: 61.44% Time: 269.0938069820404 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.2,  Val Acc: 54.85%, Val F1: 36.07% Time: 269.0938069820404 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   6.2,  Val Acc: 32.19%, Val F1: 10.38% Time: 269.0938069820404 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 68.76%, Val F1: 61.43% Time: 363.0517327785492 
top-down:SEC: Iter:   5500,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 54.94%, Val F1: 36.59% Time: 363.0517327785492 
top-down:CONN: Iter:   5500,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 32.36%, Val F1: 10.89% Time: 363.0517327785492 
 
 
Train time usage: 365.1417090892792
Test time usage: 1.717581033706665
TOP: Test Loss:   5.5,  Test Acc: 71.32%, Test F1: 63.48%
SEC: Test Loss:   5.5,  Test Acc: 59.29%, Test F1: 38.78%
CONN: Test Loss:   5.5,  Test Acc: 33.01%, Test F1: 12.33%
consistency_top_sec: 58.33%,  consistency_sec_conn: 26.66%, consistency_top_sec_conn: 26.47%
              precision    recall  f1-score   support

    Temporal     0.5652    0.3768    0.4522        69
 Contingency     0.6720    0.6154    0.6424       273
  Comparison     0.6643    0.6552    0.6597       145
   Expansion     0.7533    0.8188    0.7847       552

    accuracy                         0.7132      1039
   macro avg     0.6637    0.6166    0.6348      1039
weighted avg     0.7070    0.7132    0.7078      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6154    0.4444    0.5161        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6520    0.6059    0.6281       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6087    0.6562    0.6316       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5043    0.5800    0.5395       200
    Expansion.Instantiation     0.7982    0.7373    0.7665       118
      Expansion.Restatement     0.5602    0.6398    0.5973       211
      Expansion.Alternative     0.4000    0.6667    0.5000         9
             Expansion.List     0.0909    0.0833    0.0870        12

                   accuracy                         0.5929      1039
                  macro avg     0.3845    0.4012    0.3878      1039
               weighted avg     0.5818    0.5929    0.5848      1039

Epoch [15/15]
top-down:TOP: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 69.44%, Val F1: 62.13% Time: 88.81232595443726 
top-down:SEC: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 55.02%, Val F1: 36.19% Time: 88.81232595443726 
top-down:CONN: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   6.2,  Val Acc: 31.93%, Val F1: 10.45% Time: 88.81232595443726 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 69.18%, Val F1: 61.96% Time: 185.31342554092407 
top-down:SEC: Iter:   5700,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 55.02%, Val F1: 36.55% Time: 185.31342554092407 
top-down:CONN: Iter:   5700,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 31.85%, Val F1: 10.53% Time: 185.31342554092407 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 69.18%, Val F1: 61.91% Time: 277.30884408950806 
top-down:SEC: Iter:   5800,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 54.85%, Val F1: 36.30% Time: 277.30884408950806 
top-down:CONN: Iter:   5800,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   6.2,  Val Acc: 32.02%, Val F1: 10.36% Time: 277.30884408950806 
 
 
Train time usage: 363.1060321331024
Test time usage: 1.7138493061065674
TOP: Test Loss:   5.5,  Test Acc: 70.84%, Test F1: 63.13%
SEC: Test Loss:   5.5,  Test Acc: 58.90%, Test F1: 38.29%
CONN: Test Loss:   5.5,  Test Acc: 32.63%, Test F1: 12.49%
consistency_top_sec: 58.04%,  consistency_sec_conn: 26.28%, consistency_top_sec_conn: 26.28%
              precision    recall  f1-score   support

    Temporal     0.5532    0.3768    0.4483        69
 Contingency     0.6565    0.6277    0.6418       274
  Comparison     0.6552    0.6552    0.6552       145
   Expansion     0.7573    0.8040    0.7799       551

    accuracy                         0.7084      1039
   macro avg     0.6555    0.6159    0.6313      1039
weighted avg     0.7029    0.7084    0.7041      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.4444    0.5106        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6241    0.6357    0.6298       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6043    0.6562    0.6292       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5023    0.5550    0.5273       200
    Expansion.Instantiation     0.7963    0.7288    0.7611       118
      Expansion.Restatement     0.5890    0.6114    0.6000       211
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.0667    0.0833    0.0741        12

                   accuracy                         0.5890      1039
                  macro avg     0.3780    0.3983    0.3829      1039
               weighted avg     0.5780    0.5890    0.5819      1039

dev_best_acc_top: 68.67%,  dev_best_f1_top: 62.75%, 
dev_best_acc_sec: 55.71%,  dev_best_f1_sec: 36.41%, 
dev_best_acc_conn: 33.65%,  dev_best_f1_conn:  9.81%
