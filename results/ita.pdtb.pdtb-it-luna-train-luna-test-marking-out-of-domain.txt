nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_it_luna_train_luna_test_marking_out_of_domain/data/', 'log_file': 'data/pdtb_it_luna_train_luna_test_marking_out_of_domain/log/', 'save_file': 'data/pdtb_it_luna_train_luna_test_marking_out_of_domain/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March10-15:47:38', 'log': 'data/pdtb_it_luna_train_luna_test_marking_out_of_domain/log/March10-15:47:38.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]180it [00:00, 1794.53it/s]490it [00:00, 2556.06it/s]728it [00:00, 2615.12it/s]
0it [00:00, ?it/s]168it [00:00, 3030.55it/s]
0it [00:00, ?it/s]292it [00:00, 3176.60it/s]
Time usage: 9.644455432891846
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
0it [00:00, ?it/s]155it [00:00, 1548.43it/s]334it [00:00, 1686.71it/s]519it [00:00, 1758.89it/s]717it [00:00, 1843.79it/s]908it [00:00, 1861.43it/s]1095it [00:00, 1823.93it/s]1279it [00:00, 1828.62it/s]1462it [00:00, 1248.40it/s]1613it [00:01, 1309.67it/s]1796it [00:01, 1438.01it/s]1981it [00:01, 1544.65it/s]2163it [00:01, 1618.23it/s]2345it [00:01, 1672.54it/s]2527it [00:01, 1714.45it/s]2705it [00:01, 1733.36it/s]2883it [00:01, 1746.24it/s]3066it [00:01, 1768.02it/s]3245it [00:01, 1765.09it/s]3432it [00:02, 1794.59it/s]3613it [00:02, 1789.26it/s]3806it [00:02, 1829.23it/s]3990it [00:02, 1812.48it/s]4172it [00:02, 1762.47it/s]4349it [00:02, 1709.38it/s]4535it [00:02, 1751.44it/s]4711it [00:02, 1752.02it/s]4887it [00:02, 1730.37it/s]5075it [00:02, 1771.23it/s]5272it [00:03, 1829.52it/s]5456it [00:03, 1729.03it/s]5631it [00:03, 1727.87it/s]5810it [00:03, 1744.09it/s]5991it [00:03, 1761.91it/s]6168it [00:03, 1759.10it/s]6345it [00:03, 1743.18it/s]6520it [00:03, 1680.31it/s]6691it [00:03, 1686.59it/s]6861it [00:04, 1674.37it/s]7029it [00:04, 1620.87it/s]7203it [00:04, 1652.70it/s]7382it [00:04, 1692.28it/s]7555it [00:04, 1701.99it/s]7734it [00:04, 1727.92it/s]7926it [00:04, 1784.44it/s]8105it [00:04, 1759.53it/s]8282it [00:04, 1755.11it/s]8458it [00:04, 1736.62it/s]8636it [00:05, 1747.67it/s]8820it [00:05, 1774.34it/s]8998it [00:05, 1768.45it/s]9181it [00:05, 1785.37it/s]9360it [00:05, 1783.09it/s]9541it [00:05, 1770.01it/s]9727it [00:05, 1794.19it/s]9909it [00:05, 1799.46it/s]10097it [00:05, 1820.26it/s]10280it [00:05, 1781.68it/s]10472it [00:06, 1821.06it/s]10657it [00:06, 1828.25it/s]10840it [00:06, 1819.27it/s]11023it [00:06, 1792.87it/s]11205it [00:06, 1799.16it/s]11390it [00:06, 1811.07it/s]11572it [00:06, 1810.15it/s]11754it [00:06, 1807.40it/s]11935it [00:06, 1808.09it/s]12116it [00:07, 1738.96it/s]12291it [00:07, 1708.87it/s]12463it [00:07, 1699.16it/s]12547it [00:07, 1727.40it/s]
INFO: Training with out of domain data.
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.4,  Val Acc: 29.76%, Val F1: 11.47% Time: 85.36980891227722 *
top-down:SEC: Iter:    100,  Train Loss: 3e+01,  Train Acc: 18.75%,Val Loss:   6.4,  Val Acc: 33.93%, Val F1:  5.63% Time: 85.36980891227722 *
top-down:CONN: Iter:    100,  Train Loss: 3e+01,  Train Acc:  3.12%,Val Loss:   6.4,  Val Acc:  0.00%, Val F1:  0.00% Time: 85.36980891227722 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 38.69%, Val F1: 23.04% Time: 162.3980095386505 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.7,  Val Acc: 33.93%, Val F1:  5.63% Time: 162.3980095386505 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.7,  Val Acc:  0.00%, Val F1:  0.00% Time: 162.3980095386505 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 75.00%,Val Loss:   6.6,  Val Acc: 29.76%, Val F1: 12.97% Time: 237.36276841163635 
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 31.25%,Val Loss:   6.6,  Val Acc: 33.33%, Val F1:  9.41% Time: 237.36276841163635 
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 15.62%,Val Loss:   6.6,  Val Acc:  2.38%, Val F1:  1.55% Time: 237.36276841163635 
 
 
Train time usage: 308.46008372306824
Test time usage: 0.566415548324585
TOP: Test Loss:   6.7,  Test Acc: 43.15%, Test F1: 27.75%
SEC: Test Loss:   6.7,  Test Acc: 28.77%, Test F1: 11.57%
CONN: Test Loss:   6.7,  Test Acc:  7.53%, Test F1:  2.34%
consistency_top_sec:  4.81%,  consistency_sec_conn:  0.29%, consistency_top_sec_conn:  0.29%
              precision    recall  f1-score   support

    Temporal     0.5714    0.0755    0.1333        53
 Contingency     0.4091    0.3176    0.3576        85
  Comparison     0.2500    0.0233    0.0426        43
   Expansion     0.4372    0.8468    0.5767       111

    accuracy                         0.4315       292
   macro avg     0.4169    0.3158    0.2775       292
weighted avg     0.4258    0.4315    0.3538       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3333    0.0667    0.1111        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.3429    0.7059    0.4615        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.1500    0.0508    0.0759        59
      Expansion.Conjunction     0.2118    0.4000    0.2769        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.2877       292
                  macro avg     0.1297    0.1529    0.1157       292
               weighted avg     0.2141    0.2877    0.2095       292

Epoch [2/15]
top-down:TOP: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   6.5,  Val Acc: 44.64%, Val F1: 40.33% Time: 7.38869047164917 *
top-down:SEC: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   6.5,  Val Acc: 39.88%, Val F1: 18.84% Time: 7.38869047164917 *
top-down:CONN: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 12.50%,Val Loss:   6.5,  Val Acc: 12.50%, Val F1:  3.70% Time: 7.38869047164917 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   7.3,  Val Acc: 42.26%, Val F1: 28.39% Time: 82.2879786491394 
top-down:SEC: Iter:    500,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   7.3,  Val Acc: 37.50%, Val F1: 12.41% Time: 82.2879786491394 
top-down:CONN: Iter:    500,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   7.3,  Val Acc: 10.71%, Val F1:  3.87% Time: 82.2879786491394 
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   6.3,  Val Acc: 48.21%, Val F1: 41.59% Time: 159.01082849502563 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   6.3,  Val Acc: 41.67%, Val F1: 19.13% Time: 159.01082849502563 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   6.3,  Val Acc: 16.67%, Val F1:  4.76% Time: 159.01082849502563 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.3,  Val Acc: 47.02%, Val F1: 41.09% Time: 234.19780230522156 
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   6.3,  Val Acc: 39.29%, Val F1: 17.02% Time: 234.19780230522156 
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 31.25%,Val Loss:   6.3,  Val Acc: 17.86%, Val F1:  3.79% Time: 234.19780230522156 
 
 
Train time usage: 299.53830885887146
Test time usage: 0.5507206916809082
TOP: Test Loss:   6.0,  Test Acc: 52.40%, Test F1: 45.19%
SEC: Test Loss:   6.0,  Test Acc: 40.41%, Test F1: 21.16%
CONN: Test Loss:   6.0,  Test Acc: 17.81%, Test F1:  3.36%
consistency_top_sec:  8.66%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.73%
              precision    recall  f1-score   support

    Temporal     0.6087    0.2642    0.3684        53
 Contingency     0.6667    0.3765    0.4812        85
  Comparison     0.5000    0.2558    0.3385        43
   Expansion     0.4824    0.8649    0.6194       111

    accuracy                         0.5240       292
   macro avg     0.5644    0.4403    0.4519       292
weighted avg     0.5616    0.5240    0.4922       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3778    0.4304        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4365    0.6471    0.5213        85
Contingency.Pragmatic cause     0.0952    0.1818    0.1250        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3898    0.3898    0.3898        59
      Expansion.Conjunction     0.4118    0.4667    0.4375        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.4055    0.4041    0.4048       292
                  macro avg     0.2292    0.2579    0.2380       292
               weighted avg     0.3499    0.4041    0.3690       292

Epoch [3/15]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 51.79%, Val F1: 48.12% Time: 12.53307056427002 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 34.38%,Val Loss:   6.7,  Val Acc: 41.67%, Val F1: 19.44% Time: 12.53307056427002 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 21.88%,Val Loss:   6.7,  Val Acc: 11.90%, Val F1:  3.04% Time: 12.53307056427002 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   6.5,  Val Acc: 50.60%, Val F1: 44.20% Time: 87.49123692512512 
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   6.5,  Val Acc: 38.10%, Val F1: 19.23% Time: 87.49123692512512 
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 21.88%,Val Loss:   6.5,  Val Acc: 17.86%, Val F1:  3.79% Time: 87.49123692512512 
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.2,  Val Acc: 44.64%, Val F1: 40.11% Time: 162.31764006614685 
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 31.55%, Val F1: 18.19% Time: 162.31764006614685 
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.2,  Val Acc: 23.81%, Val F1:  5.49% Time: 162.31764006614685 
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   6.5,  Val Acc: 50.00%, Val F1: 47.46% Time: 237.1847825050354 
top-down:SEC: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   6.5,  Val Acc: 37.50%, Val F1: 20.00% Time: 237.1847825050354 
top-down:CONN: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   6.5,  Val Acc: 17.26%, Val F1:  3.68% Time: 237.1847825050354 
 
 
Train time usage: 297.67199063301086
Test time usage: 0.536442756652832
TOP: Test Loss:   6.2,  Test Acc: 49.32%, Test F1: 44.98%
SEC: Test Loss:   6.2,  Test Acc: 38.70%, Test F1: 21.93%
CONN: Test Loss:   6.2,  Test Acc: 21.23%, Test F1:  3.89%
consistency_top_sec:  8.28%,  consistency_sec_conn:  1.64%, consistency_top_sec_conn:  1.64%
              precision    recall  f1-score   support

    Temporal     0.5714    0.3774    0.4545        53
 Contingency     0.6154    0.1882    0.2883        85
  Comparison     0.4468    0.4884    0.4667        43
   Expansion     0.4728    0.7838    0.5898       111

    accuracy                         0.4932       292
   macro avg     0.5266    0.4594    0.4498       292
weighted avg     0.5284    0.4932    0.4594       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4872    0.4222    0.4524        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6000    0.4588    0.5200        85
Contingency.Pragmatic cause     0.1091    0.5455    0.1818        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3735    0.5254    0.4366        59
      Expansion.Conjunction     0.3673    0.4000    0.3830        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3883    0.3870    0.3877       292
                  macro avg     0.2421    0.2940    0.2467       292
               weighted avg     0.3859    0.3870    0.3752       292

Epoch [4/15]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   6.7,  Val Acc: 50.00%, Val F1: 47.37% Time: 15.987109661102295 
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   6.7,  Val Acc: 36.31%, Val F1: 20.03% Time: 15.987109661102295 
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   6.7,  Val Acc: 14.88%, Val F1:  2.36% Time: 15.987109661102295 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 71.88%,Val Loss:   6.8,  Val Acc: 48.21%, Val F1: 47.30% Time: 90.83397841453552 
top-down:SEC: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 62.50%,Val Loss:   6.8,  Val Acc: 33.93%, Val F1: 18.98% Time: 90.83397841453552 
top-down:CONN: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 46.88%,Val Loss:   6.8,  Val Acc: 16.67%, Val F1:  2.60% Time: 90.83397841453552 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.7,  Val Acc: 49.40%, Val F1: 47.79% Time: 165.6429901123047 
top-down:SEC: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   6.7,  Val Acc: 29.17%, Val F1: 15.49% Time: 165.6429901123047 
top-down:CONN: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   6.7,  Val Acc: 16.67%, Val F1:  2.60% Time: 165.6429901123047 
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   6.6,  Val Acc: 44.05%, Val F1: 39.49% Time: 240.51441287994385 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   6.6,  Val Acc: 25.00%, Val F1: 15.23% Time: 240.51441287994385 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 37.50%,Val Loss:   6.6,  Val Acc: 16.67%, Val F1:  2.86% Time: 240.51441287994385 
 
 
Train time usage: 295.51076555252075
Test time usage: 0.5727159976959229
TOP: Test Loss:   6.5,  Test Acc: 49.66%, Test F1: 44.25%
SEC: Test Loss:   6.5,  Test Acc: 36.99%, Test F1: 18.97%
CONN: Test Loss:   6.5,  Test Acc: 14.38%, Test F1:  2.10%
consistency_top_sec:  8.47%,  consistency_sec_conn:  1.44%, consistency_top_sec_conn:  1.35%
              precision    recall  f1-score   support

    Temporal     0.5000    0.2642    0.3457        53
 Contingency     0.6000    0.2471    0.3500        85
  Comparison     0.4468    0.4884    0.4667        43
   Expansion     0.4890    0.8018    0.6075       111

    accuracy                         0.4966       292
   macro avg     0.5090    0.4503    0.4425       292
weighted avg     0.5171    0.4966    0.4643       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4857    0.3778    0.4250        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5806    0.4235    0.4898        85
Contingency.Pragmatic cause     0.0943    0.4545    0.1562        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3934    0.4068    0.4000        59
      Expansion.Conjunction     0.3377    0.5778    0.4262        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3724    0.3699    0.3711       292
                  macro avg     0.2365    0.2801    0.2372       292
               weighted avg     0.3790    0.3699    0.3605       292

Epoch [5/15]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.5,  Val Acc: 50.00%, Val F1: 48.50% Time: 22.946258544921875 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   6.5,  Val Acc: 33.33%, Val F1: 20.06% Time: 22.946258544921875 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   6.5,  Val Acc: 21.43%, Val F1:  3.53% Time: 22.946258544921875 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   6.9,  Val Acc: 47.02%, Val F1: 45.84% Time: 97.78984451293945 
top-down:SEC: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   6.9,  Val Acc: 29.76%, Val F1: 17.02% Time: 97.78984451293945 
top-down:CONN: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   6.9,  Val Acc: 17.86%, Val F1:  2.75% Time: 97.78984451293945 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   7.0,  Val Acc: 49.40%, Val F1: 49.21% Time: 172.22587394714355 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   7.0,  Val Acc: 32.14%, Val F1: 17.35% Time: 172.22587394714355 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   7.0,  Val Acc: 17.86%, Val F1:  2.75% Time: 172.22587394714355 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.4,  Val Acc: 48.21%, Val F1: 45.04% Time: 246.89257407188416 
top-down:SEC: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   7.4,  Val Acc: 29.76%, Val F1: 17.44% Time: 246.89257407188416 
top-down:CONN: Iter:   1900,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   7.4,  Val Acc: 15.48%, Val F1:  2.23% Time: 246.89257407188416 
 
 
Train time usage: 296.6760141849518
Test time usage: 0.5388307571411133
TOP: Test Loss:   6.9,  Test Acc: 47.26%, Test F1: 42.72%
SEC: Test Loss:   6.9,  Test Acc: 34.93%, Test F1: 20.56%
CONN: Test Loss:   6.9,  Test Acc: 16.78%, Test F1:  2.21%
consistency_top_sec:  8.18%,  consistency_sec_conn:  1.44%, consistency_top_sec_conn:  1.35%
              precision    recall  f1-score   support

    Temporal     0.5385    0.2642    0.3544        53
 Contingency     0.6129    0.2235    0.3276        85
  Comparison     0.3418    0.6279    0.4426        43
   Expansion     0.5000    0.7027    0.5843       111

    accuracy                         0.4726       292
   macro avg     0.4983    0.4546    0.4272       292
weighted avg     0.5165    0.4726    0.4470       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4839    0.3333    0.3947        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5769    0.3529    0.4380        85
Contingency.Pragmatic cause     0.0921    0.6364    0.1609        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.4058    0.4746    0.4375        59
      Expansion.Conjunction     0.3667    0.4889    0.4190        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3505    0.3493    0.3499       292
                  macro avg     0.2407    0.2858    0.2313       292
               weighted avg     0.3845    0.3493    0.3474       292

Epoch [6/15]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 49.40%, Val F1: 48.55% Time: 26.339301347732544 
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.2,  Val Acc: 30.95%, Val F1: 19.21% Time: 26.339301347732544 
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   7.2,  Val Acc: 17.26%, Val F1:  3.27% Time: 26.339301347732544 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   7.8,  Val Acc: 46.43%, Val F1: 42.31% Time: 101.09143733978271 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   7.8,  Val Acc: 28.57%, Val F1: 15.19% Time: 101.09143733978271 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   7.8,  Val Acc: 14.88%, Val F1:  2.16% Time: 101.09143733978271 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   7.1,  Val Acc: 47.62%, Val F1: 43.98% Time: 176.06602549552917 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   7.1,  Val Acc: 30.36%, Val F1: 20.72% Time: 176.06602549552917 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   7.1,  Val Acc: 20.83%, Val F1:  3.13% Time: 176.06602549552917 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 84.38%,Val Loss:   7.4,  Val Acc: 48.21%, Val F1: 44.69% Time: 250.8963499069214 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 81.25%,Val Loss:   7.4,  Val Acc: 30.36%, Val F1: 19.84% Time: 250.8963499069214 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 37.50%,Val Loss:   7.4,  Val Acc: 17.86%, Val F1:  2.75% Time: 250.8963499069214 
 
 
Train time usage: 295.31446504592896
Test time usage: 0.541346549987793
TOP: Test Loss:   7.1,  Test Acc: 46.58%, Test F1: 42.65%
SEC: Test Loss:   7.1,  Test Acc: 34.25%, Test F1: 18.33%
CONN: Test Loss:   7.1,  Test Acc: 16.44%, Test F1:  2.17%
consistency_top_sec:  8.08%,  consistency_sec_conn:  1.25%, consistency_top_sec_conn:  1.25%
              precision    recall  f1-score   support

    Temporal     0.4839    0.2830    0.3571        53
 Contingency     0.5588    0.2235    0.3193        85
  Comparison     0.3768    0.6047    0.4643        43
   Expansion     0.4810    0.6847    0.5651       111

    accuracy                         0.4658       292
   macro avg     0.4751    0.4490    0.4265       292
weighted avg     0.4888    0.4658    0.4409       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4000    0.4444        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6383    0.3529    0.4545        85
Contingency.Pragmatic cause     0.0811    0.5455    0.1412        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3871    0.4068    0.3967        59
      Expansion.Conjunction     0.3333    0.4889    0.3964        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3460    0.3425    0.3442       292
                  macro avg     0.2425    0.2743    0.2292       292
               weighted avg     0.3955    0.3425    0.3474       292

Epoch [7/15]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 75.00%,Val Loss:   7.7,  Val Acc: 48.81%, Val F1: 45.94% Time: 31.593789100646973 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 27.38%, Val F1: 16.53% Time: 31.593789100646973 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 43.75%,Val Loss:   7.7,  Val Acc: 17.26%, Val F1:  2.68% Time: 31.593789100646973 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.7,  Val Acc: 48.81%, Val F1: 45.26% Time: 106.47895050048828 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   7.7,  Val Acc: 30.36%, Val F1: 17.99% Time: 106.47895050048828 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   7.7,  Val Acc: 15.48%, Val F1:  2.23% Time: 106.47895050048828 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   7.9,  Val Acc: 47.62%, Val F1: 45.73% Time: 181.12663173675537 
top-down:SEC: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 59.38%,Val Loss:   7.9,  Val Acc: 28.57%, Val F1: 16.41% Time: 181.12663173675537 
top-down:CONN: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 25.00%,Val Loss:   7.9,  Val Acc: 17.86%, Val F1:  2.53% Time: 181.12663173675537 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   7.5,  Val Acc: 46.43%, Val F1: 43.05% Time: 255.75845503807068 
top-down:SEC: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   7.5,  Val Acc: 26.79%, Val F1: 15.09% Time: 255.75845503807068 
top-down:CONN: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 34.38%,Val Loss:   7.5,  Val Acc: 20.24%, Val F1:  2.59% Time: 255.75845503807068 
 
 
Train time usage: 294.98455786705017
Test time usage: 0.5388200283050537
TOP: Test Loss:   7.3,  Test Acc: 50.68%, Test F1: 46.95%
SEC: Test Loss:   7.3,  Test Acc: 36.99%, Test F1: 19.05%
CONN: Test Loss:   7.3,  Test Acc: 16.10%, Test F1:  1.73%
consistency_top_sec:  9.62%,  consistency_sec_conn:  1.44%, consistency_top_sec_conn:  1.35%
              precision    recall  f1-score   support

    Temporal     0.4815    0.2453    0.3250        53
 Contingency     0.5455    0.3529    0.4286        85
  Comparison     0.5106    0.5581    0.5333        43
   Expansion     0.4969    0.7297    0.5912       111

    accuracy                         0.5068       292
   macro avg     0.5086    0.4715    0.4695       292
weighted avg     0.5103    0.5068    0.4870       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5333    0.3556    0.4267        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5077    0.3882    0.4400        85
Contingency.Pragmatic cause     0.0962    0.4545    0.1587        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.4054    0.5085    0.4511        59
      Expansion.Conjunction     0.3582    0.5333    0.4286        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3737    0.3699    0.3718       292
                  macro avg     0.2376    0.2800    0.2381       292
               weighted avg     0.3707    0.3699    0.3570       292

Epoch [8/15]
top-down:TOP: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 78.12%,Val Loss:   7.6,  Val Acc: 47.62%, Val F1: 44.79% Time: 36.6706166267395 
top-down:SEC: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   7.6,  Val Acc: 29.17%, Val F1: 16.15% Time: 36.6706166267395 
top-down:CONN: Iter:   2800,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   7.6,  Val Acc: 20.24%, Val F1:  2.81% Time: 36.6706166267395 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   7.7,  Val Acc: 44.64%, Val F1: 41.66% Time: 111.41436338424683 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   7.7,  Val Acc: 27.98%, Val F1: 15.73% Time: 111.41436338424683 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   7.7,  Val Acc: 22.02%, Val F1:  2.78% Time: 111.41436338424683 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 48.81%, Val F1: 45.40% Time: 186.17834186553955 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   7.9,  Val Acc: 30.95%, Val F1: 16.78% Time: 186.17834186553955 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   7.9,  Val Acc: 19.05%, Val F1:  2.67% Time: 186.17834186553955 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 47.62%, Val F1: 46.05% Time: 260.9070098400116 
top-down:SEC: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 30.36%, Val F1: 17.13% Time: 260.9070098400116 
top-down:CONN: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   7.7,  Val Acc: 20.24%, Val F1:  2.81% Time: 260.9070098400116 
 
 
Train time usage: 294.9173905849457
Test time usage: 0.5703825950622559
TOP: Test Loss:   7.2,  Test Acc: 50.34%, Test F1: 45.30%
SEC: Test Loss:   7.2,  Test Acc: 35.62%, Test F1: 18.59%
CONN: Test Loss:   7.2,  Test Acc: 21.23%, Test F1:  2.19%
consistency_top_sec:  9.24%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.83%
              precision    recall  f1-score   support

    Temporal     0.5185    0.2642    0.3500        53
 Contingency     0.6774    0.2471    0.3621        85
  Comparison     0.4889    0.5116    0.5000        43
   Expansion     0.4762    0.8108    0.6000       111

    accuracy                         0.5034       292
   macro avg     0.5403    0.4584    0.4530       292
weighted avg     0.5443    0.5034    0.4706       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3556    0.4156        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6250    0.2941    0.4000        85
Contingency.Pragmatic cause     0.1091    0.5455    0.1818        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3656    0.5763    0.4474        59
      Expansion.Conjunction     0.3485    0.5111    0.4144        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3599    0.3562    0.3580       292
                  macro avg     0.2435    0.2853    0.2324       292
               weighted avg     0.3907    0.3562    0.3416       292

Epoch [9/15]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 45.83%, Val F1: 42.96% Time: 41.89052891731262 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 30.36%, Val F1: 18.17% Time: 41.89052891731262 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   7.7,  Val Acc: 20.24%, Val F1:  3.06% Time: 41.89052891731262 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.6,  Val Acc: 45.83%, Val F1: 44.05% Time: 116.56534123420715 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   7.6,  Val Acc: 30.36%, Val F1: 16.93% Time: 116.56534123420715 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   7.6,  Val Acc: 22.02%, Val F1:  3.28% Time: 116.56534123420715 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 78.12%,Val Loss:   7.9,  Val Acc: 50.60%, Val F1: 48.04% Time: 191.54312801361084 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 33.33%, Val F1: 20.06% Time: 191.54312801361084 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   7.9,  Val Acc: 17.26%, Val F1:  2.68% Time: 191.54312801361084 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   8.2,  Val Acc: 47.02%, Val F1: 43.70% Time: 266.4251182079315 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   8.2,  Val Acc: 29.76%, Val F1: 16.66% Time: 266.4251182079315 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 50.00%,Val Loss:   8.2,  Val Acc: 17.86%, Val F1:  2.53% Time: 266.4251182079315 
 
 
Train time usage: 295.06756615638733
Test time usage: 0.5417897701263428
TOP: Test Loss:   8.0,  Test Acc: 49.66%, Test F1: 45.37%
SEC: Test Loss:   8.0,  Test Acc: 33.56%, Test F1: 17.60%
CONN: Test Loss:   8.0,  Test Acc: 16.44%, Test F1:  1.57%
consistency_top_sec:  8.76%,  consistency_sec_conn:  1.44%, consistency_top_sec_conn:  1.44%
              precision    recall  f1-score   support

    Temporal     0.5385    0.2642    0.3544        53
 Contingency     0.5854    0.2824    0.3810        85
  Comparison     0.4000    0.6047    0.4815        43
   Expansion     0.5062    0.7297    0.5978       111

    accuracy                         0.4966       292
   macro avg     0.5075    0.4702    0.4537       292
weighted avg     0.5195    0.4966    0.4734       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5357    0.3333    0.4110        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5918    0.3412    0.4328        85
Contingency.Pragmatic cause     0.0606    0.3636    0.1039        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3750    0.5085    0.4317        59
      Expansion.Conjunction     0.3333    0.4444    0.3810        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3403    0.3356    0.3379       292
                  macro avg     0.2371    0.2489    0.2200       292
               weighted avg     0.3843    0.3356    0.3392       292

Epoch [10/15]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 48.21%, Val F1: 44.99% Time: 47.19347643852234 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 81.25%,Val Loss:   8.5,  Val Acc: 32.14%, Val F1: 17.92% Time: 47.19347643852234 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   8.5,  Val Acc: 16.67%, Val F1:  2.38% Time: 47.19347643852234 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   8.3,  Val Acc: 45.83%, Val F1: 43.82% Time: 121.91719031333923 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   8.3,  Val Acc: 26.79%, Val F1: 15.17% Time: 121.91719031333923 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   8.3,  Val Acc: 20.24%, Val F1:  2.81% Time: 121.91719031333923 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   8.3,  Val Acc: 51.19%, Val F1: 50.12% Time: 196.69019770622253 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   8.3,  Val Acc: 30.36%, Val F1: 18.98% Time: 196.69019770622253 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   8.3,  Val Acc: 18.45%, Val F1:  2.83% Time: 196.69019770622253 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   8.2,  Val Acc: 49.40%, Val F1: 44.98% Time: 271.5851626396179 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   8.2,  Val Acc: 29.17%, Val F1: 16.53% Time: 271.5851626396179 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   8.2,  Val Acc: 20.24%, Val F1:  3.06% Time: 271.5851626396179 
 
 
Train time usage: 295.0781841278076
Test time usage: 0.5390205383300781
TOP: Test Loss:   7.5,  Test Acc: 50.68%, Test F1: 46.32%
SEC: Test Loss:   7.5,  Test Acc: 33.90%, Test F1: 17.82%
CONN: Test Loss:   7.5,  Test Acc: 22.60%, Test F1:  2.17%
consistency_top_sec:  9.05%,  consistency_sec_conn:  1.92%, consistency_top_sec_conn:  1.92%
              precision    recall  f1-score   support

    Temporal     0.5600    0.2642    0.3590        53
 Contingency     0.5500    0.2588    0.3520        85
  Comparison     0.4655    0.6279    0.5347        43
   Expansion     0.5030    0.7658    0.6071       111

    accuracy                         0.5068       292
   macro avg     0.5196    0.4792    0.4632       292
weighted avg     0.5215    0.5068    0.4772       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5185    0.3111    0.3889        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5435    0.2941    0.3817        85
Contingency.Pragmatic cause     0.0968    0.5455    0.1644        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3667    0.5593    0.4430        59
      Expansion.Conjunction     0.3559    0.4667    0.4038        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3449    0.3390    0.3420       292
                  macro avg     0.2352    0.2721    0.2227       292
               weighted avg     0.3707    0.3390    0.3290       292

Epoch [11/15]
top-down:TOP: Iter:   4000,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   8.5,  Val Acc: 49.40%, Val F1: 47.86% Time: 52.4228093624115 
top-down:SEC: Iter:   4000,  Train Loss: 3.3e+01,  Train Acc: 68.75%,Val Loss:   8.5,  Val Acc: 28.57%, Val F1: 16.16% Time: 52.4228093624115 
top-down:CONN: Iter:   4000,  Train Loss: 3.3e+01,  Train Acc: 37.50%,Val Loss:   8.5,  Val Acc: 19.64%, Val F1:  2.53% Time: 52.4228093624115 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   8.4,  Val Acc: 45.83%, Val F1: 43.41% Time: 127.20718598365784 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   8.4,  Val Acc: 29.17%, Val F1: 16.50% Time: 127.20718598365784 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   8.4,  Val Acc: 20.24%, Val F1:  2.24% Time: 127.20718598365784 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   8.4,  Val Acc: 46.43%, Val F1: 44.28% Time: 202.28075528144836 
top-down:SEC: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   8.4,  Val Acc: 29.76%, Val F1: 16.59% Time: 202.28075528144836 
top-down:CONN: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   8.4,  Val Acc: 20.83%, Val F1:  2.65% Time: 202.28075528144836 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 49.40%, Val F1: 48.30% Time: 276.8481328487396 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   8.6,  Val Acc: 29.17%, Val F1: 16.25% Time: 276.8481328487396 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   8.6,  Val Acc: 19.64%, Val F1:  2.53% Time: 276.8481328487396 
 
 
Train time usage: 295.01178765296936
Test time usage: 0.5345268249511719
TOP: Test Loss:   8.0,  Test Acc: 51.03%, Test F1: 47.46%
SEC: Test Loss:   8.0,  Test Acc: 35.62%, Test F1: 18.80%
CONN: Test Loss:   8.0,  Test Acc: 18.49%, Test F1:  1.84%
consistency_top_sec:  9.43%,  consistency_sec_conn:  1.54%, consistency_top_sec_conn:  1.54%
              precision    recall  f1-score   support

    Temporal     0.5862    0.3208    0.4146        53
 Contingency     0.5610    0.2706    0.3651        85
  Comparison     0.4483    0.6047    0.5149        43
   Expansion     0.5061    0.7477    0.6036       111

    accuracy                         0.5103       292
   macro avg     0.5254    0.4859    0.4746       292
weighted avg     0.5281    0.5103    0.4868       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5312    0.3778    0.4416        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5625    0.3176    0.4060        85
Contingency.Pragmatic cause     0.1017    0.5455    0.1714        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3678    0.5424    0.4384        59
      Expansion.Conjunction     0.3729    0.4889    0.4231        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3599    0.3562    0.3580       292
                  macro avg     0.2420    0.2840    0.2351       292
               weighted avg     0.3812    0.3562    0.3465       292

Epoch [12/15]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   8.6,  Val Acc: 49.40%, Val F1: 47.84% Time: 57.601322412490845 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   8.6,  Val Acc: 31.55%, Val F1: 17.49% Time: 57.601322412490845 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   8.6,  Val Acc: 18.45%, Val F1:  2.40% Time: 57.601322412490845 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   8.5,  Val Acc: 49.40%, Val F1: 48.30% Time: 132.14665818214417 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   8.5,  Val Acc: 30.95%, Val F1: 17.12% Time: 132.14665818214417 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   8.5,  Val Acc: 17.86%, Val F1:  2.33% Time: 132.14665818214417 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   8.6,  Val Acc: 48.21%, Val F1: 46.61% Time: 206.76371479034424 
top-down:SEC: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   8.6,  Val Acc: 29.17%, Val F1: 16.31% Time: 206.76371479034424 
top-down:CONN: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   8.6,  Val Acc: 20.83%, Val F1:  2.46% Time: 206.76371479034424 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   8.7,  Val Acc: 47.02%, Val F1: 43.98% Time: 281.4323434829712 
top-down:SEC: Iter:   4700,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   8.7,  Val Acc: 28.57%, Val F1: 16.09% Time: 281.4323434829712 
top-down:CONN: Iter:   4700,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   8.7,  Val Acc: 20.24%, Val F1:  2.40% Time: 281.4323434829712 
 
 
Train time usage: 294.37618017196655
Test time usage: 0.564680814743042
TOP: Test Loss:   8.1,  Test Acc: 51.71%, Test F1: 47.86%
SEC: Test Loss:   8.1,  Test Acc: 34.59%, Test F1: 18.12%
CONN: Test Loss:   8.1,  Test Acc: 19.18%, Test F1:  2.01%
consistency_top_sec:  9.14%,  consistency_sec_conn:  1.54%, consistency_top_sec_conn:  1.54%
              precision    recall  f1-score   support

    Temporal     0.5769    0.2830    0.3797        53
 Contingency     0.5854    0.2824    0.3810        85
  Comparison     0.5208    0.5814    0.5495        43
   Expansion     0.4915    0.7838    0.6042       111

    accuracy                         0.5171       292
   macro avg     0.5437    0.4826    0.4786       292
weighted avg     0.5387    0.5171    0.4904       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3333    0.4000        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5909    0.3059    0.4031        85
Contingency.Pragmatic cause     0.1091    0.5455    0.1818        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3733    0.4746    0.4179        59
      Expansion.Conjunction     0.3171    0.5778    0.4094        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3495    0.3459    0.3477       292
                  macro avg     0.2363    0.2796    0.2265       292
               weighted avg     0.3775    0.3459    0.3334       292

Epoch [13/15]
top-down:TOP: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.6,  Val Acc: 47.02%, Val F1: 44.88% Time: 62.813482999801636 
top-down:SEC: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.6,  Val Acc: 29.76%, Val F1: 16.61% Time: 62.813482999801636 
top-down:CONN: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   8.6,  Val Acc: 21.43%, Val F1:  2.52% Time: 62.813482999801636 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.8,  Val Acc: 48.81%, Val F1: 47.88% Time: 137.60277676582336 
top-down:SEC: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   8.8,  Val Acc: 30.36%, Val F1: 17.17% Time: 137.60277676582336 
top-down:CONN: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   8.8,  Val Acc: 19.05%, Val F1:  2.29% Time: 137.60277676582336 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.7,  Val Acc: 47.62%, Val F1: 45.33% Time: 212.34097242355347 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   8.7,  Val Acc: 29.76%, Val F1: 16.60% Time: 212.34097242355347 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   8.7,  Val Acc: 19.05%, Val F1:  2.46% Time: 212.34097242355347 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.6,  Val Acc: 49.40%, Val F1: 47.00% Time: 287.00863337516785 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   8.6,  Val Acc: 31.55%, Val F1: 17.45% Time: 287.00863337516785 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   8.6,  Val Acc: 19.05%, Val F1:  2.67% Time: 287.00863337516785 
 
 
Train time usage: 294.79994535446167
Test time usage: 0.5660073757171631
TOP: Test Loss:   8.0,  Test Acc: 52.05%, Test F1: 48.36%
SEC: Test Loss:   8.0,  Test Acc: 36.30%, Test F1: 18.86%
CONN: Test Loss:   8.0,  Test Acc: 18.84%, Test F1:  1.98%
consistency_top_sec:  9.53%,  consistency_sec_conn:  1.83%, consistency_top_sec_conn:  1.73%
              precision    recall  f1-score   support

    Temporal     0.5517    0.3019    0.3902        53
 Contingency     0.5814    0.2941    0.3906        85
  Comparison     0.5102    0.5814    0.5435        43
   Expansion     0.5029    0.7748    0.6099       111

    accuracy                         0.5205       292
   macro avg     0.5366    0.4880    0.4836       292
weighted avg     0.5357    0.5205    0.4964       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5517    0.3556    0.4324        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5714    0.3294    0.4179        85
Contingency.Pragmatic cause     0.0943    0.4545    0.1562        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3667    0.5593    0.4430        59
      Expansion.Conjunction     0.3692    0.5333    0.4364        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3668    0.3630    0.3649       292
                  macro avg     0.2442    0.2790    0.2357       292
               weighted avg     0.3859    0.3630    0.3509       292

Epoch [14/15]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   8.7,  Val Acc: 49.40%, Val F1: 48.55% Time: 68.32689619064331 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   8.7,  Val Acc: 30.95%, Val F1: 17.35% Time: 68.32689619064331 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   8.7,  Val Acc: 19.05%, Val F1:  2.46% Time: 68.32689619064331 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.7,  Val Acc: 48.21%, Val F1: 46.73% Time: 143.04626750946045 
top-down:SEC: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   8.7,  Val Acc: 29.17%, Val F1: 16.37% Time: 143.04626750946045 
top-down:CONN: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   8.7,  Val Acc: 20.83%, Val F1:  2.65% Time: 143.04626750946045 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   8.9,  Val Acc: 47.62%, Val F1: 46.84% Time: 217.82887864112854 
top-down:SEC: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   8.9,  Val Acc: 30.36%, Val F1: 17.24% Time: 217.82887864112854 
top-down:CONN: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   8.9,  Val Acc: 19.64%, Val F1:  2.35% Time: 217.82887864112854 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 93.75%,Val Loss:   8.9,  Val Acc: 48.81%, Val F1: 47.97% Time: 292.79572916030884 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 90.62%,Val Loss:   8.9,  Val Acc: 29.76%, Val F1: 16.48% Time: 292.79572916030884 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 43.75%,Val Loss:   8.9,  Val Acc: 19.64%, Val F1:  2.53% Time: 292.79572916030884 
 
 
Train time usage: 295.36635184288025
Test time usage: 0.5768558979034424
TOP: Test Loss:   8.3,  Test Acc: 53.08%, Test F1: 49.99%
SEC: Test Loss:   8.3,  Test Acc: 35.62%, Test F1: 18.58%
CONN: Test Loss:   8.3,  Test Acc: 17.12%, Test F1:  1.72%
consistency_top_sec:  9.62%,  consistency_sec_conn:  1.64%, consistency_top_sec_conn:  1.54%
              precision    recall  f1-score   support

    Temporal     0.5806    0.3396    0.4286        53
 Contingency     0.5952    0.2941    0.3937        85
  Comparison     0.5306    0.6047    0.5652        43
   Expansion     0.5059    0.7748    0.6121       111

    accuracy                         0.5308       292
   macro avg     0.5531    0.5033    0.4999       292
weighted avg     0.5491    0.5308    0.5083       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5152    0.3778    0.4359        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5870    0.3176    0.4122        85
Contingency.Pragmatic cause     0.0926    0.4545    0.1538        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3678    0.5424    0.4384        59
      Expansion.Conjunction     0.3538    0.5111    0.4182        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3599    0.3562    0.3580       292
                  macro avg     0.2395    0.2754    0.2323       292
               weighted avg     0.3826    0.3562    0.3460       292

Epoch [15/15]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   8.8,  Val Acc: 47.62%, Val F1: 46.84% Time: 73.28979277610779 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   8.8,  Val Acc: 29.76%, Val F1: 16.63% Time: 73.28979277610779 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   8.8,  Val Acc: 20.24%, Val F1:  2.59% Time: 73.28979277610779 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.8,  Val Acc: 47.62%, Val F1: 46.84% Time: 148.03119778633118 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   8.8,  Val Acc: 29.17%, Val F1: 16.25% Time: 148.03119778633118 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   8.8,  Val Acc: 20.83%, Val F1:  2.46% Time: 148.03119778633118 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   8.8,  Val Acc: 47.62%, Val F1: 46.81% Time: 222.83294939994812 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 81.25%,Val Loss:   8.8,  Val Acc: 30.36%, Val F1: 17.24% Time: 222.83294939994812 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 62.50%,Val Loss:   8.8,  Val Acc: 20.24%, Val F1:  2.40% Time: 222.83294939994812 
 
 
Train time usage: 294.67423915863037
Test time usage: 0.5505740642547607
TOP: Test Loss:   8.2,  Test Acc: 52.74%, Test F1: 49.64%
SEC: Test Loss:   8.2,  Test Acc: 36.99%, Test F1: 19.36%
CONN: Test Loss:   8.2,  Test Acc: 17.12%, Test F1:  1.83%
consistency_top_sec:  9.72%,  consistency_sec_conn:  1.54%, consistency_top_sec_conn:  1.44%
              precision    recall  f1-score   support

    Temporal     0.5806    0.3396    0.4286        53
 Contingency     0.6000    0.2824    0.3840        85
  Comparison     0.5306    0.6047    0.5652        43
   Expansion     0.5000    0.7748    0.6078       111

    accuracy                         0.5274       292
   macro avg     0.5528    0.5004    0.4964       292
weighted avg     0.5483    0.5274    0.5038       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5143    0.4000    0.4500        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5870    0.3176    0.4122        85
Contingency.Pragmatic cause     0.1132    0.5455    0.1875        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.3778    0.5763    0.4564        59
      Expansion.Conjunction     0.3710    0.5111    0.4299        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.3737    0.3699    0.3718       292
                  macro avg     0.2454    0.2938    0.2420       292
               weighted avg     0.3879    0.3699    0.3549       292

dev_best_acc_top: 50.00%,  dev_best_f1_top: 48.50%, 
dev_best_acc_sec: 33.33%,  dev_best_f1_sec: 20.06%, 
dev_best_acc_conn: 21.43%,  dev_best_f1_conn:  3.53%
Epoch [1/15]
Train time usage: 19.135051250457764
Test time usage: 0.5562095642089844
TOP: Test Loss:   2.4,  Test Acc: 63.36%, Test F1: 65.58%
SEC: Test Loss:   2.4,  Test Acc: 53.42%, Test F1: 31.59%
CONN: Test Loss:   2.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.19%,  consistency_sec_conn: 15.01%, consistency_top_sec_conn: 13.19%
              precision    recall  f1-score   support

    Temporal     0.5079    0.6038    0.5517        53
 Contingency     0.6562    0.7412    0.6961        85
  Comparison     0.9143    0.7442    0.8205        43
   Expansion     0.5918    0.5225    0.5550       111

    accuracy                         0.6336       292
   macro avg     0.6676    0.6529    0.6558       292
weighted avg     0.6428    0.6336    0.6346       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4474    0.7556    0.5620        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.5772    0.8353    0.6827        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5800    0.9062    0.7073        32
      Comparison.Concession     0.6957    0.2712    0.3902        59
      Expansion.Conjunction     0.3000    0.1333    0.1846        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5342       292
                  macro avg     0.3250    0.3627    0.3159       292
               weighted avg     0.4873    0.5342    0.4702       292

Epoch [2/15]
Train time usage: 19.117746114730835
Test time usage: 0.5557281970977783
TOP: Test Loss:   2.1,  Test Acc: 69.18%, Test F1: 69.96%
SEC: Test Loss:   2.1,  Test Acc: 57.88%, Test F1: 32.64%
CONN: Test Loss:   2.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.07%,  consistency_sec_conn: 16.27%, consistency_top_sec_conn: 16.07%
              precision    recall  f1-score   support

    Temporal     0.5806    0.6792    0.6261        53
 Contingency     0.7529    0.7529    0.7529        85
  Comparison     0.8000    0.7442    0.7711        43
   Expansion     0.6667    0.6306    0.6481       111

    accuracy                         0.6918       292
   macro avg     0.7001    0.7018    0.6996       292
weighted avg     0.6958    0.6918    0.6928       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4576    0.6000    0.5192        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7253    0.7765    0.7500        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6190    0.8125    0.7027        32
      Comparison.Concession     0.4925    0.5593    0.5238        59
      Expansion.Conjunction     0.5312    0.3778    0.4416        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.5808    0.5788    0.5798       292
                  macro avg     0.3532    0.3908    0.3672       292
               weighted avg     0.5309    0.5788    0.5492       292

Epoch [3/15]
Train time usage: 19.038705825805664
Test time usage: 0.5802028179168701
TOP: Test Loss:   2.1,  Test Acc: 70.55%, Test F1: 70.62%
SEC: Test Loss:   2.1,  Test Acc: 59.25%, Test F1: 37.93%
CONN: Test Loss:   2.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.46%,  consistency_sec_conn: 16.65%, consistency_top_sec_conn: 16.46%
              precision    recall  f1-score   support

    Temporal     0.5833    0.6604    0.6195        53
 Contingency     0.7528    0.7882    0.7701        85
  Comparison     0.8529    0.6744    0.7532        43
   Expansion     0.6881    0.6757    0.6818       111

    accuracy                         0.7055       292
   macro avg     0.7193    0.6997    0.7062       292
weighted avg     0.7122    0.7055    0.7067       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4516    0.6222    0.5234        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7556    0.8000    0.7771        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.7059    0.7500    0.7273        32
      Comparison.Concession     0.6122    0.5085    0.5556        59
      Expansion.Conjunction     0.4035    0.5111    0.4510        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5925       292
                  macro avg     0.3661    0.3990    0.3793       292
               weighted avg     0.5528    0.5925    0.5683       292

Epoch [4/15]
Train time usage: 18.988038301467896
Test time usage: 0.5476064682006836
TOP: Test Loss:   2.9,  Test Acc: 68.84%, Test F1: 69.25%
SEC: Test Loss:   2.9,  Test Acc: 59.93%, Test F1: 42.30%
CONN: Test Loss:   2.9,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.46%,  consistency_sec_conn: 16.84%, consistency_top_sec_conn: 16.46%
              precision    recall  f1-score   support

    Temporal     0.5181    0.8113    0.6324        53
 Contingency     0.7640    0.8000    0.7816        85
  Comparison     0.7273    0.7442    0.7356        43
   Expansion     0.7632    0.5225    0.6203       111

    accuracy                         0.6884       292
   macro avg     0.6931    0.7195    0.6925       292
weighted avg     0.7136    0.6884    0.6864       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4487    0.7778    0.5691        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7312    0.8000    0.7640        85
Contingency.Pragmatic cause     0.1429    0.0909    0.1111        11
        Comparison.Contrast     0.6000    0.6562    0.6269        32
      Comparison.Concession     0.6977    0.5085    0.5882        59
      Expansion.Conjunction     0.5429    0.4222    0.4750        45
    Expansion.Instantiation     1.0000    0.1429    0.2500         7

                   accuracy                         0.5993       292
                  macro avg     0.5204    0.4248    0.4230       292
               weighted avg     0.6017    0.5993    0.5810       292

Epoch [5/15]
top-down:TOP: Iter:    100,  Train Loss: 5.3e+01,  Train Acc: 100.00%,Val Loss:   2.1,  Val Acc: 71.43%, Val F1: 70.36% Time: 8.061107397079468 *
top-down:SEC: Iter:    100,  Train Loss: 5.3e+01,  Train Acc: 90.62%,Val Loss:   2.1,  Val Acc: 61.31%, Val F1: 47.92% Time: 8.061107397079468 *
top-down:CONN: Iter:    100,  Train Loss: 5.3e+01,  Train Acc: 100.00%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 8.061107397079468 *
 
 
Train time usage: 21.16319727897644
Test time usage: 0.5415589809417725
TOP: Test Loss:   2.9,  Test Acc: 67.47%, Test F1: 68.51%
SEC: Test Loss:   2.9,  Test Acc: 60.62%, Test F1: 44.83%
CONN: Test Loss:   2.9,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.46%,  consistency_sec_conn: 17.04%, consistency_top_sec_conn: 16.46%
              precision    recall  f1-score   support

    Temporal     0.4944    0.8302    0.6197        53
 Contingency     0.7647    0.7647    0.7647        85
  Comparison     0.7949    0.7209    0.7561        43
   Expansion     0.7215    0.5135    0.6000       111

    accuracy                         0.6747       292
   macro avg     0.6939    0.7073    0.6851       292
weighted avg     0.7037    0.6747    0.6745       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4268    0.7778    0.5512        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7711    0.7529    0.7619        85
Contingency.Pragmatic cause     0.1429    0.0909    0.1111        11
        Comparison.Contrast     0.6970    0.7188    0.7077        32
      Comparison.Concession     0.6750    0.4576    0.5455        59
      Expansion.Conjunction     0.5385    0.4667    0.5000        45
    Expansion.Instantiation     0.8571    0.8571    0.8571         7

                  micro avg     0.6082    0.6062    0.6072       292
                  macro avg     0.5135    0.5152    0.5043       292
               weighted avg     0.6119    0.6062    0.5963       292

Epoch [6/15]
Train time usage: 19.072757959365845
Test time usage: 0.5450201034545898
TOP: Test Loss:   2.9,  Test Acc: 69.52%, Test F1: 69.96%
SEC: Test Loss:   2.9,  Test Acc: 61.30%, Test F1: 44.45%
CONN: Test Loss:   2.9,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.75%,  consistency_sec_conn: 17.23%, consistency_top_sec_conn: 16.75%
              precision    recall  f1-score   support

    Temporal     0.5625    0.6792    0.6154        53
 Contingency     0.7619    0.7529    0.7574        85
  Comparison     0.7805    0.7442    0.7619        43
   Expansion     0.6893    0.6396    0.6636       111

    accuracy                         0.6952       292
   macro avg     0.6986    0.7040    0.6996       292
weighted avg     0.7009    0.6952    0.6966       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4333    0.5778    0.4952        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7927    0.7647    0.7784        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6250    0.7812    0.6944        32
      Comparison.Concession     0.6038    0.5424    0.5714        59
      Expansion.Conjunction     0.5217    0.5333    0.5275        45
    Expansion.Instantiation     0.8750    1.0000    0.9333         7

                  micro avg     0.6172    0.6130    0.6151       292
                  macro avg     0.4814    0.5249    0.5000       292
               weighted avg     0.5894    0.6130    0.5981       292

Epoch [7/15]
Train time usage: 19.041659116744995
Test time usage: 0.5714082717895508
TOP: Test Loss:   3.0,  Test Acc: 69.86%, Test F1: 69.90%
SEC: Test Loss:   3.0,  Test Acc: 60.96%, Test F1: 43.80%
CONN: Test Loss:   3.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.84%,  consistency_sec_conn: 17.13%, consistency_top_sec_conn: 16.84%
              precision    recall  f1-score   support

    Temporal     0.5667    0.6415    0.6018        53
 Contingency     0.7419    0.8118    0.7753        85
  Comparison     0.7500    0.7674    0.7586        43
   Expansion     0.7158    0.6126    0.6602       111

    accuracy                         0.6986       292
   macro avg     0.6936    0.7083    0.6990       292
weighted avg     0.7014    0.6986    0.6976       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4107    0.5111    0.4554        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7500    0.8118    0.7797        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6047    0.8125    0.6933        32
      Comparison.Concession     0.6327    0.5254    0.5741        59
      Expansion.Conjunction     0.5227    0.5111    0.5169        45
    Expansion.Instantiation     1.0000    0.8571    0.9231         7

                  micro avg     0.6117    0.6096    0.6106       292
                  macro avg     0.4901    0.5036    0.4928       292
               weighted avg     0.5802    0.6096    0.5909       292

Epoch [8/15]
Train time usage: 19.12933111190796
Test time usage: 0.5606653690338135
TOP: Test Loss:   3.1,  Test Acc: 71.23%, Test F1: 72.35%
SEC: Test Loss:   3.1,  Test Acc: 61.99%, Test F1: 51.07%
CONN: Test Loss:   3.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 17.23%,  consistency_sec_conn: 17.42%, consistency_top_sec_conn: 17.23%
              precision    recall  f1-score   support

    Temporal     0.5455    0.7925    0.6462        53
 Contingency     0.8077    0.7412    0.7730        85
  Comparison     0.8462    0.7674    0.8049        43
   Expansion     0.7143    0.6306    0.6699       111

    accuracy                         0.7123       292
   macro avg     0.7284    0.7329    0.7235       292
weighted avg     0.7303    0.7123    0.7155       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4478    0.6667    0.5357        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.8182    0.7412    0.7778        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.6579    0.7812    0.7143        32
      Comparison.Concession     0.6522    0.5085    0.5714        59
      Expansion.Conjunction     0.5306    0.5778    0.5532        45
    Expansion.Instantiation     0.8750    1.0000    0.9333         7

                   accuracy                         0.6199       292
                  macro avg     0.4977    0.5344    0.5107       292
               weighted avg     0.6138    0.6199    0.6103       292

Epoch [9/15]
top-down:TOP: Iter:    200,  Train Loss: 5.1e+01,  Train Acc: 100.00%,Val Loss:   2.8,  Val Acc: 72.02%, Val F1: 71.41% Time: 14.209894180297852 *
top-down:SEC: Iter:    200,  Train Loss: 5.1e+01,  Train Acc: 93.75%,Val Loss:   2.8,  Val Acc: 58.33%, Val F1: 51.22% Time: 14.209894180297852 *
top-down:CONN: Iter:    200,  Train Loss: 5.1e+01,  Train Acc: 100.00%,Val Loss:   2.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 14.209894180297852 *
 
 
Train time usage: 21.01803421974182
Test time usage: 0.5416126251220703
TOP: Test Loss:   3.0,  Test Acc: 71.92%, Test F1: 72.39%
SEC: Test Loss:   3.0,  Test Acc: 60.96%, Test F1: 55.51%
CONN: Test Loss:   3.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.94%,  consistency_sec_conn: 17.13%, consistency_top_sec_conn: 16.94%
              precision    recall  f1-score   support

    Temporal     0.5538    0.6792    0.6102        53
 Contingency     0.7614    0.7882    0.7746        85
  Comparison     0.8889    0.7442    0.8101        43
   Expansion     0.7282    0.6757    0.7009       111

    accuracy                         0.7192       292
   macro avg     0.7331    0.7218    0.7239       292
weighted avg     0.7299    0.7192    0.7220       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4364    0.5333    0.4800        45
         Temporal.Synchrony     0.4000    0.2500    0.3077         8
          Contingency.Cause     0.7765    0.7765    0.7765        85
Contingency.Pragmatic cause     0.1250    0.0909    0.1053        11
        Comparison.Contrast     0.6897    0.6250    0.6557        32
      Comparison.Concession     0.6346    0.5593    0.5946        59
      Expansion.Conjunction     0.4902    0.5556    0.5208        45
    Expansion.Instantiation     1.0000    1.0000    1.0000         7

                   accuracy                         0.6096       292
                  macro avg     0.5690    0.5488    0.5551       292
               weighted avg     0.6123    0.6096    0.6086       292

Epoch [10/15]
Train time usage: 19.059722900390625
Test time usage: 0.5517358779907227
TOP: Test Loss:   3.2,  Test Acc: 68.84%, Test F1: 69.39%
SEC: Test Loss:   3.2,  Test Acc: 59.59%, Test F1: 56.04%
CONN: Test Loss:   3.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.36%,  consistency_sec_conn: 16.75%, consistency_top_sec_conn: 16.36%
              precision    recall  f1-score   support

    Temporal     0.5152    0.6415    0.5714        53
 Contingency     0.7805    0.7529    0.7665        85
  Comparison     0.8205    0.7442    0.7805        43
   Expansion     0.6762    0.6396    0.6574       111

    accuracy                         0.6884       292
   macro avg     0.6981    0.6946    0.6939       292
weighted avg     0.6986    0.6884    0.6917       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4231    0.4889    0.4536        45
         Temporal.Synchrony     0.5000    0.3750    0.4286         8
          Contingency.Cause     0.7778    0.7412    0.7590        85
Contingency.Pragmatic cause     0.1250    0.0909    0.1053        11
        Comparison.Contrast     0.6897    0.6250    0.6557        32
      Comparison.Concession     0.5965    0.5763    0.5862        59
      Expansion.Conjunction     0.4615    0.5333    0.4948        45
    Expansion.Instantiation     1.0000    1.0000    1.0000         7

                   accuracy                         0.5959       292
                  macro avg     0.5717    0.5538    0.5604       292
               weighted avg     0.6012    0.5959    0.5971       292

Epoch [11/15]
Train time usage: 19.05377984046936
Test time usage: 0.5568873882293701
TOP: Test Loss:   3.3,  Test Acc: 71.23%, Test F1: 72.15%
SEC: Test Loss:   3.3,  Test Acc: 60.62%, Test F1: 56.26%
CONN: Test Loss:   3.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.94%,  consistency_sec_conn: 17.04%, consistency_top_sec_conn: 16.94%
              precision    recall  f1-score   support

    Temporal     0.5417    0.7358    0.6240        53
 Contingency     0.7805    0.7529    0.7665        85
  Comparison     0.8889    0.7442    0.8101        43
   Expansion     0.7157    0.6577    0.6854       111

    accuracy                         0.7123       292
   macro avg     0.7317    0.7227    0.7215       292
weighted avg     0.7285    0.7123    0.7162       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4426    0.6000    0.5094        45
         Temporal.Synchrony     0.5000    0.3750    0.4286         8
          Contingency.Cause     0.7875    0.7412    0.7636        85
Contingency.Pragmatic cause     0.1429    0.0909    0.1111        11
        Comparison.Contrast     0.7000    0.6562    0.6774        32
      Comparison.Concession     0.6667    0.5085    0.5769        59
      Expansion.Conjunction     0.4545    0.5556    0.5000        45
    Expansion.Instantiation     0.8750    1.0000    0.9333         7

                   accuracy                         0.6062       292
                  macro avg     0.5711    0.5659    0.5626       292
               weighted avg     0.6190    0.6062    0.6070       292

Epoch [12/15]
Train time usage: 19.068446397781372
Test time usage: 0.5364181995391846
TOP: Test Loss:   3.3,  Test Acc: 71.23%, Test F1: 72.26%
SEC: Test Loss:   3.3,  Test Acc: 61.99%, Test F1: 57.41%
CONN: Test Loss:   3.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 17.32%,  consistency_sec_conn: 17.42%, consistency_top_sec_conn: 17.32%
              precision    recall  f1-score   support

    Temporal     0.5405    0.7547    0.6299        53
 Contingency     0.7901    0.7529    0.7711        85
  Comparison     0.8889    0.7442    0.8101        43
   Expansion     0.7129    0.6486    0.6792       111

    accuracy                         0.7123       292
   macro avg     0.7331    0.7251    0.7226       292
weighted avg     0.7300    0.7123    0.7163       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4746    0.6222    0.5385        45
         Temporal.Synchrony     0.4000    0.5000    0.4444         8
          Contingency.Cause     0.7901    0.7529    0.7711        85
Contingency.Pragmatic cause     0.1429    0.0909    0.1111        11
        Comparison.Contrast     0.7241    0.6562    0.6885        32
      Comparison.Concession     0.6327    0.5254    0.5741        59
      Expansion.Conjunction     0.5102    0.5556    0.5319        45
    Expansion.Instantiation     0.8750    1.0000    0.9333         7

                   accuracy                         0.6199       292
                  macro avg     0.5687    0.5879    0.5741       292
               weighted avg     0.6263    0.6199    0.6196       292

Epoch [13/15]
Train time usage: 19.047146797180176
Test time usage: 0.5595471858978271
TOP: Test Loss:   3.3,  Test Acc: 71.92%, Test F1: 72.89%
SEC: Test Loss:   3.3,  Test Acc: 62.33%, Test F1: 57.46%
CONN: Test Loss:   3.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 17.42%,  consistency_sec_conn: 17.52%, consistency_top_sec_conn: 17.42%
              precision    recall  f1-score   support

    Temporal     0.5417    0.7358    0.6240        53
 Contingency     0.7952    0.7765    0.7857        85
  Comparison     0.9143    0.7442    0.8205        43
   Expansion     0.7157    0.6577    0.6854       111

    accuracy                         0.7192       292
   macro avg     0.7417    0.7285    0.7289       292
weighted avg     0.7365    0.7192    0.7234       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4516    0.6222    0.5234        45
         Temporal.Synchrony     0.4286    0.3750    0.4000         8
          Contingency.Cause     0.7805    0.7529    0.7665        85
Contingency.Pragmatic cause     0.3333    0.0909    0.1429        11
        Comparison.Contrast     0.7273    0.7500    0.7385        32
      Comparison.Concession     0.6667    0.5085    0.5769        59
      Expansion.Conjunction     0.4808    0.5556    0.5155        45
    Expansion.Instantiation     0.8750    1.0000    0.9333         7

                   accuracy                         0.6233       292
                  macro avg     0.5930    0.5819    0.5746       292
               weighted avg     0.6306    0.6233    0.6194       292

Epoch [14/15]
top-down:TOP: Iter:    300,  Train Loss: 5.3e+01,  Train Acc: 100.00%,Val Loss:   2.7,  Val Acc: 75.00%, Val F1: 74.81% Time: 2.7204432487487793 *
top-down:SEC: Iter:    300,  Train Loss: 5.3e+01,  Train Acc: 100.00%,Val Loss:   2.7,  Val Acc: 61.90%, Val F1: 53.73% Time: 2.7204432487487793 *
top-down:CONN: Iter:    300,  Train Loss: 5.3e+01,  Train Acc: 100.00%,Val Loss:   2.7,  Val Acc: 100.00%, Val F1: 100.00% Time: 2.7204432487487793 *
 
 
Train time usage: 21.037363290786743
Test time usage: 0.5814268589019775
TOP: Test Loss:   3.2,  Test Acc: 71.58%, Test F1: 72.36%
SEC: Test Loss:   3.2,  Test Acc: 61.99%, Test F1: 56.71%
CONN: Test Loss:   3.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 17.32%,  consistency_sec_conn: 17.42%, consistency_top_sec_conn: 17.32%
              precision    recall  f1-score   support

    Temporal     0.5373    0.6792    0.6000        53
 Contingency     0.7952    0.7765    0.7857        85
  Comparison     0.9143    0.7442    0.8205        43
   Expansion     0.7009    0.6757    0.6881       111

    accuracy                         0.7158       292
   macro avg     0.7369    0.7189    0.7236       292
weighted avg     0.7301    0.7158    0.7200       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4545    0.5556    0.5000        45
         Temporal.Synchrony     0.3750    0.3750    0.3750         8
          Contingency.Cause     0.7831    0.7647    0.7738        85
Contingency.Pragmatic cause     0.2500    0.0909    0.1333        11
        Comparison.Contrast     0.7188    0.7188    0.7188        32
      Comparison.Concession     0.6154    0.5424    0.5766        59
      Expansion.Conjunction     0.5000    0.5556    0.5263        45
    Expansion.Instantiation     0.8750    1.0000    0.9333         7

                   accuracy                         0.6199       292
                  macro avg     0.5715    0.5754    0.5671       292
               weighted avg     0.6188    0.6199    0.6164       292

Epoch [15/15]
Train time usage: 19.08564257621765
Test time usage: 0.5782897472381592
TOP: Test Loss:   3.3,  Test Acc: 71.92%, Test F1: 72.72%
SEC: Test Loss:   3.3,  Test Acc: 62.33%, Test F1: 57.13%
CONN: Test Loss:   3.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 17.52%,  consistency_sec_conn: 17.52%, consistency_top_sec_conn: 17.52%
              precision    recall  f1-score   support

    Temporal     0.5362    0.6981    0.6066        53
 Contingency     0.8049    0.7765    0.7904        85
  Comparison     0.9143    0.7442    0.8205        43
   Expansion     0.7075    0.6757    0.6912       111

    accuracy                         0.7192       292
   macro avg     0.7407    0.7236    0.7272       292
weighted avg     0.7352    0.7192    0.7238       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4483    0.5778    0.5049        45
         Temporal.Synchrony     0.3750    0.3750    0.3750         8
          Contingency.Cause     0.7831    0.7647    0.7738        85
Contingency.Pragmatic cause     0.3333    0.0909    0.1429        11
        Comparison.Contrast     0.7273    0.7500    0.7385        32
      Comparison.Concession     0.6383    0.5085    0.5660        59
      Expansion.Conjunction     0.5000    0.5778    0.5361        45
    Expansion.Instantiation     0.8750    1.0000    0.9333         7

                   accuracy                         0.6233       292
                  macro avg     0.5850    0.5806    0.5713       292
               weighted avg     0.6266    0.6233    0.6190       292

dev_best_acc_top: 75.00%,  dev_best_f1_top: 74.81%, 
dev_best_acc_sec: 61.90%,  dev_best_f1_sec: 53.73%, 
dev_best_acc_conn: 100.00%,  dev_best_f1_conn: 100.00%
