nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/ita.pdtb.luna/data/', 'log_file': 'data/ita.pdtb.luna/log/', 'save_file': 'data/ita.pdtb.luna/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 30, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March07-14:30:37', 'log': 'data/ita.pdtb.luna/log/March07-14:30:37.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]170it [00:00, 1698.36it/s]481it [00:00, 2525.95it/s]728it [00:00, 2575.42it/s]
0it [00:00, ?it/s]168it [00:00, 3024.58it/s]
0it [00:00, ?it/s]292it [00:00, 3170.43it/s]
Time usage: 8.925395250320435
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/30]
Train time usage: 26.03861379623413
Test time usage: 0.5290384292602539
TOP: Test Loss:   3.6,  Test Acc: 31.51%, Test F1: 14.07%
SEC: Test Loss:   3.6,  Test Acc: 29.11%, Test F1:  5.64%
CONN: Test Loss:   3.6,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  0.00%,  consistency_sec_conn:  8.18%, consistency_top_sec_conn:  0.00%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        53
 Contingency     0.0000    0.0000    0.0000        85
  Comparison     0.0750    0.0698    0.0723        43
   Expansion     0.3532    0.8018    0.4904       111

    accuracy                         0.3151       292
   macro avg     0.1070    0.2179    0.1407       292
weighted avg     0.1453    0.3151    0.1970       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.2911    1.0000    0.4509        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.0000    0.0000    0.0000        59
      Expansion.Conjunction     0.0000    0.0000    0.0000        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.2911       292
                  macro avg     0.0364    0.1250    0.0564       292
               weighted avg     0.0847    0.2911    0.1313       292

Epoch [2/30]
Train time usage: 18.915881633758545
Test time usage: 0.541595458984375
TOP: Test Loss:   3.2,  Test Acc: 38.36%, Test F1: 18.58%
SEC: Test Loss:   3.2,  Test Acc: 29.11%, Test F1:  5.64%
CONN: Test Loss:   3.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  1.15%,  consistency_sec_conn:  8.18%, consistency_top_sec_conn:  1.15%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        53
 Contingency     0.3158    0.1412    0.1951        85
  Comparison     0.0000    0.0000    0.0000        43
   Expansion     0.3937    0.9009    0.5479       111

    accuracy                         0.3836       292
   macro avg     0.1774    0.2605    0.1858       292
weighted avg     0.2416    0.3836    0.2651       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.2911    1.0000    0.4509        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.0000    0.0000    0.0000        59
      Expansion.Conjunction     0.0000    0.0000    0.0000        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.2911       292
                  macro avg     0.0364    0.1250    0.0564       292
               weighted avg     0.0847    0.2911    0.1313       292

Epoch [3/30]
Train time usage: 18.953054428100586
Test time usage: 0.534080982208252
TOP: Test Loss:   3.2,  Test Acc: 29.45%, Test F1: 12.11%
SEC: Test Loss:   3.2,  Test Acc: 29.11%, Test F1:  5.64%
CONN: Test Loss:   3.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  8.08%,  consistency_sec_conn:  8.18%, consistency_top_sec_conn:  8.08%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        53
 Contingency     0.2907    0.9882    0.4492        85
  Comparison     0.0000    0.0000    0.0000        43
   Expansion     0.6667    0.0180    0.0351       111

    accuracy                         0.2945       292
   macro avg     0.2393    0.2516    0.1211       292
weighted avg     0.3380    0.2945    0.1441       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.2911    1.0000    0.4509        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.0000    0.0000    0.0000        59
      Expansion.Conjunction     0.0000    0.0000    0.0000        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.2911       292
                  macro avg     0.0364    0.1250    0.0564       292
               weighted avg     0.0847    0.2911    0.1313       292

Epoch [4/30]
Train time usage: 18.896910190582275
Test time usage: 0.5531497001647949
TOP: Test Loss:   3.2,  Test Acc: 38.01%, Test F1: 13.77%
SEC: Test Loss:   3.2,  Test Acc: 29.11%, Test F1:  5.64%
CONN: Test Loss:   3.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  0.00%,  consistency_sec_conn:  8.18%, consistency_top_sec_conn:  0.00%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        53
 Contingency     0.0000    0.0000    0.0000        85
  Comparison     0.0000    0.0000    0.0000        43
   Expansion     0.3801    1.0000    0.5509       111

    accuracy                         0.3801       292
   macro avg     0.0950    0.2500    0.1377       292
weighted avg     0.1445    0.3801    0.2094       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.2911    1.0000    0.4509        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.0000    0.0000    0.0000        59
      Expansion.Conjunction     0.0000    0.0000    0.0000        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.2911       292
                  macro avg     0.0364    0.1250    0.0564       292
               weighted avg     0.0847    0.2911    0.1313       292

Epoch [5/30]
top-down:TOP: Iter:    100,  Train Loss: 5.7e+01,  Train Acc: 43.75%,Val Loss:   3.2,  Val Acc: 35.12%, Val F1: 14.77% Time: 8.027410745620728 *
top-down:SEC: Iter:    100,  Train Loss: 5.7e+01,  Train Acc: 40.62%,Val Loss:   3.2,  Val Acc: 33.93%, Val F1:  5.63% Time: 8.027410745620728 *
top-down:CONN: Iter:    100,  Train Loss: 5.7e+01,  Train Acc: 100.00%,Val Loss:   3.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 8.027410745620728 *
 
 
Train time usage: 20.87061882019043
Test time usage: 0.5789036750793457
TOP: Test Loss:   3.1,  Test Acc: 41.44%, Test F1: 24.75%
SEC: Test Loss:   3.1,  Test Acc: 29.11%, Test F1:  5.64%
CONN: Test Loss:   3.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  5.20%,  consistency_sec_conn:  8.18%, consistency_top_sec_conn:  5.20%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        53
 Contingency     0.3913    0.6353    0.4843        85
  Comparison     0.0000    0.0000    0.0000        43
   Expansion     0.4351    0.6036    0.5057       111

    accuracy                         0.4144       292
   macro avg     0.2066    0.3097    0.2475       292
weighted avg     0.2793    0.4144    0.3332       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.2911    1.0000    0.4509        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.0000    0.0000    0.0000        32
      Comparison.Concession     0.0000    0.0000    0.0000        59
      Expansion.Conjunction     0.0000    0.0000    0.0000        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.2911       292
                  macro avg     0.0364    0.1250    0.0564       292
               weighted avg     0.0847    0.2911    0.1313       292

Epoch [6/30]
Train time usage: 18.884053945541382
Test time usage: 0.5618276596069336
TOP: Test Loss:   3.1,  Test Acc: 41.78%, Test F1: 24.91%
SEC: Test Loss:   3.1,  Test Acc: 28.77%, Test F1: 13.32%
CONN: Test Loss:   3.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  5.10%,  consistency_sec_conn:  8.08%, consistency_top_sec_conn:  5.10%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        53
 Contingency     0.4080    0.6000    0.4857        85
  Comparison     0.0000    0.0000    0.0000        43
   Expansion     0.4251    0.6396    0.5108       111

    accuracy                         0.4178       292
   macro avg     0.2083    0.3099    0.2491       292
weighted avg     0.2804    0.4178    0.3356       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3158    0.4000    0.3529        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.3819    0.6471    0.4803        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.1059    0.2812    0.1538        32
      Comparison.Concession     0.0000    0.0000    0.0000        59
      Expansion.Conjunction     0.3333    0.0444    0.0784        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.2877       292
                  macro avg     0.1421    0.1716    0.1332       292
               weighted avg     0.2228    0.2877    0.2232       292

Epoch [7/30]
Train time usage: 18.879040718078613
Test time usage: 0.5488300323486328
TOP: Test Loss:   2.9,  Test Acc: 46.92%, Test F1: 34.02%
SEC: Test Loss:   2.9,  Test Acc: 36.99%, Test F1: 18.96%
CONN: Test Loss:   2.9,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  5.97%,  consistency_sec_conn: 10.39%, consistency_top_sec_conn:  5.97%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        53
 Contingency     0.6282    0.5765    0.6012        85
  Comparison     0.2500    0.1860    0.2133        43
   Expansion     0.4396    0.7207    0.5461       111

    accuracy                         0.4692       292
   macro avg     0.3294    0.3708    0.3402       292
weighted avg     0.3868    0.4692    0.4140       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4118    0.4667    0.4375        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.4571    0.7529    0.5689        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.1449    0.3125    0.1980        32
      Comparison.Concession     0.4000    0.2034    0.2697        59
      Expansion.Conjunction     0.5000    0.0222    0.0426        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.3699       292
                  macro avg     0.2392    0.2197    0.1896       292
               weighted avg     0.3703    0.3699    0.3158       292

Epoch [8/30]
Train time usage: 18.92026138305664
Test time usage: 0.5451104640960693
TOP: Test Loss:   3.0,  Test Acc: 48.29%, Test F1: 48.00%
SEC: Test Loss:   3.0,  Test Acc: 36.64%, Test F1: 17.74%
CONN: Test Loss:   3.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec:  8.28%,  consistency_sec_conn: 10.30%, consistency_top_sec_conn:  8.28%
              precision    recall  f1-score   support

    Temporal     0.4545    0.4717    0.4630        53
 Contingency     0.7164    0.5647    0.6316        85
  Comparison     0.3026    0.5349    0.3866        43
   Expansion     0.4787    0.4054    0.4390       111

    accuracy                         0.4829       292
   macro avg     0.4881    0.4942    0.4800       292
weighted avg     0.5176    0.4829    0.4917       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.2910    0.8667    0.4358        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6753    0.6118    0.6420        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.1892    0.4375    0.2642        32
      Comparison.Concession     0.0000    0.0000    0.0000        59
      Expansion.Conjunction     0.2857    0.0444    0.0769        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.3664       292
                  macro avg     0.1802    0.2450    0.1774       292
               weighted avg     0.3062    0.3664    0.2948       292

Epoch [9/30]
top-down:TOP: Iter:    200,  Train Loss: 5.6e+01,  Train Acc: 71.88%,Val Loss:   2.7,  Val Acc: 55.36%, Val F1: 53.93% Time: 14.126805782318115 *
top-down:SEC: Iter:    200,  Train Loss: 5.6e+01,  Train Acc: 75.00%,Val Loss:   2.7,  Val Acc: 42.86%, Val F1: 18.53% Time: 14.126805782318115 *
top-down:CONN: Iter:    200,  Train Loss: 5.6e+01,  Train Acc: 100.00%,Val Loss:   2.7,  Val Acc: 100.00%, Val F1: 100.00% Time: 14.126805782318115 *
 
 
Train time usage: 21.0170578956604
Test time usage: 0.5696184635162354
TOP: Test Loss:   2.7,  Test Acc: 54.45%, Test F1: 54.02%
SEC: Test Loss:   2.7,  Test Acc: 44.86%, Test F1: 26.69%
CONN: Test Loss:   2.7,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 11.45%,  consistency_sec_conn: 12.61%, consistency_top_sec_conn: 11.45%
              precision    recall  f1-score   support

    Temporal     0.5106    0.4528    0.4800        53
 Contingency     0.7353    0.5882    0.6536        85
  Comparison     0.4242    0.6512    0.5138        43
   Expansion     0.5135    0.5135    0.5135       111

    accuracy                         0.5445       292
   macro avg     0.5459    0.5514    0.5402       292
weighted avg     0.5644    0.5445    0.5482       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4211    0.7111    0.5289        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6933    0.6118    0.6500        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.2740    0.6250    0.3810        32
      Comparison.Concession     0.4118    0.3559    0.3818        59
      Expansion.Conjunction     0.3529    0.1333    0.1935        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.4486       292
                  macro avg     0.2691    0.3046    0.2669       292
               weighted avg     0.4343    0.4486    0.4194       292

Epoch [10/30]
Train time usage: 18.96397566795349
Test time usage: 0.5389001369476318
TOP: Test Loss:   3.0,  Test Acc: 53.08%, Test F1: 53.76%
SEC: Test Loss:   3.0,  Test Acc: 45.21%, Test F1: 26.42%
CONN: Test Loss:   3.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 12.22%,  consistency_sec_conn: 12.70%, consistency_top_sec_conn: 12.22%
              precision    recall  f1-score   support

    Temporal     0.4206    0.8491    0.5625        53
 Contingency     0.9412    0.5647    0.7059        85
  Comparison     0.3976    0.7674    0.5238        43
   Expansion     0.5686    0.2613    0.3580       111

    accuracy                         0.5308       292
   macro avg     0.5820    0.6106    0.5376       292
weighted avg     0.6250    0.5308    0.5208       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3611    0.8667    0.5098        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.8929    0.5882    0.7092        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.3086    0.7812    0.4425        32
      Comparison.Concession     0.3824    0.2203    0.2796        59
      Expansion.Conjunction     0.3846    0.1111    0.1724        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.4521       292
                  macro avg     0.2912    0.3210    0.2642       292
               weighted avg     0.4859    0.4521    0.4166       292

Epoch [11/30]
Train time usage: 18.960288763046265
Test time usage: 0.5504860877990723
TOP: Test Loss:   2.8,  Test Acc: 58.90%, Test F1: 59.40%
SEC: Test Loss:   2.8,  Test Acc: 50.00%, Test F1: 30.82%
CONN: Test Loss:   2.8,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.76%,  consistency_sec_conn: 14.05%, consistency_top_sec_conn: 13.76%
              precision    recall  f1-score   support

    Temporal     0.4769    0.5849    0.5254        53
 Contingency     0.7195    0.6941    0.7066        85
  Comparison     0.5741    0.7209    0.6392        43
   Expansion     0.5604    0.4595    0.5050       111

    accuracy                         0.5890       292
   macro avg     0.5827    0.6149    0.5940       292
weighted avg     0.5936    0.5890    0.5871       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4167    0.6667    0.5128        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6818    0.7059    0.6936        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.4182    0.7188    0.5287        32
      Comparison.Concession     0.6000    0.3051    0.4045        59
      Expansion.Conjunction     0.3191    0.3333    0.3261        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5000       292
                  macro avg     0.3045    0.3412    0.3082       292
               weighted avg     0.4789    0.5000    0.4709       292

Epoch [12/30]
Train time usage: 18.94225835800171
Test time usage: 0.5764634609222412
TOP: Test Loss:   2.9,  Test Acc: 60.27%, Test F1: 60.41%
SEC: Test Loss:   2.9,  Test Acc: 51.37%, Test F1: 30.98%
CONN: Test Loss:   2.9,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.86%,  consistency_sec_conn: 14.44%, consistency_top_sec_conn: 13.86%
              precision    recall  f1-score   support

    Temporal     0.5278    0.7170    0.6080        53
 Contingency     0.6778    0.7176    0.6971        85
  Comparison     0.5370    0.6744    0.5979        43
   Expansion     0.6316    0.4324    0.5134       111

    accuracy                         0.6027       292
   macro avg     0.5935    0.6354    0.6041       292
weighted avg     0.6123    0.6027    0.5965       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4384    0.7111    0.5424        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6809    0.7529    0.7151        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.4259    0.7188    0.5349        32
      Comparison.Concession     0.4884    0.3559    0.4118        59
      Expansion.Conjunction     0.3571    0.2222    0.2740        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5137       292
                  macro avg     0.2988    0.3451    0.3098       292
               weighted avg     0.4661    0.5137    0.4758       292

Epoch [13/30]
Train time usage: 19.009738445281982
Test time usage: 0.5363857746124268
TOP: Test Loss:   3.1,  Test Acc: 59.59%, Test F1: 59.76%
SEC: Test Loss:   3.1,  Test Acc: 50.00%, Test F1: 30.59%
CONN: Test Loss:   3.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.76%,  consistency_sec_conn: 14.05%, consistency_top_sec_conn: 13.76%
              precision    recall  f1-score   support

    Temporal     0.5000    0.6981    0.5827        53
 Contingency     0.6905    0.6824    0.6864        85
  Comparison     0.5370    0.6744    0.5979        43
   Expansion     0.6250    0.4505    0.5236       111

    accuracy                         0.5959       292
   macro avg     0.5881    0.6263    0.5976       292
weighted avg     0.6084    0.5959    0.5926       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4265    0.6444    0.5133        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.6932    0.7176    0.7052        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.4340    0.7188    0.5412        32
      Comparison.Concession     0.4783    0.3729    0.4190        59
      Expansion.Conjunction     0.2973    0.2444    0.2683        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5000       292
                  macro avg     0.2911    0.3373    0.3059       292
               weighted avg     0.4575    0.5000    0.4697       292

Epoch [14/30]
top-down:TOP: Iter:    300,  Train Loss: 5.4e+01,  Train Acc: 93.75%,Val Loss:   2.6,  Val Acc: 68.45%, Val F1: 69.18% Time: 2.7095842361450195 *
top-down:SEC: Iter:    300,  Train Loss: 5.4e+01,  Train Acc: 75.00%,Val Loss:   2.6,  Val Acc: 53.57%, Val F1: 28.74% Time: 2.7095842361450195 *
top-down:CONN: Iter:    300,  Train Loss: 5.4e+01,  Train Acc: 100.00%,Val Loss:   2.6,  Val Acc: 100.00%, Val F1: 100.00% Time: 2.7095842361450195 *
 
 
Train time usage: 20.880852222442627
Test time usage: 0.5545022487640381
TOP: Test Loss:   3.1,  Test Acc: 60.96%, Test F1: 61.38%
SEC: Test Loss:   3.1,  Test Acc: 49.66%, Test F1: 30.95%
CONN: Test Loss:   3.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.86%,  consistency_sec_conn: 13.96%, consistency_top_sec_conn: 13.86%
              precision    recall  f1-score   support

    Temporal     0.4937    0.7358    0.5909        53
 Contingency     0.7846    0.6000    0.6800        85
  Comparison     0.5556    0.6977    0.6186        43
   Expansion     0.6170    0.5225    0.5659       111

    accuracy                         0.6096       292
   macro avg     0.6127    0.6390    0.6138       292
weighted avg     0.6344    0.6096    0.6114       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4304    0.7556    0.5484        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7761    0.6118    0.6842        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.4035    0.7188    0.5169        32
      Comparison.Concession     0.4898    0.4068    0.4444        59
      Expansion.Conjunction     0.3000    0.2667    0.2824        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.4966       292
                  macro avg     0.3000    0.3449    0.3095       292
               weighted avg     0.4817    0.4966    0.4736       292

Epoch [15/30]
Train time usage: 19.15345287322998
Test time usage: 0.5338740348815918
TOP: Test Loss:   3.1,  Test Acc: 59.93%, Test F1: 59.25%
SEC: Test Loss:   3.1,  Test Acc: 50.00%, Test F1: 31.06%
CONN: Test Loss:   3.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.86%,  consistency_sec_conn: 14.05%, consistency_top_sec_conn: 13.86%
              precision    recall  f1-score   support

    Temporal     0.5102    0.4717    0.4902        53
 Contingency     0.7284    0.6941    0.7108        85
  Comparison     0.5077    0.7674    0.6111        43
   Expansion     0.5979    0.5225    0.5577       111

    accuracy                         0.5993       292
   macro avg     0.5861    0.6139    0.5925       292
weighted avg     0.6067    0.5993    0.5979       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4667    0.4667    0.4667        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7143    0.7059    0.7101        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.4000    0.7500    0.5217        32
      Comparison.Concession     0.4364    0.4068    0.4211        59
      Expansion.Conjunction     0.3542    0.3778    0.3656        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5000       292
                  macro avg     0.2964    0.3384    0.3106       292
               weighted avg     0.4664    0.5000    0.4772       292

Epoch [16/30]
Train time usage: 18.93141484260559
Test time usage: 0.5428183078765869
TOP: Test Loss:   3.3,  Test Acc: 61.30%, Test F1: 61.56%
SEC: Test Loss:   3.3,  Test Acc: 48.63%, Test F1: 30.77%
CONN: Test Loss:   3.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.47%,  consistency_sec_conn: 13.67%, consistency_top_sec_conn: 13.47%
              precision    recall  f1-score   support

    Temporal     0.5000    0.6226    0.5546        53
 Contingency     0.8226    0.6000    0.6939        85
  Comparison     0.5410    0.7674    0.6346        43
   Expansion     0.6019    0.5586    0.5794       111

    accuracy                         0.6130       292
   macro avg     0.6164    0.6372    0.6156       292
weighted avg     0.6387    0.6130    0.6164       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4262    0.5778    0.4906        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.8226    0.6000    0.6939        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.3968    0.7812    0.5263        32
      Comparison.Concession     0.3968    0.4237    0.4098        59
      Expansion.Conjunction     0.3488    0.3333    0.3409        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.4863       292
                  macro avg     0.2989    0.3395    0.3077       292
               weighted avg     0.4826    0.4863    0.4706       292

Epoch [17/30]
Train time usage: 18.9426212310791
Test time usage: 0.5315632820129395
TOP: Test Loss:   3.2,  Test Acc: 60.62%, Test F1: 61.10%
SEC: Test Loss:   3.2,  Test Acc: 50.34%, Test F1: 31.85%
CONN: Test Loss:   3.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.67%,  consistency_sec_conn: 14.15%, consistency_top_sec_conn: 13.67%
              precision    recall  f1-score   support

    Temporal     0.4925    0.6226    0.5500        53
 Contingency     0.8095    0.6000    0.6892        85
  Comparison     0.5614    0.7442    0.6400        43
   Expansion     0.5810    0.5495    0.5648       111

    accuracy                         0.6062       292
   macro avg     0.6111    0.6291    0.6110       292
weighted avg     0.6286    0.6062    0.6094       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4194    0.5778    0.4860        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.8060    0.6353    0.7105        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.4630    0.7812    0.5814        32
      Comparison.Concession     0.4308    0.4746    0.4516        59
      Expansion.Conjunction     0.3256    0.3111    0.3182        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5034       292
                  macro avg     0.3056    0.3475    0.3185       292
               weighted avg     0.4872    0.5034    0.4857       292

Epoch [18/30]
top-down:TOP: Iter:    400,  Train Loss: 5.2e+01,  Train Acc: 96.88%,Val Loss:   2.7,  Val Acc: 70.24%, Val F1: 71.08% Time: 8.809430599212646 *
top-down:SEC: Iter:    400,  Train Loss: 5.2e+01,  Train Acc: 87.50%,Val Loss:   2.7,  Val Acc: 53.57%, Val F1: 29.36% Time: 8.809430599212646 *
top-down:CONN: Iter:    400,  Train Loss: 5.2e+01,  Train Acc: 100.00%,Val Loss:   2.7,  Val Acc: 100.00%, Val F1: 100.00% Time: 8.809430599212646 *
 
 
Train time usage: 20.96272850036621
Test time usage: 0.5506107807159424
TOP: Test Loss:   3.1,  Test Acc: 63.70%, Test F1: 64.69%
SEC: Test Loss:   3.1,  Test Acc: 52.40%, Test F1: 33.21%
CONN: Test Loss:   3.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.24%,  consistency_sec_conn: 14.73%, consistency_top_sec_conn: 14.24%
              precision    recall  f1-score   support

    Temporal     0.5373    0.6792    0.6000        53
 Contingency     0.7606    0.6353    0.6923        85
  Comparison     0.6889    0.7209    0.7045        43
   Expansion     0.5963    0.5856    0.5909       111

    accuracy                         0.6370       292
   macro avg     0.6458    0.6553    0.6469       292
weighted avg     0.6471    0.6370    0.6388       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4677    0.6444    0.5421        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7600    0.6706    0.7125        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5217    0.7500    0.6154        32
      Comparison.Concession     0.4603    0.4915    0.4754        59
      Expansion.Conjunction     0.3111    0.3111    0.3111        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5240       292
                  macro avg     0.3151    0.3585    0.3321       292
               weighted avg     0.4914    0.5240    0.5024       292

Epoch [19/30]
Train time usage: 18.90977668762207
Test time usage: 0.5403463840484619
TOP: Test Loss:   3.2,  Test Acc: 63.36%, Test F1: 64.62%
SEC: Test Loss:   3.2,  Test Acc: 51.03%, Test F1: 32.20%
CONN: Test Loss:   3.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.05%,  consistency_sec_conn: 14.34%, consistency_top_sec_conn: 14.05%
              precision    recall  f1-score   support

    Temporal     0.5063    0.7547    0.6061        53
 Contingency     0.7846    0.6000    0.6800        85
  Comparison     0.7045    0.7209    0.7126        43
   Expansion     0.6058    0.5676    0.5860       111

    accuracy                         0.6336       292
   macro avg     0.6503    0.6608    0.6462       292
weighted avg     0.6543    0.6336    0.6357       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4304    0.7556    0.5484        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7714    0.6353    0.6968        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5111    0.7188    0.5974        32
      Comparison.Concession     0.4310    0.4237    0.4274        59
      Expansion.Conjunction     0.3250    0.2889    0.3059        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5103       292
                  macro avg     0.3086    0.3528    0.3220       292
               weighted avg     0.4841    0.5103    0.4863       292

Epoch [20/30]
Train time usage: 18.978188514709473
Test time usage: 0.5774891376495361
TOP: Test Loss:   3.5,  Test Acc: 62.67%, Test F1: 63.29%
SEC: Test Loss:   3.5,  Test Acc: 52.40%, Test F1: 32.97%
CONN: Test Loss:   3.5,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.44%,  consistency_sec_conn: 14.73%, consistency_top_sec_conn: 14.44%
              precision    recall  f1-score   support

    Temporal     0.4824    0.7736    0.5942        53
 Contingency     0.8088    0.6471    0.7190        85
  Comparison     0.5962    0.7209    0.6526        43
   Expansion     0.6437    0.5045    0.5657       111

    accuracy                         0.6267       292
   macro avg     0.6328    0.6615    0.6329       292
weighted avg     0.6555    0.6267    0.6283       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4321    0.7778    0.5556        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.8088    0.6471    0.7190        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.4364    0.7500    0.5517        32
      Comparison.Concession     0.4694    0.3898    0.4259        59
      Expansion.Conjunction     0.4211    0.3556    0.3855        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5240       292
                  macro avg     0.3210    0.3650    0.3297       292
               weighted avg     0.5096    0.5240    0.5008       292

Epoch [21/30]
Train time usage: 19.00481390953064
Test time usage: 0.5364944934844971
TOP: Test Loss:   3.3,  Test Acc: 64.73%, Test F1: 65.39%
SEC: Test Loss:   3.3,  Test Acc: 51.03%, Test F1: 32.52%
CONN: Test Loss:   3.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.05%,  consistency_sec_conn: 14.34%, consistency_top_sec_conn: 14.05%
              precision    recall  f1-score   support

    Temporal     0.5263    0.7547    0.6202        53
 Contingency     0.8387    0.6118    0.7075        85
  Comparison     0.7000    0.6512    0.6747        43
   Expansion     0.6053    0.6216    0.6133       111

    accuracy                         0.6473       292
   macro avg     0.6676    0.6598    0.6539       292
weighted avg     0.6728    0.6473    0.6510       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4521    0.7333    0.5593        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.8281    0.6235    0.7114        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5238    0.6875    0.5946        32
      Comparison.Concession     0.4030    0.4576    0.4286        59
      Expansion.Conjunction     0.3043    0.3111    0.3077        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5103       292
                  macro avg     0.3139    0.3516    0.3252       292
               weighted avg     0.4965    0.5103    0.4925       292

Epoch [22/30]
top-down:TOP: Iter:    500,  Train Loss: 5.1e+01,  Train Acc: 100.00%,Val Loss:   2.8,  Val Acc: 69.05%, Val F1: 70.35% Time: 14.943744421005249 *
top-down:SEC: Iter:    500,  Train Loss: 5.1e+01,  Train Acc: 81.25%,Val Loss:   2.8,  Val Acc: 53.57%, Val F1: 32.48% Time: 14.943744421005249 *
top-down:CONN: Iter:    500,  Train Loss: 5.1e+01,  Train Acc: 100.00%,Val Loss:   2.8,  Val Acc: 100.00%, Val F1: 100.00% Time: 14.943744421005249 *
 
 
Train time usage: 20.983748197555542
Test time usage: 0.5334575176239014
TOP: Test Loss:   3.4,  Test Acc: 65.41%, Test F1: 66.61%
SEC: Test Loss:   3.4,  Test Acc: 54.45%, Test F1: 31.17%
CONN: Test Loss:   3.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.11%,  consistency_sec_conn: 15.30%, consistency_top_sec_conn: 15.11%
              precision    recall  f1-score   support

    Temporal     0.5333    0.7547    0.6250        53
 Contingency     0.7671    0.6588    0.7089        85
  Comparison     0.7838    0.6744    0.7250        43
   Expansion     0.6168    0.5946    0.6055       111

    accuracy                         0.6541       292
   macro avg     0.6753    0.6706    0.6661       292
weighted avg     0.6700    0.6541    0.6567       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4789    0.7556    0.5862        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7778    0.6588    0.7134        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5500    0.6875    0.6111        32
      Comparison.Concession     0.5102    0.4237    0.4630        59
      Expansion.Conjunction     0.3860    0.4889    0.4314        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                  micro avg     0.5464    0.5445    0.5455       292
                  macro avg     0.3379    0.3768    0.3506       292
               weighted avg     0.5231    0.5445    0.5250       292

Epoch [23/30]
Train time usage: 18.81065845489502
Test time usage: 0.5747253894805908
TOP: Test Loss:   3.4,  Test Acc: 65.07%, Test F1: 65.57%
SEC: Test Loss:   3.4,  Test Acc: 52.40%, Test F1: 33.65%
CONN: Test Loss:   3.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.53%,  consistency_sec_conn: 14.73%, consistency_top_sec_conn: 14.53%
              precision    recall  f1-score   support

    Temporal     0.5333    0.7547    0.6250        53
 Contingency     0.8209    0.6471    0.7237        85
  Comparison     0.6383    0.6977    0.6667        43
   Expansion     0.6311    0.5856    0.6075       111

    accuracy                         0.6507       292
   macro avg     0.6559    0.6713    0.6557       292
weighted avg     0.6697    0.6507    0.6532       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4545    0.6667    0.5405        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7746    0.6471    0.7051        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5227    0.7188    0.6053        32
      Comparison.Concession     0.4694    0.3898    0.4259        59
      Expansion.Conjunction     0.3607    0.4889    0.4151        45
    Expansion.Instantiation     0.0000    0.0000    0.0000         7

                   accuracy                         0.5240       292
                  macro avg     0.3227    0.3639    0.3365       292
               weighted avg     0.5033    0.5240    0.5049       292

Epoch [24/30]
Train time usage: 19.353809356689453
Test time usage: 0.5620818138122559
TOP: Test Loss:   3.3,  Test Acc: 64.04%, Test F1: 64.51%
SEC: Test Loss:   3.3,  Test Acc: 53.77%, Test F1: 37.77%
CONN: Test Loss:   3.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.82%,  consistency_sec_conn: 15.11%, consistency_top_sec_conn: 14.82%
              precision    recall  f1-score   support

    Temporal     0.5161    0.6038    0.5565        53
 Contingency     0.7703    0.6706    0.7170        85
  Comparison     0.7250    0.6744    0.6988        43
   Expansion     0.5948    0.6216    0.6079       111

    accuracy                         0.6404       292
   macro avg     0.6516    0.6426    0.6451       292
weighted avg     0.6508    0.6404    0.6437       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4800    0.5333    0.5053        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7808    0.6706    0.7215        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5500    0.6875    0.6111        32
      Comparison.Concession     0.4906    0.4407    0.4643        59
      Expansion.Conjunction     0.3857    0.6000    0.4696        45
    Expansion.Instantiation     1.0000    0.1429    0.2500         7

                   accuracy                         0.5377       292
                  macro avg     0.4609    0.3844    0.3777       292
               weighted avg     0.5441    0.5377    0.5270       292

Epoch [25/30]
Train time usage: 19.077909469604492
Test time usage: 0.563004732131958
TOP: Test Loss:   3.4,  Test Acc: 64.38%, Test F1: 65.18%
SEC: Test Loss:   3.4,  Test Acc: 54.11%, Test F1: 33.70%
CONN: Test Loss:   3.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.73%,  consistency_sec_conn: 15.21%, consistency_top_sec_conn: 14.73%
              precision    recall  f1-score   support

    Temporal     0.5270    0.7358    0.6142        53
 Contingency     0.7671    0.6588    0.7089        85
  Comparison     0.6818    0.6977    0.6897        43
   Expansion     0.6238    0.5676    0.5943       111

    accuracy                         0.6438       292
   macro avg     0.6499    0.6650    0.6518       292
weighted avg     0.6565    0.6438    0.6453       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4667    0.6222    0.5333        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7778    0.6588    0.7134        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5106    0.7500    0.6076        32
      Comparison.Concession     0.5208    0.4237    0.4673        59
      Expansion.Conjunction     0.4068    0.5333    0.4615        45
    Expansion.Instantiation     1.0000    0.1429    0.2500         7

                  micro avg     0.5430    0.5411    0.5420       292
                  macro avg     0.4603    0.3914    0.3791       292
               weighted avg     0.5462    0.5411    0.5280       292

Epoch [26/30]
Train time usage: 19.0362811088562
Test time usage: 0.5616359710693359
TOP: Test Loss:   3.4,  Test Acc: 64.73%, Test F1: 65.42%
SEC: Test Loss:   3.4,  Test Acc: 53.77%, Test F1: 33.42%
CONN: Test Loss:   3.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.92%,  consistency_sec_conn: 15.11%, consistency_top_sec_conn: 14.92%
              precision    recall  f1-score   support

    Temporal     0.5263    0.7547    0.6202        53
 Contingency     0.7703    0.6706    0.7170        85
  Comparison     0.6905    0.6744    0.6824        43
   Expansion     0.6300    0.5676    0.5972       111

    accuracy                         0.6473       292
   macro avg     0.6543    0.6668    0.6542       292
weighted avg     0.6609    0.6473    0.6488       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4531    0.6444    0.5321        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7808    0.6706    0.7215        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5238    0.6875    0.5946        32
      Comparison.Concession     0.5106    0.4068    0.4528        59
      Expansion.Conjunction     0.4000    0.5333    0.4571        45
    Expansion.Instantiation     1.0000    0.1429    0.2500         7

                  micro avg     0.5395    0.5377    0.5386       292
                  macro avg     0.4585    0.3857    0.3760       292
               weighted avg     0.5433    0.5377    0.5251       292

Epoch [27/30]
top-down:TOP: Iter:    600,  Train Loss: 5.2e+01,  Train Acc: 100.00%,Val Loss:   2.9,  Val Acc: 68.45%, Val F1: 69.21% Time: 1.7985882759094238 
top-down:SEC: Iter:    600,  Train Loss: 5.2e+01,  Train Acc: 93.75%,Val Loss:   2.9,  Val Acc: 54.17%, Val F1: 32.60% Time: 1.7985882759094238 
top-down:CONN: Iter:    600,  Train Loss: 5.2e+01,  Train Acc: 100.00%,Val Loss:   2.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 1.7985882759094238 
 
 
Train time usage: 19.469597101211548
Test time usage: 0.5634844303131104
TOP: Test Loss:   3.4,  Test Acc: 63.70%, Test F1: 64.52%
SEC: Test Loss:   3.4,  Test Acc: 53.08%, Test F1: 33.26%
CONN: Test Loss:   3.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.53%,  consistency_sec_conn: 14.92%, consistency_top_sec_conn: 14.53%
              precision    recall  f1-score   support

    Temporal     0.5278    0.7170    0.6080        53
 Contingency     0.7639    0.6471    0.7006        85
  Comparison     0.6905    0.6744    0.6824        43
   Expansion     0.6038    0.5766    0.5899       111

    accuracy                         0.6370       292
   macro avg     0.6465    0.6538    0.6452       292
weighted avg     0.6494    0.6370    0.6390       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4655    0.6000    0.5243        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7746    0.6471    0.7051        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5333    0.7500    0.6234        32
      Comparison.Concession     0.5192    0.4576    0.4865        59
      Expansion.Conjunction     0.3559    0.4667    0.4038        45
    Expansion.Instantiation     1.0000    0.1429    0.2500         7

                  micro avg     0.5326    0.5308    0.5317       292
                  macro avg     0.4561    0.3830    0.3741       292
               weighted avg     0.5394    0.5308    0.5209       292

Epoch [28/30]
Train time usage: 22.763063669204712
Test time usage: 0.7577066421508789
TOP: Test Loss:   3.4,  Test Acc: 64.04%, Test F1: 64.76%
SEC: Test Loss:   3.4,  Test Acc: 53.08%, Test F1: 32.99%
CONN: Test Loss:   3.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.44%,  consistency_sec_conn: 14.92%, consistency_top_sec_conn: 14.44%
              precision    recall  f1-score   support

    Temporal     0.5278    0.7170    0.6080        53
 Contingency     0.7671    0.6588    0.7089        85
  Comparison     0.6667    0.6977    0.6818        43
   Expansion     0.6176    0.5676    0.5915       111

    accuracy                         0.6404       292
   macro avg     0.6448    0.6603    0.6476       292
weighted avg     0.6521    0.6404    0.6420       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4561    0.5778    0.5098        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7703    0.6706    0.7170        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5000    0.7500    0.6000        32
      Comparison.Concession     0.5098    0.4407    0.4727        59
      Expansion.Conjunction     0.3818    0.4667    0.4200        45
    Expansion.Instantiation     1.0000    0.1429    0.2500         7

                  micro avg     0.5326    0.5308    0.5317       292
                  macro avg     0.4523    0.3811    0.3712       292
               weighted avg     0.5351    0.5308    0.5193       292

Epoch [29/30]
Train time usage: 23.3709716796875
Test time usage: 0.7491061687469482
TOP: Test Loss:   3.4,  Test Acc: 63.70%, Test F1: 64.24%
SEC: Test Loss:   3.4,  Test Acc: 52.40%, Test F1: 32.57%
CONN: Test Loss:   3.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.44%,  consistency_sec_conn: 14.73%, consistency_top_sec_conn: 14.44%
              precision    recall  f1-score   support

    Temporal     0.5211    0.6981    0.5968        53
 Contingency     0.7778    0.6588    0.7134        85
  Comparison     0.6591    0.6744    0.6667        43
   Expansion     0.6095    0.5766    0.5926       111

    accuracy                         0.6370       292
   macro avg     0.6419    0.6520    0.6424       292
weighted avg     0.6498    0.6370    0.6394       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4333    0.5778    0.4952        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7778    0.6588    0.7134        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.4894    0.7188    0.5823        32
      Comparison.Concession     0.5000    0.4576    0.4779        59
      Expansion.Conjunction     0.3846    0.4444    0.4124        45
    Expansion.Instantiation     1.0000    0.1429    0.2500         7

                  micro avg     0.5258    0.5240    0.5249       292
                  macro avg     0.4481    0.3750    0.3664       292
               weighted avg     0.5311    0.5240    0.5139       292

Epoch [30/30]
Train time usage: 23.14966320991516
Test time usage: 0.7534053325653076
TOP: Test Loss:   3.4,  Test Acc: 63.70%, Test F1: 64.31%
SEC: Test Loss:   3.4,  Test Acc: 52.74%, Test F1: 32.81%
CONN: Test Loss:   3.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.44%,  consistency_sec_conn: 14.82%, consistency_top_sec_conn: 14.44%
              precision    recall  f1-score   support

    Temporal     0.5278    0.7170    0.6080        53
 Contingency     0.7746    0.6471    0.7051        85
  Comparison     0.6591    0.6744    0.6667        43
   Expansion     0.6095    0.5766    0.5926       111

    accuracy                         0.6370       292
   macro avg     0.6428    0.6538    0.6431       292
weighted avg     0.6501    0.6370    0.6391       292

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4426    0.6000    0.5094        45
         Temporal.Synchrony     0.0000    0.0000    0.0000         8
          Contingency.Cause     0.7778    0.6588    0.7134        85
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        11
        Comparison.Contrast     0.5000    0.7188    0.5897        32
      Comparison.Concession     0.5000    0.4576    0.4779        59
      Expansion.Conjunction     0.3846    0.4444    0.4124        45
    Expansion.Instantiation     1.0000    0.1429    0.2500         7

                  micro avg     0.5292    0.5274    0.5283       292
                  macro avg     0.4506    0.3778    0.3691       292
               weighted avg     0.5337    0.5274    0.5169       292

dev_best_acc_top: 69.05%,  dev_best_f1_top: 70.35%, 
dev_best_acc_sec: 53.57%,  dev_best_f1_sec: 32.48%, 
dev_best_acc_conn: 100.00%,  dev_best_f1_conn: 100.00%
