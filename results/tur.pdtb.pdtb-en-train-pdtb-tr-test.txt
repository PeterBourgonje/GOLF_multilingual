nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_en_train_pdtb_tr_test/data/', 'log_file': 'data/pdtb_en_train_pdtb_tr_test/log/', 'save_file': 'data/pdtb_en_train_pdtb_tr_test/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 30, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March09-16:23:51', 'log': 'data/pdtb_en_train_pdtb_tr_test/log/March09-16:23:51.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]101it [00:00, 1007.58it/s]299it [00:00, 1572.08it/s]496it [00:00, 1752.51it/s]706it [00:00, 1889.34it/s]899it [00:00, 1900.42it/s]1098it [00:00, 1929.61it/s]1328it [00:00, 2048.97it/s]1545it [00:00, 2084.31it/s]1754it [00:00, 2031.77it/s]1970it [00:01, 2068.74it/s]2179it [00:01, 2073.08it/s]2387it [00:01, 2054.40it/s]2597it [00:01, 2064.91it/s]2805it [00:01, 2068.54it/s]3014it [00:01, 2072.68it/s]3224it [00:01, 2079.28it/s]3437it [00:01, 2093.98it/s]3647it [00:01, 2072.97it/s]3855it [00:01, 2040.77it/s]4066it [00:02, 2060.77it/s]4273it [00:02, 1992.48it/s]4473it [00:02, 1935.15it/s]4668it [00:02, 1939.21it/s]4879it [00:02, 1986.77it/s]5085it [00:02, 2006.72it/s]5287it [00:02, 1413.66it/s]5472it [00:02, 1512.91it/s]5678it [00:02, 1645.35it/s]5910it [00:03, 1819.90it/s]6118it [00:03, 1888.03it/s]6332it [00:03, 1956.31it/s]6536it [00:03, 1979.30it/s]6740it [00:03, 1986.14it/s]6943it [00:03, 1998.06it/s]7150it [00:03, 2017.19it/s]7363it [00:03, 2049.09it/s]7570it [00:03, 2031.99it/s]7775it [00:04, 2031.14it/s]7980it [00:04, 2034.74it/s]8185it [00:04, 1996.23it/s]8386it [00:04, 1972.73it/s]8589it [00:04, 1989.28it/s]8804it [00:04, 2036.42it/s]9008it [00:04, 2005.27it/s]9219it [00:04, 2035.99it/s]9423it [00:04, 2005.51it/s]9637it [00:04, 2043.66it/s]9857it [00:05, 2085.94it/s]10070it [00:05, 2098.32it/s]10281it [00:05, 2029.37it/s]10494it [00:05, 2057.47it/s]10701it [00:05, 1989.15it/s]10916it [00:05, 2032.89it/s]11120it [00:05, 2034.17it/s]11327it [00:05, 2042.86it/s]11532it [00:05, 2025.92it/s]11735it [00:05, 2021.14it/s]11942it [00:06, 2034.30it/s]12146it [00:06, 1945.19it/s]12345it [00:06, 1957.10it/s]12542it [00:06, 1945.76it/s]12547it [00:06, 1967.61it/s]
0it [00:00, ?it/s]194it [00:00, 1931.71it/s]389it [00:00, 1941.55it/s]584it [00:00, 1943.06it/s]779it [00:00, 1875.72it/s]967it [00:00, 1872.24it/s]1165it [00:00, 1910.05it/s]
0it [00:00, ?it/s]148it [00:00, 1469.04it/s]319it [00:00, 1607.20it/s]489it [00:00, 1647.24it/s]654it [00:00, 1643.65it/s]819it [00:00, 1612.31it/s]983it [00:00, 1618.75it/s]1039it [00:00, 1617.35it/s]
Time usage: 17.68267798423767
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/30]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 55.88%, Val F1: 17.92% Time: 88.1895215511322 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   7.5,  Val Acc: 24.21%, Val F1:  3.54% Time: 88.1895215511322 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  0.00%,Val Loss:   7.5,  Val Acc:  2.66%, Val F1:  0.25% Time: 88.1895215511322 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 55.79%, Val F1: 17.91% Time: 168.86775612831116 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.6,  Val Acc: 27.64%, Val F1:  5.84% Time: 168.86775612831116 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.6,  Val Acc: 12.88%, Val F1:  0.34% Time: 168.86775612831116 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 71.88%,Val Loss:   6.4,  Val Acc: 55.62%, Val F1: 20.55% Time: 249.45780444145203 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 31.25%,Val Loss:   6.4,  Val Acc: 30.30%, Val F1:  9.70% Time: 249.45780444145203 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 15.62%,Val Loss:   6.4,  Val Acc: 17.00%, Val F1:  1.04% Time: 249.45780444145203 *
 
 
Train time usage: 322.5021221637726
Test time usage: 1.7325291633605957
TOP: Test Loss:   6.4,  Test Acc: 53.61%, Test F1: 17.45%
SEC: Test Loss:   6.4,  Test Acc: 33.88%, Test F1: 11.48%
CONN: Test Loss:   6.4,  Test Acc: 14.34%, Test F1:  0.76%
consistency_top_sec: 21.75%,  consistency_sec_conn:  9.14%, consistency_top_sec_conn:  3.56%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        68
 Contingency     0.0000    0.0000    0.0000       270
  Comparison     0.0000    0.0000    0.0000       144
   Expansion     0.5361    1.0000    0.6980       557

    accuracy                         0.5361      1039
   macro avg     0.1340    0.2500    0.1745      1039
weighted avg     0.2874    0.5361    0.3742      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5244    0.4813    0.5019       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.0000    0.0000    0.0000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3780    0.4800    0.4229       200
    Expansion.Instantiation     0.0000    0.0000    0.0000       118
      Expansion.Restatement     0.2356    0.5991    0.3382       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.3388      1039
                  macro avg     0.1035    0.1419    0.1148      1039
               weighted avg     0.2561    0.3388    0.2799      1039

Epoch [2/30]
top-down:TOP: Iter:    400,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.2,  Val Acc: 55.79%, Val F1: 30.86% Time: 8.874348640441895 *
top-down:SEC: Iter:    400,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   6.2,  Val Acc: 35.28%, Val F1: 11.83% Time: 8.874348640441895 *
top-down:CONN: Iter:    400,  Train Loss: 3e+01,  Train Acc:  9.38%,Val Loss:   6.2,  Val Acc: 17.94%, Val F1:  1.49% Time: 8.874348640441895 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 53.30%, Val F1: 30.85% Time: 87.77926707267761 
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   6.2,  Val Acc: 35.54%, Val F1: 12.51% Time: 87.77926707267761 
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   6.2,  Val Acc: 18.97%, Val F1:  1.82% Time: 87.77926707267761 
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   5.9,  Val Acc: 56.65%, Val F1: 40.45% Time: 168.24885749816895 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 40.62%,Val Loss:   5.9,  Val Acc: 39.48%, Val F1: 19.14% Time: 168.24885749816895 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 18.75%,Val Loss:   5.9,  Val Acc: 22.40%, Val F1:  2.83% Time: 168.24885749816895 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 55.97%, Val F1: 47.65% Time: 248.74275660514832 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 42.49%, Val F1: 21.28% Time: 248.74275660514832 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.7,  Val Acc: 23.69%, Val F1:  3.59% Time: 248.74275660514832 *
 
 
Train time usage: 316.1452000141144
Test time usage: 1.7558159828186035
TOP: Test Loss:   5.4,  Test Acc: 60.35%, Test F1: 44.05%
SEC: Test Loss:   5.4,  Test Acc: 45.72%, Test F1: 24.95%
CONN: Test Loss:   5.4,  Test Acc: 23.77%, Test F1:  3.94%
consistency_top_sec: 36.09%,  consistency_sec_conn: 19.35%, consistency_top_sec_conn: 15.30%
              precision    recall  f1-score   support

    Temporal     0.4792    0.3382    0.3966        68
 Contingency     0.5943    0.2325    0.3342       271
  Comparison     0.6222    0.1944    0.2963       144
   Expansion     0.6107    0.9227    0.7350       556

    accuracy                         0.6035      1039
   macro avg     0.5766    0.4220    0.4405      1039
weighted avg     0.5994    0.6035    0.5475      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3623    0.4630    0.4065        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5134    0.5709    0.5406       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5741    0.2422    0.3407       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4164    0.5850    0.4865       200
    Expansion.Instantiation     0.6702    0.5294    0.5915       119
      Expansion.Restatement     0.3539    0.4076    0.3789       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4572      1039
                  macro avg     0.2628    0.2544    0.2495      1039
               weighted avg     0.4508    0.4572    0.4409      1039

Epoch [3/30]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 43.75%,Val Loss:   5.5,  Val Acc: 60.69%, Val F1: 50.70% Time: 14.30807614326477 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 37.50%,Val Loss:   5.5,  Val Acc: 45.06%, Val F1: 25.51% Time: 14.30807614326477 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 28.12%,Val Loss:   5.5,  Val Acc: 23.78%, Val F1:  3.87% Time: 14.30807614326477 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 63.35%, Val F1: 51.16% Time: 94.77918243408203 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 48.58%, Val F1: 27.46% Time: 94.77918243408203 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 18.75%,Val Loss:   5.4,  Val Acc: 27.21%, Val F1:  4.46% Time: 94.77918243408203 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 62.15%, Val F1: 42.76% Time: 173.66641330718994 
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   5.3,  Val Acc: 48.15%, Val F1: 24.26% Time: 173.66641330718994 
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   5.3,  Val Acc: 27.55%, Val F1:  4.42% Time: 173.66641330718994 
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   5.3,  Val Acc: 62.83%, Val F1: 50.68% Time: 254.41971707344055 *
top-down:SEC: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   5.3,  Val Acc: 49.53%, Val F1: 29.44% Time: 254.41971707344055 *
top-down:CONN: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   5.3,  Val Acc: 28.58%, Val F1:  5.28% Time: 254.41971707344055 *
 
 
Train time usage: 316.3190929889679
Test time usage: 1.727600336074829
TOP: Test Loss:   5.4,  Test Acc: 61.21%, Test F1: 48.27%
SEC: Test Loss:   5.4,  Test Acc: 46.58%, Test F1: 25.40%
CONN: Test Loss:   5.4,  Test Acc: 20.50%, Test F1:  4.08%
consistency_top_sec: 39.85%,  consistency_sec_conn: 16.94%, consistency_top_sec_conn: 13.96%
              precision    recall  f1-score   support

    Temporal     0.4130    0.2794    0.3333        68
 Contingency     0.5818    0.3542    0.4404       271
  Comparison     0.5426    0.3542    0.4286       144
   Expansion     0.6403    0.8453    0.7287       556

    accuracy                         0.6121      1039
   macro avg     0.5444    0.4583    0.4827      1039
weighted avg     0.5966    0.6121    0.5860      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4043    0.3519    0.3762        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5361    0.5821    0.5581       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4717    0.3906    0.4274       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3702    0.6700    0.4769       200
    Expansion.Instantiation     0.7126    0.5210    0.6019       119
      Expansion.Restatement     0.4315    0.2986    0.3529       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4658      1039
                  macro avg     0.2660    0.2558    0.2540      1039
               weighted avg     0.4579    0.4658    0.4486      1039

Epoch [4/30]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   5.2,  Val Acc: 63.69%, Val F1: 52.02% Time: 19.668127298355103 *
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   5.2,  Val Acc: 50.64%, Val F1: 28.09% Time: 19.668127298355103 *
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 43.75%,Val Loss:   5.2,  Val Acc: 28.76%, Val F1:  6.07% Time: 19.668127298355103 *
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 64.03%, Val F1: 50.27% Time: 98.55890989303589 
top-down:SEC: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 62.50%,Val Loss:   5.3,  Val Acc: 48.58%, Val F1: 29.86% Time: 98.55890989303589 
top-down:CONN: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 50.00%,Val Loss:   5.3,  Val Acc: 28.41%, Val F1:  5.52% Time: 98.55890989303589 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.3,  Val Acc: 62.23%, Val F1: 51.27% Time: 179.22878456115723 *
top-down:SEC: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   5.3,  Val Acc: 49.53%, Val F1: 31.74% Time: 179.22878456115723 *
top-down:CONN: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   5.3,  Val Acc: 28.58%, Val F1:  6.28% Time: 179.22878456115723 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 62.50%,Val Loss:   5.3,  Val Acc: 59.83%, Val F1: 49.77% Time: 258.3616659641266 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 48.15%, Val F1: 31.53% Time: 258.3616659641266 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 31.25%,Val Loss:   5.3,  Val Acc: 28.93%, Val F1:  6.59% Time: 258.3616659641266 
 
 
Train time usage: 314.94291400909424
Test time usage: 1.7615416049957275
TOP: Test Loss:   5.2,  Test Acc: 60.92%, Test F1: 51.15%
SEC: Test Loss:   5.2,  Test Acc: 47.45%, Test F1: 26.31%
CONN: Test Loss:   5.2,  Test Acc: 23.10%, Test F1:  7.19%
consistency_top_sec: 42.35%,  consistency_sec_conn: 17.52%, consistency_top_sec_conn: 15.88%
              precision    recall  f1-score   support

    Temporal     0.6923    0.2647    0.3830        68
 Contingency     0.5168    0.4522    0.4824       272
  Comparison     0.5455    0.4167    0.4724       144
   Expansion     0.6496    0.7784    0.7082       555

    accuracy                         0.6092      1039
   macro avg     0.6010    0.4780    0.5115      1039
weighted avg     0.6032    0.6092    0.5951      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6667    0.2963    0.4103        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5030    0.6343    0.5611       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4957    0.4453    0.4691       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4167    0.5000    0.4545       200
    Expansion.Instantiation     0.6700    0.5630    0.6119       119
      Expansion.Restatement     0.3807    0.3934    0.3869       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4745      1039
                  macro avg     0.2848    0.2575    0.2631      1039
               weighted avg     0.4597    0.4745    0.4600      1039

Epoch [5/30]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 64.64%, Val F1: 53.51% Time: 25.077139139175415 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   5.3,  Val Acc: 51.16%, Val F1: 30.64% Time: 25.077139139175415 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 31.25%,Val Loss:   5.3,  Val Acc: 29.53%, Val F1:  6.06% Time: 25.077139139175415 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 62.66%, Val F1: 52.05% Time: 104.016028881073 
top-down:SEC: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 49.27%, Val F1: 31.04% Time: 104.016028881073 
top-down:CONN: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   5.4,  Val Acc: 27.90%, Val F1:  6.50% Time: 104.016028881073 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 60.69%, Val F1: 52.95% Time: 182.93618178367615 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 48.93%, Val F1: 30.90% Time: 182.93618178367615 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 27.64%, Val F1:  6.40% Time: 182.93618178367615 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.4,  Val Acc: 61.55%, Val F1: 51.54% Time: 261.76792311668396 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 49.87%, Val F1: 31.73% Time: 261.76792311668396 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 28.24%, Val F1:  6.56% Time: 261.76792311668396 
 
 
Train time usage: 312.8380777835846
Test time usage: 1.7686583995819092
TOP: Test Loss:   5.3,  Test Acc: 62.46%, Test F1: 53.19%
SEC: Test Loss:   5.3,  Test Acc: 48.22%, Test F1: 29.61%
CONN: Test Loss:   5.3,  Test Acc: 24.35%, Test F1:  7.39%
consistency_top_sec: 45.43%,  consistency_sec_conn: 19.83%, consistency_top_sec_conn: 18.77%
              precision    recall  f1-score   support

    Temporal     0.6333    0.2794    0.3878        68
 Contingency     0.5368    0.5604    0.5484       273
  Comparison     0.5495    0.4236    0.4784       144
   Expansion     0.6786    0.7509    0.7129       554

    accuracy                         0.6246      1039
   macro avg     0.5996    0.5036    0.5319      1039
weighted avg     0.6205    0.6246    0.6159      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5588    0.3519    0.4318        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4807    0.6493    0.5524       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4821    0.4219    0.4500       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4549    0.5300    0.4896       200
    Expansion.Instantiation     0.6667    0.5546    0.6055       119
      Expansion.Restatement     0.4180    0.3744    0.3950       211
      Expansion.Alternative     0.3333    0.3333    0.3333         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4822      1039
                  macro avg     0.3086    0.2923    0.2961      1039
               weighted avg     0.4641    0.4822    0.4671      1039

Epoch [6/30]
top-down:TOP: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   5.5,  Val Acc: 63.00%, Val F1: 52.90% Time: 28.80731201171875 
top-down:SEC: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 50.30%, Val F1: 31.69% Time: 28.80731201171875 
top-down:CONN: Iter:   2000,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.5,  Val Acc: 30.13%, Val F1:  7.43% Time: 28.80731201171875 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 63.86%, Val F1: 52.08% Time: 108.07617259025574 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.6,  Val Acc: 50.13%, Val F1: 30.90% Time: 108.07617259025574 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   5.6,  Val Acc: 29.61%, Val F1:  7.49% Time: 108.07617259025574 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   5.6,  Val Acc: 62.49%, Val F1: 52.13% Time: 186.99120163917542 
top-down:SEC: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 50.30%, Val F1: 30.61% Time: 186.99120163917542 
top-down:CONN: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   5.6,  Val Acc: 30.90%, Val F1:  7.44% Time: 186.99120163917542 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 61.72%, Val F1: 51.66% Time: 265.83412051200867 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 62.50%,Val Loss:   5.6,  Val Acc: 49.27%, Val F1: 32.00% Time: 265.83412051200867 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 28.93%, Val F1:  7.24% Time: 265.83412051200867 
 
 
Train time usage: 311.60784888267517
Test time usage: 1.7652757167816162
TOP: Test Loss:   5.4,  Test Acc: 62.66%, Test F1: 53.45%
SEC: Test Loss:   5.4,  Test Acc: 49.09%, Test F1: 29.17%
CONN: Test Loss:   5.4,  Test Acc: 24.54%, Test F1:  7.34%
consistency_top_sec: 46.58%,  consistency_sec_conn: 20.50%, consistency_top_sec_conn: 19.44%
              precision    recall  f1-score   support

    Temporal     0.5111    0.3382    0.4071        68
 Contingency     0.5333    0.5000    0.5161       272
  Comparison     0.5810    0.4236    0.4900       144
   Expansion     0.6798    0.7766    0.7250       555

    accuracy                         0.6266      1039
   macro avg     0.5763    0.5096    0.5345      1039
weighted avg     0.6167    0.6266    0.6169      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3704    0.4255        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4952    0.5843    0.5361       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5229    0.4453    0.4810       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4346    0.6650    0.5257       200
    Expansion.Instantiation     0.7128    0.5630    0.6291       119
      Expansion.Restatement     0.4630    0.3538    0.4011       212
      Expansion.Alternative     0.2000    0.2222    0.2105         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4909      1039
                  macro avg     0.3026    0.2913    0.2917      1039
               weighted avg     0.4792    0.4909    0.4760      1039

Epoch [7/30]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 60.94%, Val F1: 52.73% Time: 34.29206848144531 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 75.00%,Val Loss:   5.8,  Val Acc: 48.84%, Val F1: 32.43% Time: 34.29206848144531 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 31.25%,Val Loss:   5.8,  Val Acc: 28.58%, Val F1:  7.60% Time: 34.29206848144531 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.7,  Val Acc: 62.83%, Val F1: 52.77% Time: 115.09012150764465 *
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   5.7,  Val Acc: 50.64%, Val F1: 32.78% Time: 115.09012150764465 *
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   5.7,  Val Acc: 29.10%, Val F1:  7.71% Time: 115.09012150764465 *
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   5.9,  Val Acc: 62.40%, Val F1: 52.41% Time: 193.8718500137329 
top-down:SEC: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   5.9,  Val Acc: 49.53%, Val F1: 30.88% Time: 193.8718500137329 
top-down:CONN: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 28.12%,Val Loss:   5.9,  Val Acc: 29.10%, Val F1:  7.57% Time: 193.8718500137329 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 87.50%,Val Loss:   5.8,  Val Acc: 60.43%, Val F1: 51.47% Time: 272.8959701061249 
top-down:SEC: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 75.00%,Val Loss:   5.8,  Val Acc: 49.53%, Val F1: 30.44% Time: 272.8959701061249 
top-down:CONN: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 28.12%,Val Loss:   5.8,  Val Acc: 29.36%, Val F1:  8.24% Time: 272.8959701061249 
 
 
Train time usage: 313.4175179004669
Test time usage: 1.7645280361175537
TOP: Test Loss:   5.7,  Test Acc: 60.54%, Test F1: 51.36%
SEC: Test Loss:   5.7,  Test Acc: 48.03%, Test F1: 28.75%
CONN: Test Loss:   5.7,  Test Acc: 25.12%, Test F1:  7.85%
consistency_top_sec: 46.01%,  consistency_sec_conn: 21.08%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.4565    0.3088    0.3684        68
 Contingency     0.5047    0.5934    0.5455       273
  Comparison     0.5288    0.3819    0.4435       144
   Expansion     0.6884    0.7058    0.6970       554

    accuracy                         0.6054      1039
   macro avg     0.5446    0.4975    0.5136      1039
weighted avg     0.6028    0.6054    0.6005      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4783    0.4074    0.4400        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4856    0.6330    0.5496       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4636    0.3984    0.4286       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4368    0.5700    0.4946       200
    Expansion.Instantiation     0.7473    0.5714    0.6476       119
      Expansion.Restatement     0.4244    0.3443    0.3802       212
      Expansion.Alternative     0.2222    0.2222    0.2222         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4803      1039
                  macro avg     0.2962    0.2861    0.2875      1039
               weighted avg     0.4650    0.4803    0.4658      1039

Epoch [8/30]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   5.9,  Val Acc: 61.46%, Val F1: 53.14% Time: 39.65196442604065 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.9,  Val Acc: 49.36%, Val F1: 30.98% Time: 39.65196442604065 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   5.9,  Val Acc: 28.67%, Val F1:  7.56% Time: 39.65196442604065 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 62.83%, Val F1: 53.70% Time: 118.6717939376831 
top-down:SEC: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 81.25%,Val Loss:   6.0,  Val Acc: 49.79%, Val F1: 30.02% Time: 118.6717939376831 
top-down:CONN: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   6.0,  Val Acc: 28.33%, Val F1:  7.89% Time: 118.6717939376831 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.0,  Val Acc: 62.66%, Val F1: 52.78% Time: 198.01855444908142 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 48.84%, Val F1: 30.46% Time: 198.01855444908142 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.0,  Val Acc: 28.24%, Val F1:  7.81% Time: 198.01855444908142 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   6.0,  Val Acc: 61.89%, Val F1: 51.62% Time: 277.0842673778534 
top-down:SEC: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.0,  Val Acc: 47.98%, Val F1: 30.65% Time: 277.0842673778534 
top-down:CONN: Iter:   3100,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   6.0,  Val Acc: 28.41%, Val F1:  7.76% Time: 277.0842673778534 
 
 
Train time usage: 312.15588760375977
Test time usage: 1.7537426948547363
TOP: Test Loss:   6.0,  Test Acc: 61.60%, Test F1: 50.22%
SEC: Test Loss:   6.0,  Test Acc: 46.01%, Test F1: 26.78%
CONN: Test Loss:   6.0,  Test Acc: 22.52%, Test F1:  7.38%
consistency_top_sec: 43.12%,  consistency_sec_conn: 17.42%, consistency_top_sec_conn: 16.46%
              precision    recall  f1-score   support

    Temporal     0.5143    0.2647    0.3495        68
 Contingency     0.5691    0.3787    0.4547       272
  Comparison     0.5339    0.4375    0.4809       144
   Expansion     0.6468    0.8216    0.7238       555

    accuracy                         0.6160      1039
   macro avg     0.5660    0.4756    0.5022      1039
weighted avg     0.6021    0.6160    0.5952      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5600    0.2593    0.3544        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5656    0.4664    0.5112       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4915    0.4567    0.4735       127
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3472    0.6667    0.4566       201
    Expansion.Instantiation     0.6827    0.5966    0.6368       119
      Expansion.Restatement     0.4286    0.3555    0.3886       211
      Expansion.Alternative     0.1429    0.1111    0.1250         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4601      1039
                  macro avg     0.2926    0.2647    0.2678      1039
               weighted avg     0.4687    0.4601    0.4494      1039

Epoch [9/30]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 61.72%, Val F1: 52.55% Time: 44.833123207092285 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 48.76%, Val F1: 30.51% Time: 44.833123207092285 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 28.67%, Val F1:  7.53% Time: 44.833123207092285 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.2,  Val Acc: 62.49%, Val F1: 53.55% Time: 123.87274599075317 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 49.36%, Val F1: 30.98% Time: 123.87274599075317 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 28.67%, Val F1:  7.66% Time: 123.87274599075317 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 62.06%, Val F1: 51.90% Time: 202.85816359519958 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   6.2,  Val Acc: 49.27%, Val F1: 31.47% Time: 202.85816359519958 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   6.2,  Val Acc: 27.47%, Val F1:  7.34% Time: 202.85816359519958 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 61.55%, Val F1: 53.03% Time: 282.1793112754822 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 50.30%, Val F1: 30.71% Time: 282.1793112754822 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   6.2,  Val Acc: 28.67%, Val F1:  8.06% Time: 282.1793112754822 
 
 
Train time usage: 311.72129702568054
Test time usage: 1.7615976333618164
TOP: Test Loss:   6.1,  Test Acc: 60.73%, Test F1: 51.31%
SEC: Test Loss:   6.1,  Test Acc: 46.01%, Test F1: 28.75%
CONN: Test Loss:   6.1,  Test Acc: 23.77%, Test F1:  7.77%
consistency_top_sec: 44.08%,  consistency_sec_conn: 19.35%, consistency_top_sec_conn: 18.58%
              precision    recall  f1-score   support

    Temporal     0.4565    0.3088    0.3684        68
 Contingency     0.5367    0.5092    0.5226       273
  Comparison     0.4737    0.4375    0.4549       144
   Expansion     0.6789    0.7365    0.7065       554

    accuracy                         0.6073      1039
   macro avg     0.5364    0.4980    0.5131      1039
weighted avg     0.5985    0.6073    0.6012      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4318    0.3519    0.3878        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5087    0.5506    0.5288       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4242    0.4375    0.4308       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3911    0.5300    0.4501       200
    Expansion.Instantiation     0.6505    0.5630    0.6036       119
      Expansion.Restatement     0.4270    0.3726    0.3980       212
      Expansion.Alternative     0.3077    0.4444    0.3636         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4601      1039
                  macro avg     0.2856    0.2955    0.2875      1039
               weighted avg     0.4450    0.4601    0.4492      1039

Epoch [10/30]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   6.4,  Val Acc: 60.77%, Val F1: 51.72% Time: 50.83562660217285 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 48.50%, Val F1: 31.13% Time: 50.83562660217285 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 43.75%,Val Loss:   6.4,  Val Acc: 28.50%, Val F1:  7.32% Time: 50.83562660217285 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.5,  Val Acc: 61.46%, Val F1: 51.84% Time: 129.73003101348877 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   6.5,  Val Acc: 48.84%, Val F1: 33.24% Time: 129.73003101348877 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   6.5,  Val Acc: 27.64%, Val F1:  8.50% Time: 129.73003101348877 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   6.4,  Val Acc: 62.83%, Val F1: 53.06% Time: 208.5762369632721 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 50.30%, Val F1: 31.31% Time: 208.5762369632721 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   6.4,  Val Acc: 29.53%, Val F1:  8.16% Time: 208.5762369632721 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.4,  Val Acc: 63.00%, Val F1: 52.73% Time: 287.4310381412506 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.4,  Val Acc: 49.44%, Val F1: 31.61% Time: 287.4310381412506 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   6.4,  Val Acc: 28.33%, Val F1:  8.01% Time: 287.4310381412506 
 
 
Train time usage: 311.79730582237244
Test time usage: 1.7292020320892334
TOP: Test Loss:   6.4,  Test Acc: 60.25%, Test F1: 49.96%
SEC: Test Loss:   6.4,  Test Acc: 45.14%, Test F1: 30.33%
CONN: Test Loss:   6.4,  Test Acc: 22.81%, Test F1:  8.00%
consistency_top_sec: 44.08%,  consistency_sec_conn: 17.90%, consistency_top_sec_conn: 17.90%
              precision    recall  f1-score   support

    Temporal     0.6250    0.2206    0.3261        68
 Contingency     0.5579    0.4945    0.5243       273
  Comparison     0.4379    0.4653    0.4512       144
   Expansion     0.6597    0.7383    0.6968       554

    accuracy                         0.6025      1039
   macro avg     0.5701    0.4797    0.4996      1039
weighted avg     0.5999    0.6025    0.5931      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5833    0.2593    0.3590        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5292    0.5075    0.5181       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.3804    0.4844    0.4261       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3625    0.5600    0.4401       200
    Expansion.Instantiation     0.6699    0.5798    0.6216       119
      Expansion.Restatement     0.4268    0.3318    0.3733       211
      Expansion.Alternative     0.2857    0.4444    0.3478         9
             Expansion.List     0.5000    0.1667    0.2500        12

                   accuracy                         0.4514      1039
                  macro avg     0.3398    0.3031    0.3033      1039
               weighted avg     0.4551    0.4514    0.4424      1039

Epoch [11/30]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 93.75%,Val Loss:   6.6,  Val Acc: 63.09%, Val F1: 53.68% Time: 56.05389142036438 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 75.00%,Val Loss:   6.6,  Val Acc: 49.18%, Val F1: 30.24% Time: 56.05389142036438 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 46.88%,Val Loss:   6.6,  Val Acc: 28.67%, Val F1:  8.07% Time: 56.05389142036438 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   6.6,  Val Acc: 63.78%, Val F1: 53.85% Time: 136.86036658287048 *
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   6.6,  Val Acc: 49.53%, Val F1: 31.72% Time: 136.86036658287048 *
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 43.75%,Val Loss:   6.6,  Val Acc: 29.61%, Val F1:  8.74% Time: 136.86036658287048 *
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.5,  Val Acc: 63.26%, Val F1: 53.34% Time: 215.78660941123962 
top-down:SEC: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   6.5,  Val Acc: 49.36%, Val F1: 31.29% Time: 215.78660941123962 
top-down:CONN: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   6.5,  Val Acc: 29.18%, Val F1:  8.86% Time: 215.78660941123962 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.5,  Val Acc: 62.83%, Val F1: 52.46% Time: 294.55013036727905 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.5,  Val Acc: 49.61%, Val F1: 32.69% Time: 294.55013036727905 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   6.5,  Val Acc: 28.15%, Val F1:  7.77% Time: 294.55013036727905 
 
 
Train time usage: 313.223694562912
Test time usage: 1.7482731342315674
TOP: Test Loss:   6.5,  Test Acc: 61.31%, Test F1: 51.95%
SEC: Test Loss:   6.5,  Test Acc: 47.55%, Test F1: 30.99%
CONN: Test Loss:   6.5,  Test Acc: 23.48%, Test F1:  8.03%
consistency_top_sec: 46.20%,  consistency_sec_conn: 19.25%, consistency_top_sec_conn: 18.77%
              precision    recall  f1-score   support

    Temporal     0.3898    0.3382    0.3622        68
 Contingency     0.5486    0.5165    0.5321       273
  Comparison     0.4818    0.4583    0.4698       144
   Expansion     0.6945    0.7347    0.7140       554

    accuracy                         0.6131      1039
   macro avg     0.5287    0.5119    0.5195      1039
weighted avg     0.6068    0.6131    0.6093      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4286    0.3889    0.4078        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5248    0.5543    0.5392       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4307    0.4609    0.4453       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4115    0.5350    0.4652       200
    Expansion.Instantiation     0.6762    0.5966    0.6339       119
      Expansion.Restatement     0.4560    0.3915    0.4213       212
      Expansion.Alternative     0.2500    0.3333    0.2857         9
             Expansion.List     0.2857    0.1667    0.2105        12

                   accuracy                         0.4755      1039
                  macro avg     0.3149    0.3116    0.3099      1039
               weighted avg     0.4654    0.4755    0.4676      1039

Epoch [12/30]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 61.89%, Val F1: 52.80% Time: 60.88062071800232 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 49.61%, Val F1: 31.34% Time: 60.88062071800232 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   6.7,  Val Acc: 28.58%, Val F1:  8.27% Time: 60.88062071800232 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 61.55%, Val F1: 52.00% Time: 139.82777047157288 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   6.8,  Val Acc: 50.21%, Val F1: 35.12% Time: 139.82777047157288 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.8,  Val Acc: 28.24%, Val F1:  7.68% Time: 139.82777047157288 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 60.77%, Val F1: 49.44% Time: 218.74288034439087 
top-down:SEC: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   6.8,  Val Acc: 48.24%, Val F1: 30.96% Time: 218.74288034439087 
top-down:CONN: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   6.8,  Val Acc: 28.24%, Val F1:  8.12% Time: 218.74288034439087 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 62.92%, Val F1: 51.41% Time: 297.95979714393616 
top-down:SEC: Iter:   4700,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 49.10%, Val F1: 31.12% Time: 297.95979714393616 
top-down:CONN: Iter:   4700,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   6.7,  Val Acc: 29.44%, Val F1:  8.82% Time: 297.95979714393616 
 
 
Train time usage: 311.38280391693115
Test time usage: 1.7337298393249512
TOP: Test Loss:   6.7,  Test Acc: 59.77%, Test F1: 50.39%
SEC: Test Loss:   6.7,  Test Acc: 46.58%, Test F1: 31.84%
CONN: Test Loss:   6.7,  Test Acc: 23.20%, Test F1:  8.61%
consistency_top_sec: 45.14%,  consistency_sec_conn: 18.96%, consistency_top_sec_conn: 18.58%
              precision    recall  f1-score   support

    Temporal     0.4082    0.2941    0.3419        68
 Contingency     0.5252    0.5348    0.5299       273
  Comparison     0.4632    0.4375    0.4500       144
   Expansion     0.6806    0.7076    0.6938       554

    accuracy                         0.5977      1039
   macro avg     0.5193    0.4935    0.5039      1039
weighted avg     0.5918    0.5977    0.5939      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4091    0.3333    0.3673        54
         Temporal.Synchrony     0.2500    0.0714    0.1111        14
          Contingency.Cause     0.5016    0.5730    0.5350       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4214    0.4609    0.4403       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4320    0.4450    0.4384       200
    Expansion.Instantiation     0.6381    0.5630    0.5982       119
      Expansion.Restatement     0.4144    0.4340    0.4240       212
      Expansion.Alternative     0.3750    0.3333    0.3529         9
             Expansion.List     0.4000    0.1667    0.2353        12

                   accuracy                         0.4658      1039
                  macro avg     0.3492    0.3073    0.3184      1039
               weighted avg     0.4541    0.4658    0.4575      1039

Epoch [13/30]
top-down:TOP: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 62.66%, Val F1: 53.46% Time: 66.76831459999084 
top-down:SEC: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 49.87%, Val F1: 32.23% Time: 66.76831459999084 
top-down:CONN: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 62.50%,Val Loss:   6.9,  Val Acc: 29.87%, Val F1:  9.09% Time: 66.76831459999084 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.0,  Val Acc: 61.72%, Val F1: 51.93% Time: 146.08078408241272 
top-down:SEC: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 81.25%,Val Loss:   7.0,  Val Acc: 48.84%, Val F1: 31.02% Time: 146.08078408241272 
top-down:CONN: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   7.0,  Val Acc: 29.36%, Val F1:  8.22% Time: 146.08078408241272 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.1,  Val Acc: 62.06%, Val F1: 52.32% Time: 225.44103598594666 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 48.84%, Val F1: 30.61% Time: 225.44103598594666 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   7.1,  Val Acc: 27.81%, Val F1:  8.07% Time: 225.44103598594666 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 62.06%, Val F1: 52.07% Time: 304.5030279159546 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   6.9,  Val Acc: 49.79%, Val F1: 32.34% Time: 304.5030279159546 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.9,  Val Acc: 29.36%, Val F1:  8.11% Time: 304.5030279159546 
 
 
Train time usage: 312.45978355407715
Test time usage: 1.763850450515747
TOP: Test Loss:   6.8,  Test Acc: 60.73%, Test F1: 51.96%
SEC: Test Loss:   6.8,  Test Acc: 46.78%, Test F1: 32.22%
CONN: Test Loss:   6.8,  Test Acc: 22.91%, Test F1:  8.12%
consistency_top_sec: 45.52%,  consistency_sec_conn: 18.48%, consistency_top_sec_conn: 18.09%
              precision    recall  f1-score   support

    Temporal     0.4600    0.3382    0.3898        68
 Contingency     0.5308    0.5074    0.5188       272
  Comparison     0.4885    0.4444    0.4655       144
   Expansion     0.6789    0.7315    0.7042       555

    accuracy                         0.6073      1039
   macro avg     0.5396    0.5054    0.5196      1039
weighted avg     0.5994    0.6073    0.6020      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4468    0.3889    0.4158        54
         Temporal.Synchrony     0.2500    0.0714    0.1111        14
          Contingency.Cause     0.5238    0.5356    0.5296       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4485    0.4766    0.4621       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3936    0.5550    0.4606       200
    Expansion.Instantiation     0.6566    0.5462    0.5963       119
      Expansion.Restatement     0.4382    0.3679    0.4000       212
      Expansion.Alternative     0.2667    0.4444    0.3333         9
             Expansion.List     0.4000    0.1667    0.2353        12

                   accuracy                         0.4678      1039
                  macro avg     0.3477    0.3230    0.3222      1039
               weighted avg     0.4638    0.4678    0.4603      1039

Epoch [14/30]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   7.0,  Val Acc: 62.06%, Val F1: 51.34% Time: 71.98026323318481 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 49.27%, Val F1: 31.34% Time: 71.98026323318481 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   7.0,  Val Acc: 28.33%, Val F1:  8.19% Time: 71.98026323318481 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.0,  Val Acc: 63.18%, Val F1: 52.29% Time: 151.0008647441864 
top-down:SEC: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   7.0,  Val Acc: 49.18%, Val F1: 30.32% Time: 151.0008647441864 
top-down:CONN: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   7.0,  Val Acc: 28.15%, Val F1:  7.82% Time: 151.0008647441864 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 100.00%,Val Loss:   7.1,  Val Acc: 62.75%, Val F1: 52.08% Time: 230.15236067771912 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 49.01%, Val F1: 30.99% Time: 230.15236067771912 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   7.1,  Val Acc: 28.07%, Val F1:  8.21% Time: 230.15236067771912 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   7.1,  Val Acc: 62.40%, Val F1: 51.59% Time: 309.2085130214691 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 49.10%, Val F1: 31.31% Time: 309.2085130214691 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 27.38%, Val F1:  7.92% Time: 309.2085130214691 
 
 
Train time usage: 311.8514668941498
Test time usage: 1.7538678646087646
TOP: Test Loss:   7.0,  Test Acc: 62.85%, Test F1: 52.50%
SEC: Test Loss:   7.0,  Test Acc: 46.68%, Test F1: 33.71%
CONN: Test Loss:   7.0,  Test Acc: 23.00%, Test F1:  8.13%
consistency_top_sec: 45.52%,  consistency_sec_conn: 18.96%, consistency_top_sec_conn: 18.67%
              precision    recall  f1-score   support

    Temporal     0.4561    0.3824    0.4160        68
 Contingency     0.5803    0.4118    0.4817       272
  Comparison     0.5644    0.3958    0.4653       144
   Expansion     0.6657    0.8252    0.7369       555

    accuracy                         0.6285      1039
   macro avg     0.5666    0.5038    0.5250      1039
weighted avg     0.6156    0.6285    0.6115      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4444    0.4444    0.4444        54
         Temporal.Synchrony     0.5000    0.0714    0.1250        14
          Contingency.Cause     0.5616    0.4254    0.4841       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4907    0.4141    0.4492       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3687    0.6250    0.4638       200
    Expansion.Instantiation     0.7128    0.5630    0.6291       119
      Expansion.Restatement     0.4215    0.4455    0.4332       211
      Expansion.Alternative     0.4615    0.6667    0.5455         9
             Expansion.List     0.3333    0.0833    0.1333        12

                   accuracy                         0.4668      1039
                  macro avg     0.3904    0.3399    0.3371      1039
               weighted avg     0.4812    0.4668    0.4606      1039

Epoch [15/30]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 62.32%, Val F1: 51.72% Time: 77.61061453819275 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 49.79%, Val F1: 30.49% Time: 77.61061453819275 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   7.2,  Val Acc: 27.90%, Val F1:  8.05% Time: 77.61061453819275 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.2,  Val Acc: 62.40%, Val F1: 52.51% Time: 156.59209728240967 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 50.13%, Val F1: 32.30% Time: 156.59209728240967 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   7.2,  Val Acc: 27.12%, Val F1:  7.88% Time: 156.59209728240967 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 62.06%, Val F1: 50.85% Time: 235.48992824554443 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 49.53%, Val F1: 30.53% Time: 235.48992824554443 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.2,  Val Acc: 29.18%, Val F1:  9.20% Time: 235.48992824554443 
 
 
Train time usage: 309.6161148548126
Test time usage: 1.7527508735656738
TOP: Test Loss:   7.0,  Test Acc: 62.37%, Test F1: 52.26%
SEC: Test Loss:   7.0,  Test Acc: 47.83%, Test F1: 31.59%
CONN: Test Loss:   7.0,  Test Acc: 22.71%, Test F1:  8.09%
consistency_top_sec: 46.01%,  consistency_sec_conn: 19.54%, consistency_top_sec_conn: 19.15%
              precision    recall  f1-score   support

    Temporal     0.5128    0.2941    0.3738        68
 Contingency     0.5326    0.5404    0.5365       272
  Comparison     0.5600    0.3889    0.4590       144
   Expansion     0.6811    0.7658    0.7209       555

    accuracy                         0.6237      1039
   macro avg     0.5716    0.4973    0.5226      1039
weighted avg     0.6144    0.6237    0.6136      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4667    0.2593    0.3333        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5154    0.5655    0.5393       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5050    0.3984    0.4454       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3878    0.6050    0.4727       200
    Expansion.Instantiation     0.7529    0.5378    0.6275       119
      Expansion.Restatement     0.4569    0.4245    0.4401       212
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.4000    0.1667    0.2353        12

                   accuracy                         0.4783      1039
                  macro avg     0.3471    0.3092    0.3159      1039
               weighted avg     0.4805    0.4783    0.4694      1039

Epoch [16/30]
top-down:TOP: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 61.03%, Val F1: 50.03% Time: 5.708743572235107 
top-down:SEC: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   7.3,  Val Acc: 48.67%, Val F1: 30.36% Time: 5.708743572235107 
top-down:CONN: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   7.3,  Val Acc: 27.12%, Val F1:  7.83% Time: 5.708743572235107 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 61.12%, Val F1: 51.50% Time: 84.86884570121765 
top-down:SEC: Iter:   6000,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 48.33%, Val F1: 31.89% Time: 84.86884570121765 
top-down:CONN: Iter:   6000,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   7.4,  Val Acc: 27.55%, Val F1:  8.15% Time: 84.86884570121765 
 
 
top-down:TOP: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 60.60%, Val F1: 51.15% Time: 164.09077739715576 
top-down:SEC: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 49.44%, Val F1: 30.83% Time: 164.09077739715576 
top-down:CONN: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 59.38%,Val Loss:   7.3,  Val Acc: 29.27%, Val F1:  8.22% Time: 164.09077739715576 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 61.89%, Val F1: 52.90% Time: 243.0220980644226 
top-down:SEC: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.1,  Val Acc: 49.36%, Val F1: 32.09% Time: 243.0220980644226 
top-down:CONN: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   7.1,  Val Acc: 27.64%, Val F1:  8.13% Time: 243.0220980644226 
 
 
Train time usage: 312.0123815536499
Test time usage: 1.7206532955169678
TOP: Test Loss:   7.1,  Test Acc: 61.31%, Test F1: 52.94%
SEC: Test Loss:   7.1,  Test Acc: 48.12%, Test F1: 32.54%
CONN: Test Loss:   7.1,  Test Acc: 22.62%, Test F1:  7.78%
consistency_top_sec: 46.20%,  consistency_sec_conn: 18.96%, consistency_top_sec_conn: 18.48%
              precision    recall  f1-score   support

    Temporal     0.3968    0.3676    0.3817        68
 Contingency     0.5350    0.5625    0.5484       272
  Comparison     0.5000    0.4653    0.4820       144
   Expansion     0.7050    0.7063    0.7057       555

    accuracy                         0.6131      1039
   macro avg     0.5342    0.5254    0.5294      1039
weighted avg     0.6119    0.6131    0.6123      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4630    0.4630    0.4630        54
         Temporal.Synchrony     0.2000    0.0714    0.1053        14
          Contingency.Cause     0.5110    0.6067    0.5548       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4593    0.4844    0.4715       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4091    0.4950    0.4480       200
    Expansion.Instantiation     0.7412    0.5294    0.6176       119
      Expansion.Restatement     0.4536    0.3915    0.4203       212
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4812      1039
                  macro avg     0.3428    0.3245    0.3254      1039
               weighted avg     0.4760    0.4812    0.4735      1039

Epoch [17/30]
top-down:TOP: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 61.63%, Val F1: 51.71% Time: 11.166731357574463 
top-down:SEC: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 50.30%, Val F1: 33.17% Time: 11.166731357574463 
top-down:CONN: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   7.3,  Val Acc: 28.67%, Val F1:  8.05% Time: 11.166731357574463 
 
 
top-down:TOP: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.5,  Val Acc: 60.52%, Val F1: 51.70% Time: 90.1760094165802 
top-down:SEC: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 84.38%,Val Loss:   7.5,  Val Acc: 48.15%, Val F1: 31.45% Time: 90.1760094165802 
top-down:CONN: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   7.5,  Val Acc: 27.55%, Val F1:  8.72% Time: 90.1760094165802 
 
 
top-down:TOP: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 60.69%, Val F1: 49.45% Time: 169.21418046951294 
top-down:SEC: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 47.98%, Val F1: 31.34% Time: 169.21418046951294 
top-down:CONN: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   7.3,  Val Acc: 27.30%, Val F1:  7.90% Time: 169.21418046951294 
 
 
top-down:TOP: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.4,  Val Acc: 62.15%, Val F1: 51.06% Time: 248.09002661705017 
top-down:SEC: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 48.76%, Val F1: 30.46% Time: 248.09002661705017 
top-down:CONN: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   7.4,  Val Acc: 27.73%, Val F1:  8.01% Time: 248.09002661705017 
 
 
Train time usage: 311.69806385040283
Test time usage: 1.7398216724395752
TOP: Test Loss:   7.2,  Test Acc: 61.31%, Test F1: 52.69%
SEC: Test Loss:   7.2,  Test Acc: 46.97%, Test F1: 31.55%
CONN: Test Loss:   7.2,  Test Acc: 22.23%, Test F1:  8.26%
consistency_top_sec: 45.81%,  consistency_sec_conn: 18.38%, consistency_top_sec_conn: 18.19%
              precision    recall  f1-score   support

    Temporal     0.4490    0.3235    0.3761        68
 Contingency     0.5142    0.5971    0.5525       273
  Comparison     0.5345    0.4306    0.4769       144
   Expansion     0.7002    0.7040    0.7021       554

    accuracy                         0.6131      1039
   macro avg     0.5495    0.5138    0.5269      1039
weighted avg     0.6119    0.6131    0.6102      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4750    0.3519    0.4043        54
         Temporal.Synchrony     0.3333    0.0714    0.1176        14
          Contingency.Cause     0.5031    0.6119    0.5522       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4907    0.4141    0.4492       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3810    0.5600    0.4534       200
    Expansion.Instantiation     0.7273    0.5378    0.6184       119
      Expansion.Restatement     0.4375    0.3318    0.3774       211
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4697      1039
                  macro avg     0.3528    0.3097    0.3155      1039
               weighted avg     0.4701    0.4697    0.4598      1039

Epoch [18/30]
top-down:TOP: Iter:   6700,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 61.46%, Val F1: 50.13% Time: 16.528985023498535 
top-down:SEC: Iter:   6700,  Train Loss: 2.4e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 48.58%, Val F1: 31.04% Time: 16.528985023498535 
top-down:CONN: Iter:   6700,  Train Loss: 2.4e+01,  Train Acc: 62.50%,Val Loss:   7.2,  Val Acc: 26.95%, Val F1:  7.95% Time: 16.528985023498535 
 
 
top-down:TOP: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.4,  Val Acc: 62.23%, Val F1: 52.29% Time: 95.66693043708801 
top-down:SEC: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 49.01%, Val F1: 30.51% Time: 95.66693043708801 
top-down:CONN: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   7.4,  Val Acc: 27.55%, Val F1:  8.36% Time: 95.66693043708801 
 
 
top-down:TOP: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 61.55%, Val F1: 52.00% Time: 174.49153065681458 
top-down:SEC: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 93.75%,Val Loss:   7.5,  Val Acc: 48.15%, Val F1: 31.17% Time: 174.49153065681458 
top-down:CONN: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 68.75%,Val Loss:   7.5,  Val Acc: 27.98%, Val F1:  8.22% Time: 174.49153065681458 
 
 
top-down:TOP: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 61.12%, Val F1: 49.99% Time: 253.32109999656677 
top-down:SEC: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   7.6,  Val Acc: 48.76%, Val F1: 32.02% Time: 253.32109999656677 
top-down:CONN: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 59.38%,Val Loss:   7.6,  Val Acc: 27.04%, Val F1:  8.26% Time: 253.32109999656677 
 
 
Train time usage: 311.379821062088
Test time usage: 1.7398595809936523
TOP: Test Loss:   7.4,  Test Acc: 62.56%, Test F1: 53.01%
SEC: Test Loss:   7.4,  Test Acc: 47.26%, Test F1: 31.82%
CONN: Test Loss:   7.4,  Test Acc: 20.60%, Test F1:  7.86%
consistency_top_sec: 45.72%,  consistency_sec_conn: 16.84%, consistency_top_sec_conn: 16.46%
              precision    recall  f1-score   support

    Temporal     0.4800    0.3529    0.4068        68
 Contingency     0.5636    0.4872    0.5226       273
  Comparison     0.5263    0.4167    0.4651       144
   Expansion     0.6776    0.7816    0.7259       554

    accuracy                         0.6256      1039
   macro avg     0.5619    0.5096    0.5301      1039
weighted avg     0.6137    0.6256    0.6155      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4773    0.3889    0.4286        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5367    0.5187    0.5275       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4909    0.4219    0.4538       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3883    0.5650    0.4603       200
    Expansion.Instantiation     0.7083    0.5714    0.6326       119
      Expansion.Restatement     0.4245    0.4265    0.4255       211
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4726      1039
                  macro avg     0.3282    0.3210    0.3182      1039
               weighted avg     0.4714    0.4726    0.4670      1039

Epoch [19/30]
top-down:TOP: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 60.69%, Val F1: 51.06% Time: 21.87332034111023 
top-down:SEC: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 48.24%, Val F1: 31.34% Time: 21.87332034111023 
top-down:CONN: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   7.6,  Val Acc: 26.95%, Val F1:  7.98% Time: 21.87332034111023 
 
 
top-down:TOP: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 61.46%, Val F1: 50.82% Time: 100.68135595321655 
top-down:SEC: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 84.38%,Val Loss:   7.6,  Val Acc: 48.33%, Val F1: 31.44% Time: 100.68135595321655 
top-down:CONN: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 62.50%,Val Loss:   7.6,  Val Acc: 27.64%, Val F1:  8.26% Time: 100.68135595321655 
 
 
top-down:TOP: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.29%, Val F1: 51.42% Time: 179.74233102798462 
top-down:SEC: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 48.76%, Val F1: 31.11% Time: 179.74233102798462 
top-down:CONN: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   7.7,  Val Acc: 28.41%, Val F1:  8.20% Time: 179.74233102798462 
 
 
top-down:TOP: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 60.77%, Val F1: 49.70% Time: 258.6683540344238 
top-down:SEC: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.6,  Val Acc: 48.67%, Val F1: 31.23% Time: 258.6683540344238 
top-down:CONN: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   7.6,  Val Acc: 27.81%, Val F1:  8.08% Time: 258.6683540344238 
 
 
Train time usage: 311.4588294029236
Test time usage: 1.7504794597625732
TOP: Test Loss:   7.5,  Test Acc: 60.64%, Test F1: 52.37%
SEC: Test Loss:   7.5,  Test Acc: 47.35%, Test F1: 31.00%
CONN: Test Loss:   7.5,  Test Acc: 22.23%, Test F1:  8.09%
consistency_top_sec: 46.10%,  consistency_sec_conn: 18.48%, consistency_top_sec_conn: 18.09%
              precision    recall  f1-score   support

    Temporal     0.5122    0.3088    0.3853        68
 Contingency     0.5032    0.5824    0.5399       273
  Comparison     0.4853    0.4583    0.4714       144
   Expansion     0.7033    0.6931    0.6982       554

    accuracy                         0.6064      1039
   macro avg     0.5510    0.5107    0.5237      1039
weighted avg     0.6080    0.6064    0.6047      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3333    0.4000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4879    0.6007    0.5385       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4511    0.4688    0.4598       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3971    0.5400    0.4576       200
    Expansion.Instantiation     0.7303    0.5462    0.6250       119
      Expansion.Restatement     0.4747    0.3555    0.4065       211
      Expansion.Alternative     0.2727    0.3333    0.3000         9
             Expansion.List     0.3333    0.1667    0.2222        12

                   accuracy                         0.4735      1039
                  macro avg     0.3316    0.3040    0.3100      1039
               weighted avg     0.4701    0.4735    0.4637      1039

Epoch [20/30]
top-down:TOP: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 61.29%, Val F1: 50.36% Time: 27.21367597579956 
top-down:SEC: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 48.58%, Val F1: 31.77% Time: 27.21367597579956 
top-down:CONN: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   7.6,  Val Acc: 28.07%, Val F1:  8.20% Time: 27.21367597579956 
 
 
top-down:TOP: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 62.49%, Val F1: 50.78% Time: 106.58325290679932 
top-down:SEC: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 47.47%, Val F1: 31.41% Time: 106.58325290679932 
top-down:CONN: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 27.21%, Val F1:  8.64% Time: 106.58325290679932 
 
 
top-down:TOP: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.89%, Val F1: 50.67% Time: 185.51219415664673 
top-down:SEC: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 48.84%, Val F1: 31.22% Time: 185.51219415664673 
top-down:CONN: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   7.7,  Val Acc: 28.07%, Val F1:  8.45% Time: 185.51219415664673 
 
 
top-down:TOP: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 60.86%, Val F1: 51.09% Time: 264.6793417930603 
top-down:SEC: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 49.18%, Val F1: 31.92% Time: 264.6793417930603 
top-down:CONN: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   7.7,  Val Acc: 27.38%, Val F1:  8.38% Time: 264.6793417930603 
 
 
Train time usage: 311.8739159107208
Test time usage: 1.723731517791748
TOP: Test Loss:   7.5,  Test Acc: 61.69%, Test F1: 51.76%
SEC: Test Loss:   7.5,  Test Acc: 46.87%, Test F1: 30.97%
CONN: Test Loss:   7.5,  Test Acc: 21.66%, Test F1:  7.91%
consistency_top_sec: 45.62%,  consistency_sec_conn: 17.71%, consistency_top_sec_conn: 17.52%
              precision    recall  f1-score   support

    Temporal     0.4750    0.2794    0.3519        68
 Contingency     0.5447    0.5147    0.5293       272
  Comparison     0.5000    0.4514    0.4745       144
   Expansion     0.6814    0.7514    0.7147       555

    accuracy                         0.6169      1039
   macro avg     0.5503    0.4992    0.5176      1039
weighted avg     0.6070    0.6169    0.6091      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4848    0.2963    0.3678        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5196    0.5448    0.5319       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4488    0.4453    0.4471       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3887    0.5500    0.4555       200
    Expansion.Instantiation     0.7317    0.5042    0.5970       119
      Expansion.Restatement     0.4340    0.4360    0.4350       211
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4687      1039
                  macro avg     0.3266    0.3105    0.3097      1039
               weighted avg     0.4669    0.4687    0.4611      1039

Epoch [21/30]
top-down:TOP: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 60.77%, Val F1: 49.83% Time: 32.78033208847046 
top-down:SEC: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 48.41%, Val F1: 31.36% Time: 32.78033208847046 
top-down:CONN: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 71.88%,Val Loss:   7.7,  Val Acc: 26.95%, Val F1:  8.50% Time: 32.78033208847046 
 
 
top-down:TOP: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.55%, Val F1: 50.40% Time: 111.67107629776001 
top-down:SEC: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   7.7,  Val Acc: 47.55%, Val F1: 30.39% Time: 111.67107629776001 
top-down:CONN: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   7.7,  Val Acc: 26.87%, Val F1:  8.19% Time: 111.67107629776001 
 
 
top-down:TOP: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 60.69%, Val F1: 50.17% Time: 190.82392072677612 
top-down:SEC: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 48.07%, Val F1: 30.76% Time: 190.82392072677612 
top-down:CONN: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   7.7,  Val Acc: 27.30%, Val F1:  8.61% Time: 190.82392072677612 
 
 
top-down:TOP: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 60.77%, Val F1: 50.85% Time: 269.8293960094452 
top-down:SEC: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   7.6,  Val Acc: 48.84%, Val F1: 31.49% Time: 269.8293960094452 
top-down:CONN: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   7.6,  Val Acc: 26.87%, Val F1:  8.03% Time: 269.8293960094452 
 
 
Train time usage: 311.77319717407227
Test time usage: 1.7528176307678223
TOP: Test Loss:   7.5,  Test Acc: 60.15%, Test F1: 49.74%
SEC: Test Loss:   7.5,  Test Acc: 47.16%, Test F1: 31.08%
CONN: Test Loss:   7.5,  Test Acc: 22.71%, Test F1:  8.53%
consistency_top_sec: 45.91%,  consistency_sec_conn: 18.58%, consistency_top_sec_conn: 18.48%
              precision    recall  f1-score   support

    Temporal     0.4167    0.2206    0.2885        68
 Contingency     0.5130    0.5788    0.5439       273
  Comparison     0.4812    0.4444    0.4621       144
   Expansion     0.6904    0.7004    0.6953       554

    accuracy                         0.6015      1039
   macro avg     0.5253    0.4860    0.4974      1039
weighted avg     0.5969    0.6015    0.5966      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4828    0.2593    0.3373        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5047    0.6045    0.5501       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4275    0.4375    0.4324       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4015    0.5300    0.4569       200
    Expansion.Instantiation     0.7083    0.5714    0.6326       119
      Expansion.Restatement     0.4457    0.3697    0.4041       211
      Expansion.Alternative     0.4545    0.5556    0.5000         9
             Expansion.List     0.1429    0.0833    0.1053        12

                   accuracy                         0.4716      1039
                  macro avg     0.3244    0.3101    0.3108      1039
               weighted avg     0.4624    0.4716    0.4607      1039

Epoch [22/30]
top-down:TOP: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 60.77%, Val F1: 50.47% Time: 38.18179368972778 
top-down:SEC: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   7.8,  Val Acc: 48.33%, Val F1: 31.03% Time: 38.18179368972778 
top-down:CONN: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 62.50%,Val Loss:   7.8,  Val Acc: 26.70%, Val F1:  8.07% Time: 38.18179368972778 
 
 
top-down:TOP: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 60.69%, Val F1: 50.84% Time: 117.19859862327576 
top-down:SEC: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 48.84%, Val F1: 31.85% Time: 117.19859862327576 
top-down:CONN: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   7.7,  Val Acc: 27.12%, Val F1:  8.47% Time: 117.19859862327576 
 
 
top-down:TOP: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 61.80%, Val F1: 49.78% Time: 196.22893714904785 
top-down:SEC: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 48.41%, Val F1: 31.69% Time: 196.22893714904785 
top-down:CONN: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   7.7,  Val Acc: 26.95%, Val F1:  8.90% Time: 196.22893714904785 
 
 
top-down:TOP: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.7,  Val Acc: 62.15%, Val F1: 51.96% Time: 275.4357388019562 
top-down:SEC: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   7.7,  Val Acc: 48.93%, Val F1: 31.68% Time: 275.4357388019562 
top-down:CONN: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 27.38%, Val F1:  8.70% Time: 275.4357388019562 
 
 
Train time usage: 311.9824616909027
Test time usage: 1.753096103668213
TOP: Test Loss:   7.6,  Test Acc: 61.02%, Test F1: 51.45%
SEC: Test Loss:   7.6,  Test Acc: 45.62%, Test F1: 30.20%
CONN: Test Loss:   7.6,  Test Acc: 19.92%, Test F1:  7.86%
consistency_top_sec: 44.08%,  consistency_sec_conn: 15.98%, consistency_top_sec_conn: 15.69%
              precision    recall  f1-score   support

    Temporal     0.4872    0.2794    0.3551        68
 Contingency     0.5430    0.5092    0.5255       273
  Comparison     0.4658    0.4722    0.4690       144
   Expansion     0.6823    0.7365    0.7083       554

    accuracy                         0.6102      1039
   macro avg     0.5445    0.4993    0.5145      1039
weighted avg     0.6029    0.6102    0.6040      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4595    0.3148    0.3736        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5089    0.5336    0.5209       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4097    0.4609    0.4338       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3835    0.5100    0.4378       200
    Expansion.Instantiation     0.6701    0.5462    0.6019       119
      Expansion.Restatement     0.4278    0.3934    0.4099       211
      Expansion.Alternative     0.3333    0.3333    0.3333         9
             Expansion.List     0.2857    0.1667    0.2105        12

                   accuracy                         0.4562      1039
                  macro avg     0.3162    0.2963    0.3020      1039
               weighted avg     0.4493    0.4562    0.4490      1039

Epoch [23/30]
top-down:TOP: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.12%, Val F1: 50.59% Time: 43.51583933830261 
top-down:SEC: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 48.67%, Val F1: 31.45% Time: 43.51583933830261 
top-down:CONN: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   7.7,  Val Acc: 27.30%, Val F1:  8.67% Time: 43.51583933830261 
 
 
top-down:TOP: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 61.97%, Val F1: 51.55% Time: 122.263592004776 
top-down:SEC: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 48.50%, Val F1: 31.01% Time: 122.263592004776 
top-down:CONN: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   7.8,  Val Acc: 27.90%, Val F1:  8.81% Time: 122.263592004776 
 
 
top-down:TOP: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 60.60%, Val F1: 49.45% Time: 201.13204097747803 
top-down:SEC: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 47.98%, Val F1: 30.66% Time: 201.13204097747803 
top-down:CONN: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   7.8,  Val Acc: 26.95%, Val F1:  8.16% Time: 201.13204097747803 
 
 
top-down:TOP: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 61.46%, Val F1: 50.41% Time: 280.1303286552429 
top-down:SEC: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 47.55%, Val F1: 30.83% Time: 280.1303286552429 
top-down:CONN: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   7.8,  Val Acc: 27.21%, Val F1:  8.31% Time: 280.1303286552429 
 
 
Train time usage: 311.20141196250916
Test time usage: 1.754223108291626
TOP: Test Loss:   7.6,  Test Acc: 61.41%, Test F1: 50.94%
SEC: Test Loss:   7.6,  Test Acc: 45.81%, Test F1: 30.39%
CONN: Test Loss:   7.6,  Test Acc: 22.14%, Test F1:  8.52%
consistency_top_sec: 44.66%,  consistency_sec_conn: 18.09%, consistency_top_sec_conn: 18.00%
              precision    recall  f1-score   support

    Temporal     0.4872    0.2794    0.3551        68
 Contingency     0.5348    0.5348    0.5348       273
  Comparison     0.4870    0.3889    0.4324       144
   Expansion     0.6814    0.7527    0.7153       554

    accuracy                         0.6141      1039
   macro avg     0.5476    0.4890    0.5094      1039
weighted avg     0.6032    0.6141    0.6051      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4474    0.3148    0.3696        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5139    0.5522    0.5324       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4237    0.3906    0.4065       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3875    0.5600    0.4581       200
    Expansion.Instantiation     0.6889    0.5210    0.5933       119
      Expansion.Restatement     0.4219    0.3839    0.4020       211
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.2500    0.1667    0.2000        12

                   accuracy                         0.4581      1039
                  macro avg     0.3151    0.3031    0.3039      1039
               weighted avg     0.4530    0.4581    0.4500      1039

Epoch [24/30]
top-down:TOP: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 61.29%, Val F1: 50.85% Time: 48.77640962600708 
top-down:SEC: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 47.21%, Val F1: 30.31% Time: 48.77640962600708 
top-down:CONN: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 62.50%,Val Loss:   7.8,  Val Acc: 26.87%, Val F1:  8.25% Time: 48.77640962600708 
 
 
top-down:TOP: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 60.43%, Val F1: 49.43% Time: 127.6385247707367 
top-down:SEC: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 46.87%, Val F1: 30.34% Time: 127.6385247707367 
top-down:CONN: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 59.38%,Val Loss:   7.9,  Val Acc: 26.27%, Val F1:  8.45% Time: 127.6385247707367 
 
 
top-down:TOP: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 61.37%, Val F1: 50.04% Time: 206.22411632537842 
top-down:SEC: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 47.38%, Val F1: 30.64% Time: 206.22411632537842 
top-down:CONN: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   7.8,  Val Acc: 26.18%, Val F1:  8.00% Time: 206.22411632537842 
 
 
top-down:TOP: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.46%, Val F1: 50.78% Time: 285.33788990974426 
top-down:SEC: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 48.41%, Val F1: 31.38% Time: 285.33788990974426 
top-down:CONN: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 59.38%,Val Loss:   7.9,  Val Acc: 26.44%, Val F1:  7.82% Time: 285.33788990974426 
 
 
Train time usage: 311.12761211395264
Test time usage: 1.7427494525909424
TOP: Test Loss:   7.8,  Test Acc: 61.12%, Test F1: 51.47%
SEC: Test Loss:   7.8,  Test Acc: 46.10%, Test F1: 29.08%
CONN: Test Loss:   7.8,  Test Acc: 21.66%, Test F1:  8.10%
consistency_top_sec: 44.66%,  consistency_sec_conn: 17.71%, consistency_top_sec_conn: 17.52%
              precision    recall  f1-score   support

    Temporal     0.4545    0.2941    0.3571        68
 Contingency     0.5428    0.5348    0.5387       273
  Comparison     0.4737    0.4375    0.4549       144
   Expansion     0.6847    0.7329    0.7079       554

    accuracy                         0.6112      1039
   macro avg     0.5389    0.4998    0.5147      1039
weighted avg     0.6031    0.6112    0.6054      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4390    0.3333    0.3789        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5213    0.5485    0.5345       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4394    0.4531    0.4462       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3693    0.5300    0.4353       200
    Expansion.Instantiation     0.6979    0.5630    0.6233       119
      Expansion.Restatement     0.4293    0.3744    0.4000       211
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4610      1039
                  macro avg     0.2936    0.2952    0.2908      1039
               weighted avg     0.4525    0.4610    0.4523      1039

Epoch [25/30]
top-down:TOP: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 61.29%, Val F1: 49.44% Time: 54.48724937438965 
top-down:SEC: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 47.64%, Val F1: 30.33% Time: 54.48724937438965 
top-down:CONN: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   7.9,  Val Acc: 26.78%, Val F1:  8.21% Time: 54.48724937438965 
 
 
top-down:TOP: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 61.20%, Val F1: 50.50% Time: 133.60375905036926 
top-down:SEC: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 48.33%, Val F1: 31.99% Time: 133.60375905036926 
top-down:CONN: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   8.0,  Val Acc: 27.21%, Val F1:  8.26% Time: 133.60375905036926 
 
 
top-down:TOP: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 61.63%, Val F1: 50.94% Time: 212.66440773010254 
top-down:SEC: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 30.94% Time: 212.66440773010254 
top-down:CONN: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 59.38%,Val Loss:   8.0,  Val Acc: 26.52%, Val F1:  8.27% Time: 212.66440773010254 
 
 
top-down:TOP: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.12%, Val F1: 50.87% Time: 291.5754315853119 
top-down:SEC: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 48.33%, Val F1: 31.31% Time: 291.5754315853119 
top-down:CONN: Iter:   9800,  Train Loss: 2.2e+01,  Train Acc: 65.62%,Val Loss:   7.9,  Val Acc: 26.44%, Val F1:  8.11% Time: 291.5754315853119 
 
 
Train time usage: 311.8776834011078
Test time usage: 1.7265515327453613
TOP: Test Loss:   7.8,  Test Acc: 61.89%, Test F1: 51.74%
SEC: Test Loss:   7.8,  Test Acc: 46.01%, Test F1: 29.86%
CONN: Test Loss:   7.8,  Test Acc: 19.73%, Test F1:  8.23%
consistency_top_sec: 44.47%,  consistency_sec_conn: 15.78%, consistency_top_sec_conn: 15.59%
              precision    recall  f1-score   support

    Temporal     0.4545    0.2941    0.3571        68
 Contingency     0.5565    0.5055    0.5298       273
  Comparison     0.4922    0.4375    0.4632       144
   Expansion     0.6817    0.7617    0.7195       554

    accuracy                         0.6189      1039
   macro avg     0.5462    0.4997    0.5174      1039
weighted avg     0.6077    0.6189    0.6104      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4750    0.3519    0.4043        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5222    0.5261    0.5242       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4359    0.3984    0.4163       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3594    0.5750    0.4423       200
    Expansion.Instantiation     0.7071    0.5882    0.6422       119
      Expansion.Restatement     0.4457    0.3697    0.4041       211
      Expansion.Alternative     0.3333    0.3333    0.3333         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4601      1039
                  macro avg     0.3162    0.2933    0.2986      1039
               weighted avg     0.4590    0.4601    0.4525      1039

Epoch [26/30]
top-down:TOP: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 60.86%, Val F1: 50.57% Time: 59.54973387718201 
top-down:SEC: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 48.58%, Val F1: 31.25% Time: 59.54973387718201 
top-down:CONN: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   8.0,  Val Acc: 26.09%, Val F1:  8.14% Time: 59.54973387718201 
 
 
top-down:TOP: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 60.94%, Val F1: 50.02% Time: 138.4558708667755 
top-down:SEC: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 47.47%, Val F1: 30.52% Time: 138.4558708667755 
top-down:CONN: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 26.95%, Val F1:  8.30% Time: 138.4558708667755 
 
 
top-down:TOP: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.03%, Val F1: 50.41% Time: 217.51220607757568 
top-down:SEC: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 48.24%, Val F1: 30.94% Time: 217.51220607757568 
top-down:CONN: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   7.9,  Val Acc: 26.70%, Val F1:  8.29% Time: 217.51220607757568 
 
 
top-down:TOP: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.72%, Val F1: 50.99% Time: 296.2586917877197 
top-down:SEC: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 48.24%, Val F1: 31.27% Time: 296.2586917877197 
top-down:CONN: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   8.0,  Val Acc: 26.35%, Val F1:  8.13% Time: 296.2586917877197 
 
 
Train time usage: 311.30396580696106
Test time usage: 1.7358477115631104
TOP: Test Loss:   7.8,  Test Acc: 61.98%, Test F1: 51.55%
SEC: Test Loss:   7.8,  Test Acc: 47.06%, Test F1: 30.57%
CONN: Test Loss:   7.8,  Test Acc: 21.27%, Test F1:  8.36%
consistency_top_sec: 45.33%,  consistency_sec_conn: 17.71%, consistency_top_sec_conn: 17.42%
              precision    recall  f1-score   support

    Temporal     0.5000    0.2647    0.3462        68
 Contingency     0.5382    0.5678    0.5526       273
  Comparison     0.5182    0.3958    0.4488       144
   Expansion     0.6843    0.7473    0.7144       554

    accuracy                         0.6198      1039
   macro avg     0.5602    0.4939    0.5155      1039
weighted avg     0.6108    0.6198    0.6110      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4865    0.3333    0.3956        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5212    0.5970    0.5565       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4630    0.3906    0.4237       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3789    0.5400    0.4454       200
    Expansion.Instantiation     0.6957    0.5378    0.6066       119
      Expansion.Restatement     0.4375    0.3981    0.4169       211
      Expansion.Alternative     0.3636    0.4444    0.4000         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4706      1039
                  macro avg     0.3224    0.3022    0.3057      1039
               weighted avg     0.4637    0.4706    0.4610      1039

Epoch [27/30]
top-down:TOP: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 60.77%, Val F1: 50.30% Time: 65.18383622169495 
top-down:SEC: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 48.50%, Val F1: 31.27% Time: 65.18383622169495 
top-down:CONN: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.0,  Val Acc: 26.01%, Val F1:  7.91% Time: 65.18383622169495 
 
 
top-down:TOP: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 60.60%, Val F1: 49.94% Time: 144.07110953330994 
top-down:SEC: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 47.64%, Val F1: 30.76% Time: 144.07110953330994 
top-down:CONN: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   8.0,  Val Acc: 26.09%, Val F1:  8.05% Time: 144.07110953330994 
 
 
top-down:TOP: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 60.86%, Val F1: 49.62% Time: 223.0009195804596 
top-down:SEC: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 47.64%, Val F1: 30.80% Time: 223.0009195804596 
top-down:CONN: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 78.12%,Val Loss:   8.0,  Val Acc: 26.44%, Val F1:  8.29% Time: 223.0009195804596 
 
 
top-down:TOP: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.29%, Val F1: 50.02% Time: 301.937792301178 
top-down:SEC: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 87.50%,Val Loss:   8.0,  Val Acc: 48.07%, Val F1: 30.93% Time: 301.937792301178 
top-down:CONN: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 26.27%, Val F1:  8.15% Time: 301.937792301178 
 
 
Train time usage: 311.4755630493164
Test time usage: 1.7603514194488525
TOP: Test Loss:   7.8,  Test Acc: 61.98%, Test F1: 51.94%
SEC: Test Loss:   7.8,  Test Acc: 46.87%, Test F1: 30.71%
CONN: Test Loss:   7.8,  Test Acc: 19.92%, Test F1:  8.46%
consistency_top_sec: 45.72%,  consistency_sec_conn: 16.17%, consistency_top_sec_conn: 15.88%
              precision    recall  f1-score   support

    Temporal     0.4634    0.2794    0.3486        68
 Contingency     0.5409    0.5568    0.5487       273
  Comparison     0.5169    0.4236    0.4656       144
   Expansion     0.6878    0.7437    0.7147       554

    accuracy                         0.6198      1039
   macro avg     0.5523    0.5009    0.5194      1039
weighted avg     0.6109    0.6198    0.6126      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4722    0.3148    0.3778        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5134    0.5709    0.5406       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4561    0.4062    0.4298       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3797    0.5600    0.4525       200
    Expansion.Instantiation     0.6947    0.5546    0.6168       119
      Expansion.Restatement     0.4432    0.3886    0.4141       211
      Expansion.Alternative     0.4000    0.4444    0.4211         9
             Expansion.List     0.2500    0.0833    0.1250        12

                   accuracy                         0.4687      1039
                  macro avg     0.3281    0.3021    0.3071      1039
               weighted avg     0.4622    0.4687    0.4590      1039

Epoch [28/30]
top-down:TOP: Iter:  10700,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.03%, Val F1: 49.34% Time: 70.49218845367432 
top-down:SEC: Iter:  10700,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 48.07%, Val F1: 31.08% Time: 70.49218845367432 
top-down:CONN: Iter:  10700,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   8.0,  Val Acc: 26.44%, Val F1:  8.26% Time: 70.49218845367432 
 
 
top-down:TOP: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.12%, Val F1: 50.05% Time: 149.45551896095276 
top-down:SEC: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 47.81%, Val F1: 30.44% Time: 149.45551896095276 
top-down:CONN: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   8.0,  Val Acc: 26.35%, Val F1:  8.43% Time: 149.45551896095276 
 
 
top-down:TOP: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 60.86%, Val F1: 49.19% Time: 228.46042776107788 
top-down:SEC: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 47.90%, Val F1: 31.24% Time: 228.46042776107788 
top-down:CONN: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 26.35%, Val F1:  8.36% Time: 228.46042776107788 
 
 
top-down:TOP: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 60.77%, Val F1: 49.04% Time: 307.5835032463074 
top-down:SEC: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 48.15%, Val F1: 31.56% Time: 307.5835032463074 
top-down:CONN: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 78.12%,Val Loss:   8.0,  Val Acc: 26.78%, Val F1:  8.36% Time: 307.5835032463074 
 
 
Train time usage: 311.6895570755005
Test time usage: 1.7713406085968018
TOP: Test Loss:   7.8,  Test Acc: 61.31%, Test F1: 51.11%
SEC: Test Loss:   7.8,  Test Acc: 46.58%, Test F1: 31.10%
CONN: Test Loss:   7.8,  Test Acc: 20.69%, Test F1:  8.77%
consistency_top_sec: 44.95%,  consistency_sec_conn: 17.23%, consistency_top_sec_conn: 16.75%
              precision    recall  f1-score   support

    Temporal     0.4524    0.2794    0.3455        68
 Contingency     0.5428    0.5348    0.5387       273
  Comparison     0.5000    0.4097    0.4504       144
   Expansion     0.6770    0.7455    0.7096       554

    accuracy                         0.6131      1039
   macro avg     0.5430    0.4924    0.5111      1039
weighted avg     0.6025    0.6131    0.6050      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4737    0.3333    0.3913        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5103    0.5560    0.5321       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4630    0.3906    0.4237       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3793    0.5500    0.4490       200
    Expansion.Instantiation     0.7079    0.5294    0.6058       119
      Expansion.Restatement     0.4335    0.4171    0.4251       211
      Expansion.Alternative     0.4167    0.5556    0.4762         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4658      1039
                  macro avg     0.3258    0.3105    0.3110      1039
               weighted avg     0.4613    0.4658    0.4574      1039

Epoch [29/30]
top-down:TOP: Iter:  11100,  Train Loss: 3.5e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.03%, Val F1: 49.39% Time: 75.88476967811584 
top-down:SEC: Iter:  11100,  Train Loss: 3.5e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 47.81%, Val F1: 30.57% Time: 75.88476967811584 
top-down:CONN: Iter:  11100,  Train Loss: 3.5e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 25.84%, Val F1:  7.99% Time: 75.88476967811584 
 
 
top-down:TOP: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 61.20%, Val F1: 49.84% Time: 154.9157154560089 
top-down:SEC: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 47.21%, Val F1: 30.44% Time: 154.9157154560089 
top-down:CONN: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   8.1,  Val Acc: 26.44%, Val F1:  8.30% Time: 154.9157154560089 
 
 
top-down:TOP: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 61.12%, Val F1: 49.67% Time: 234.00732946395874 
top-down:SEC: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 47.55%, Val F1: 30.45% Time: 234.00732946395874 
top-down:CONN: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   8.1,  Val Acc: 26.61%, Val F1:  8.46% Time: 234.00732946395874 
 
 
Train time usage: 309.85436272621155
Test time usage: 1.7569570541381836
TOP: Test Loss:   7.9,  Test Acc: 61.69%, Test F1: 51.68%
SEC: Test Loss:   7.9,  Test Acc: 46.01%, Test F1: 30.18%
CONN: Test Loss:   7.9,  Test Acc: 19.92%, Test F1:  8.30%
consistency_top_sec: 44.47%,  consistency_sec_conn: 16.17%, consistency_top_sec_conn: 15.78%
              precision    recall  f1-score   support

    Temporal     0.4545    0.2941    0.3571        68
 Contingency     0.5492    0.5311    0.5400       273
  Comparison     0.4919    0.4236    0.4552       144
   Expansion     0.6837    0.7491    0.7149       554

    accuracy                         0.6169      1039
   macro avg     0.5449    0.4995    0.5168      1039
weighted avg     0.6068    0.6169    0.6096      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3519    0.4130        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5160    0.5410    0.5282       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4522    0.4062    0.4280       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3688    0.5550    0.4431       200
    Expansion.Instantiation     0.6809    0.5378    0.6009       119
      Expansion.Restatement     0.4293    0.3886    0.4080       211
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4601      1039
                  macro avg     0.3164    0.3008    0.3018      1039
               weighted avg     0.4561    0.4601    0.4521      1039

Epoch [30/30]
top-down:TOP: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   8.1,  Val Acc: 60.94%, Val F1: 49.63% Time: 4.17012095451355 
top-down:SEC: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   8.1,  Val Acc: 47.73%, Val F1: 30.90% Time: 4.17012095451355 
top-down:CONN: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   8.1,  Val Acc: 26.35%, Val F1:  8.29% Time: 4.17012095451355 
 
 
top-down:TOP: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.03%, Val F1: 49.91% Time: 83.06392025947571 
top-down:SEC: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 47.55%, Val F1: 30.55% Time: 83.06392025947571 
top-down:CONN: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   8.0,  Val Acc: 26.35%, Val F1:  8.16% Time: 83.06392025947571 
 
 
top-down:TOP: Iter:  11600,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 61.03%, Val F1: 50.03% Time: 161.96691012382507 
top-down:SEC: Iter:  11600,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 47.55%, Val F1: 30.49% Time: 161.96691012382507 
top-down:CONN: Iter:  11600,  Train Loss: 2.3e+01,  Train Acc: 81.25%,Val Loss:   8.1,  Val Acc: 26.09%, Val F1:  8.13% Time: 161.96691012382507 
 
 
top-down:TOP: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 61.37%, Val F1: 50.23% Time: 240.91685938835144 
top-down:SEC: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 47.55%, Val F1: 30.45% Time: 240.91685938835144 
top-down:CONN: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   8.1,  Val Acc: 26.18%, Val F1:  8.12% Time: 240.91685938835144 
 
 
Train time usage: 311.2193031311035
Test time usage: 1.7223846912384033
TOP: Test Loss:   7.9,  Test Acc: 61.50%, Test F1: 51.24%
SEC: Test Loss:   7.9,  Test Acc: 46.29%, Test F1: 30.41%
CONN: Test Loss:   7.9,  Test Acc: 20.21%, Test F1:  8.45%
consistency_top_sec: 44.85%,  consistency_sec_conn: 16.36%, consistency_top_sec_conn: 16.07%
              precision    recall  f1-score   support

    Temporal     0.4419    0.2794    0.3423        68
 Contingency     0.5472    0.5311    0.5390       273
  Comparison     0.4919    0.4236    0.4552       144
   Expansion     0.6820    0.7473    0.7132       554

    accuracy                         0.6150      1039
   macro avg     0.5408    0.4954    0.5124      1039
weighted avg     0.6045    0.6150    0.6074      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3519    0.4130        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5193    0.5522    0.5353       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4386    0.3906    0.4132       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3741    0.5500    0.4453       200
    Expansion.Instantiation     0.6882    0.5378    0.6038       119
      Expansion.Restatement     0.4315    0.4028    0.4167       211
      Expansion.Alternative     0.3636    0.4444    0.4000         9
             Expansion.List     0.2000    0.0833    0.1176        12

                   accuracy                         0.4629      1039
                  macro avg     0.3196    0.3012    0.3041      1039
               weighted avg     0.4579    0.4629    0.4548      1039

dev_best_acc_top: 63.78%,  dev_best_f1_top: 53.85%, 
dev_best_acc_sec: 49.53%,  dev_best_f1_sec: 31.72%, 
dev_best_acc_conn: 29.61%,  dev_best_f1_conn:  8.74%
