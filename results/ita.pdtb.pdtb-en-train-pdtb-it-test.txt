nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_en_train_pdtb_it_test/data/', 'log_file': 'data/pdtb_en_train_pdtb_it_test/log/', 'save_file': 'data/pdtb_en_train_pdtb_it_test/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 30, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March09-09:56:50', 'log': 'data/pdtb_en_train_pdtb_it_test/log/March09-09:56:50.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]95it [00:00, 949.79it/s]295it [00:00, 1563.91it/s]492it [00:00, 1746.04it/s]703it [00:00, 1887.17it/s]917it [00:00, 1977.13it/s]1122it [00:00, 2001.43it/s]1354it [00:00, 2104.26it/s]1565it [00:00, 2072.05it/s]1773it [00:00, 2071.70it/s]1988it [00:01, 2093.65it/s]2198it [00:01, 2074.40it/s]2406it [00:01, 2055.18it/s]2617it [00:01, 2069.20it/s]2828it [00:01, 2077.71it/s]3037it [00:01, 2080.60it/s]3246it [00:01, 2072.21it/s]3457it [00:01, 2081.20it/s]3666it [00:01, 2062.78it/s]3873it [00:01, 2010.59it/s]4082it [00:02, 2033.13it/s]4286it [00:02, 1973.57it/s]4491it [00:02, 1995.41it/s]4693it [00:02, 2000.62it/s]4894it [00:02, 1976.09it/s]5099it [00:02, 1996.34it/s]5299it [00:02, 1373.53it/s]5481it [00:02, 1473.28it/s]5687it [00:03, 1615.12it/s]5918it [00:03, 1792.94it/s]6123it [00:03, 1860.06it/s]6336it [00:03, 1933.17it/s]6539it [00:03, 1942.73it/s]6751it [00:03, 1992.25it/s]6955it [00:03, 2005.52it/s]7163it [00:03, 2027.20it/s]7377it [00:03, 2058.48it/s]7585it [00:03, 2010.39it/s]7789it [00:04, 2018.62it/s]7992it [00:04, 2021.02it/s]8195it [00:04, 1981.09it/s]8395it [00:04, 1984.38it/s]8597it [00:04, 1994.00it/s]8812it [00:04, 2039.21it/s]9017it [00:04, 2037.91it/s]9225it [00:04, 2033.77it/s]9429it [00:04, 1998.40it/s]9646it [00:04, 2047.91it/s]9864it [00:05, 2086.33it/s]10080it [00:05, 2107.53it/s]10291it [00:05, 2082.77it/s]10507it [00:05, 2105.21it/s]10718it [00:05, 2067.79it/s]10931it [00:05, 2085.13it/s]11144it [00:05, 2097.00it/s]11357it [00:05, 2104.72it/s]11568it [00:05, 2098.79it/s]11778it [00:05, 2095.85it/s]11988it [00:06, 2061.78it/s]12195it [00:06, 1967.50it/s]12393it [00:06, 1912.16it/s]12547it [00:06, 1973.02it/s]
0it [00:00, ?it/s]192it [00:00, 1918.52it/s]400it [00:00, 2008.61it/s]601it [00:00, 1936.54it/s]795it [00:00, 1888.04it/s]988it [00:00, 1902.38it/s]1165it [00:00, 1924.26it/s]
0it [00:00, ?it/s]144it [00:00, 1436.61it/s]304it [00:00, 1529.10it/s]457it [00:00, 1526.00it/s]616it [00:00, 1550.17it/s]772it [00:00, 1455.02it/s]919it [00:00, 1446.33it/s]1039it [00:00, 1482.54it/s]
Time usage: 19.563844203948975
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/30]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 55.88%, Val F1: 17.92% Time: 103.1367735862732 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   7.5,  Val Acc: 24.21%, Val F1:  3.54% Time: 103.1367735862732 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  0.00%,Val Loss:   7.5,  Val Acc:  3.61%, Val F1:  0.28% Time: 103.1367735862732 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 55.88%, Val F1: 17.92% Time: 195.80410528182983 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.6,  Val Acc: 27.47%, Val F1:  5.80% Time: 195.80410528182983 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.6,  Val Acc: 12.88%, Val F1:  0.34% Time: 195.80410528182983 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 68.75%,Val Loss:   6.4,  Val Acc: 56.22%, Val F1: 22.43% Time: 288.369530916214 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 31.25%,Val Loss:   6.4,  Val Acc: 30.56%, Val F1:  9.68% Time: 288.369530916214 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 15.62%,Val Loss:   6.4,  Val Acc: 17.08%, Val F1:  1.11% Time: 288.369530916214 *
 
 
Train time usage: 372.1119575500488
Test time usage: 2.3675780296325684
TOP: Test Loss:   6.4,  Test Acc: 53.61%, Test F1: 17.45%
SEC: Test Loss:   6.4,  Test Acc: 32.44%, Test F1: 10.95%
CONN: Test Loss:   6.4,  Test Acc: 15.21%, Test F1:  1.06%
consistency_top_sec: 21.08%,  consistency_sec_conn:  9.62%, consistency_top_sec_conn:  4.23%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        68
 Contingency     0.0000    0.0000    0.0000       270
  Comparison     0.0000    0.0000    0.0000       144
   Expansion     0.5361    1.0000    0.6980       557

    accuracy                         0.5361      1039
   macro avg     0.1340    0.2500    0.1745      1039
weighted avg     0.2874    0.5361    0.3742      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4615    0.4478    0.4545       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.0000    0.0000    0.0000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4557    0.3600    0.4022       200
    Expansion.Instantiation     0.0000    0.0000    0.0000       118
      Expansion.Restatement     0.2335    0.6840    0.3481       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.3244      1039
                  macro avg     0.1046    0.1356    0.1095      1039
               weighted avg     0.2544    0.3244    0.2657      1039

Epoch [2/30]
top-down:TOP: Iter:    400,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.3,  Val Acc: 55.62%, Val F1: 32.39% Time: 10.41542935371399 *
top-down:SEC: Iter:    400,  Train Loss: 3e+01,  Train Acc: 25.00%,Val Loss:   6.3,  Val Acc: 33.99%, Val F1: 11.39% Time: 10.41542935371399 *
top-down:CONN: Iter:    400,  Train Loss: 3e+01,  Train Acc:  9.38%,Val Loss:   6.3,  Val Acc: 17.00%, Val F1:  1.54% Time: 10.41542935371399 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 52.79%, Val F1: 29.86% Time: 103.14847207069397 *
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   6.2,  Val Acc: 35.88%, Val F1: 13.28% Time: 103.14847207069397 *
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   6.2,  Val Acc: 20.09%, Val F1:  2.01% Time: 103.14847207069397 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   5.9,  Val Acc: 56.14%, Val F1: 38.94% Time: 195.77412486076355 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   5.9,  Val Acc: 41.37%, Val F1: 20.22% Time: 195.77412486076355 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 21.88%,Val Loss:   5.9,  Val Acc: 21.63%, Val F1:  2.76% Time: 195.77412486076355 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   5.7,  Val Acc: 54.94%, Val F1: 44.25% Time: 288.4005000591278 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   5.7,  Val Acc: 40.17%, Val F1: 19.21% Time: 288.4005000591278 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   5.7,  Val Acc: 23.18%, Val F1:  3.34% Time: 288.4005000591278 *
 
 
Train time usage: 365.30980587005615
Test time usage: 2.3522679805755615
TOP: Test Loss:   5.4,  Test Acc: 59.67%, Test F1: 42.41%
SEC: Test Loss:   5.4,  Test Acc: 46.87%, Test F1: 25.40%
CONN: Test Loss:   5.4,  Test Acc: 21.94%, Test F1:  3.60%
consistency_top_sec: 35.80%,  consistency_sec_conn: 16.75%, consistency_top_sec_conn: 11.65%
              precision    recall  f1-score   support

    Temporal     0.5366    0.3235    0.4037        68
 Contingency     0.5556    0.1107    0.1846       271
  Comparison     0.6852    0.2569    0.3737       144
   Expansion     0.5966    0.9550    0.7344       556

    accuracy                         0.5967      1039
   macro avg     0.5935    0.4116    0.4241      1039
weighted avg     0.5943    0.5967    0.5194      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4545    0.3704    0.4082        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5613    0.5299    0.5451       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6304    0.2266    0.3333       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.3737    0.7250    0.4932       200
    Expansion.Instantiation     0.6635    0.5798    0.6188       119
      Expansion.Restatement     0.4020    0.3886    0.3952       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4687      1039
                  macro avg     0.2805    0.2564    0.2540      1039
               weighted avg     0.4756    0.4687    0.4490      1039

Epoch [3/30]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 61.29%, Val F1: 50.48% Time: 16.58683729171753 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 45.24%, Val F1: 23.66% Time: 16.58683729171753 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 31.25%,Val Loss:   5.5,  Val Acc: 23.86%, Val F1:  4.09% Time: 16.58683729171753 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.4,  Val Acc: 63.61%, Val F1: 50.10% Time: 109.7698814868927 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 46.44%, Val F1: 26.94% Time: 109.7698814868927 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 15.62%,Val Loss:   5.4,  Val Acc: 26.61%, Val F1:  4.39% Time: 109.7698814868927 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.3,  Val Acc: 62.66%, Val F1: 45.12% Time: 200.8115520477295 
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.3,  Val Acc: 48.41%, Val F1: 26.09% Time: 200.8115520477295 
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 31.25%,Val Loss:   5.3,  Val Acc: 27.81%, Val F1:  4.65% Time: 200.8115520477295 
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   5.3,  Val Acc: 63.00%, Val F1: 50.12% Time: 293.127197265625 *
top-down:SEC: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   5.3,  Val Acc: 49.44%, Val F1: 30.04% Time: 293.127197265625 *
top-down:CONN: Iter:   1100,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   5.3,  Val Acc: 27.81%, Val F1:  5.12% Time: 293.127197265625 *
 
 
Train time usage: 363.832852602005
Test time usage: 2.3464205265045166
TOP: Test Loss:   5.1,  Test Acc: 64.29%, Test F1: 52.46%
SEC: Test Loss:   5.1,  Test Acc: 51.01%, Test F1: 27.85%
CONN: Test Loss:   5.1,  Test Acc: 24.45%, Test F1:  4.90%
consistency_top_sec: 43.50%,  consistency_sec_conn: 20.60%, consistency_top_sec_conn: 17.52%
              precision    recall  f1-score   support

    Temporal     0.6000    0.3088    0.4078        68
 Contingency     0.6154    0.3810    0.4706       273
  Comparison     0.5644    0.3958    0.4653       144
   Expansion     0.6621    0.8773    0.7547       554

    accuracy                         0.6429      1039
   macro avg     0.6105    0.4907    0.5246      1039
weighted avg     0.6322    0.6429    0.6172      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4884    0.3818    0.4286        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5455    0.6245    0.5823       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4531    0.4531    0.4531       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4582    0.6850    0.5491       200
    Expansion.Instantiation     0.7143    0.5556    0.6250       117
      Expansion.Restatement     0.4765    0.3839    0.4252       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5101      1039
                  macro avg     0.2851    0.2804    0.2785      1039
               weighted avg     0.4883    0.5101    0.4917      1039

Epoch [4/30]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.3,  Val Acc: 62.49%, Val F1: 51.54% Time: 22.905120611190796 *
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 68.75%,Val Loss:   5.3,  Val Acc: 49.61%, Val F1: 29.74% Time: 22.905120611190796 *
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 50.00%,Val Loss:   5.3,  Val Acc: 28.84%, Val F1:  5.93% Time: 22.905120611190796 *
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 64.03%, Val F1: 50.50% Time: 116.04973936080933 *
top-down:SEC: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 59.38%,Val Loss:   5.3,  Val Acc: 49.18%, Val F1: 30.73% Time: 116.04973936080933 *
top-down:CONN: Iter:   1300,  Train Loss: 3.4e+01,  Train Acc: 50.00%,Val Loss:   5.3,  Val Acc: 29.87%, Val F1:  5.95% Time: 116.04973936080933 *
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.4,  Val Acc: 62.23%, Val F1: 51.43% Time: 207.29863262176514 
top-down:SEC: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 47.30%, Val F1: 30.58% Time: 207.29863262176514 
top-down:CONN: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   5.4,  Val Acc: 27.98%, Val F1:  5.71% Time: 207.29863262176514 
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 59.91%, Val F1: 48.52% Time: 298.64017152786255 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   5.4,  Val Acc: 47.30%, Val F1: 30.85% Time: 298.64017152786255 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 27.04%, Val F1:  6.11% Time: 298.64017152786255 
 
 
Train time usage: 366.8234987258911
Test time usage: 2.3162639141082764
TOP: Test Loss:   5.1,  Test Acc: 63.23%, Test F1: 51.75%
SEC: Test Loss:   5.1,  Test Acc: 51.01%, Test F1: 33.27%
CONN: Test Loss:   5.1,  Test Acc: 25.22%, Test F1:  7.97%
consistency_top_sec: 45.81%,  consistency_sec_conn: 20.60%, consistency_top_sec_conn: 18.58%
              precision    recall  f1-score   support

    Temporal     0.7895    0.2206    0.3448        68
 Contingency     0.5894    0.4469    0.5083       273
  Comparison     0.5116    0.4583    0.4835       144
   Expansion     0.6637    0.8195    0.7334       554

    accuracy                         0.6323      1039
   macro avg     0.6386    0.4863    0.5175      1039
weighted avg     0.6313    0.6323    0.6142      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7727    0.3148    0.4474        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5439    0.6030    0.5719       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4857    0.5312    0.5075       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4847    0.4750    0.4798       200
    Expansion.Instantiation     0.7500    0.6050    0.6698       119
      Expansion.Restatement     0.4029    0.5283    0.4571       212
      Expansion.Alternative     0.5000    0.5556    0.5263         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5101      1039
                  macro avg     0.3582    0.3285    0.3327      1039
               weighted avg     0.5055    0.5101    0.4996      1039

Epoch [5/30]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   5.3,  Val Acc: 63.00%, Val F1: 52.10% Time: 29.068267583847046 *
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.3,  Val Acc: 49.27%, Val F1: 32.40% Time: 29.068267583847046 *
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   5.3,  Val Acc: 29.27%, Val F1:  6.55% Time: 29.068267583847046 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 62.23%, Val F1: 53.28% Time: 120.38757729530334 
top-down:SEC: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 47.81%, Val F1: 30.90% Time: 120.38757729530334 
top-down:CONN: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   5.5,  Val Acc: 27.04%, Val F1:  6.21% Time: 120.38757729530334 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 60.86%, Val F1: 52.85% Time: 211.75212001800537 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 48.15%, Val F1: 30.99% Time: 211.75212001800537 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 26.09%, Val F1:  6.16% Time: 211.75212001800537 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   5.4,  Val Acc: 61.63%, Val F1: 51.24% Time: 303.084025144577 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   5.4,  Val Acc: 48.07%, Val F1: 30.90% Time: 303.084025144577 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 28.93%, Val F1:  6.89% Time: 303.084025144577 
 
 
Train time usage: 361.5952250957489
Test time usage: 2.3436334133148193
TOP: Test Loss:   5.2,  Test Acc: 64.39%, Test F1: 54.15%
SEC: Test Loss:   5.2,  Test Acc: 51.49%, Test F1: 32.97%
CONN: Test Loss:   5.2,  Test Acc: 26.18%, Test F1:  8.09%
consistency_top_sec: 48.80%,  consistency_sec_conn: 22.14%, consistency_top_sec_conn: 20.89%
              precision    recall  f1-score   support

    Temporal     0.7273    0.2353    0.3556        68
 Contingency     0.5839    0.5818    0.5829       275
  Comparison     0.5000    0.4897    0.4948       145
   Expansion     0.7022    0.7659    0.7326       551

    accuracy                         0.6439      1039
   macro avg     0.6283    0.5182    0.5415      1039
weighted avg     0.6443    0.6439    0.6351      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5862    0.3091    0.4048        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5413    0.6580    0.5940       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4429    0.4844    0.4627       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4695    0.5000    0.4843       200
    Expansion.Instantiation     0.6970    0.5897    0.6389       117
      Expansion.Restatement     0.4751    0.4976    0.4861       211
      Expansion.Alternative     0.5556    0.5556    0.5556         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5149      1039
                  macro avg     0.3425    0.3268    0.3297      1039
               weighted avg     0.4959    0.5149    0.5009      1039

Epoch [6/30]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 63.43%, Val F1: 51.91% Time: 35.30659341812134 *
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 49.96%, Val F1: 31.87% Time: 35.30659341812134 *
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   5.5,  Val Acc: 30.13%, Val F1:  7.47% Time: 35.30659341812134 *
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 62.83%, Val F1: 50.97% Time: 126.35089564323425 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.6,  Val Acc: 50.04%, Val F1: 32.56% Time: 126.35089564323425 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 29.01%, Val F1:  7.52% Time: 126.35089564323425 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   5.6,  Val Acc: 62.06%, Val F1: 51.54% Time: 217.67597603797913 
top-down:SEC: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 49.79%, Val F1: 31.50% Time: 217.67597603797913 
top-down:CONN: Iter:   2200,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.6,  Val Acc: 29.53%, Val F1:  7.05% Time: 217.67597603797913 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 62.49%, Val F1: 51.40% Time: 310.2506265640259 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 48.93%, Val F1: 31.50% Time: 310.2506265640259 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 28.93%, Val F1:  7.06% Time: 310.2506265640259 
 
 
Train time usage: 363.80881094932556
Test time usage: 2.326997995376587
TOP: Test Loss:   5.2,  Test Acc: 64.77%, Test F1: 55.50%
SEC: Test Loss:   5.2,  Test Acc: 50.82%, Test F1: 33.06%
CONN: Test Loss:   5.2,  Test Acc: 26.28%, Test F1:  7.73%
consistency_top_sec: 47.74%,  consistency_sec_conn: 21.46%, consistency_top_sec_conn: 20.40%
              precision    recall  f1-score   support

    Temporal     0.5854    0.3529    0.4404        68
 Contingency     0.6018    0.4982    0.5451       273
  Comparison     0.5517    0.4444    0.4923       144
   Expansion     0.6845    0.8105    0.7421       554

    accuracy                         0.6477      1039
   macro avg     0.6058    0.5265    0.5550      1039
weighted avg     0.6378    0.6477    0.6360      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5750    0.4182    0.4842        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5616    0.5762    0.5688       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4831    0.4453    0.4634       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4251    0.6100    0.5010       200
    Expansion.Instantiation     0.7347    0.6154    0.6698       117
      Expansion.Restatement     0.4541    0.4455    0.4498       211
      Expansion.Alternative     0.4545    0.5556    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5082      1039
                  macro avg     0.3353    0.3333    0.3306      1039
               weighted avg     0.4961    0.5082    0.4975      1039

Epoch [7/30]
top-down:TOP: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 62.32%, Val F1: 51.61% Time: 40.06143569946289 
top-down:SEC: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.8,  Val Acc: 48.41%, Val F1: 31.06% Time: 40.06143569946289 
top-down:CONN: Iter:   2400,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   5.8,  Val Acc: 28.24%, Val F1:  7.40% Time: 40.06143569946289 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   5.7,  Val Acc: 61.89%, Val F1: 49.05% Time: 131.3129916191101 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.7,  Val Acc: 48.93%, Val F1: 30.61% Time: 131.3129916191101 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   5.7,  Val Acc: 28.15%, Val F1:  6.95% Time: 131.3129916191101 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.9,  Val Acc: 61.37%, Val F1: 49.76% Time: 222.6896197795868 
top-down:SEC: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 65.62%,Val Loss:   5.9,  Val Acc: 49.70%, Val F1: 30.96% Time: 222.6896197795868 
top-down:CONN: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 31.25%,Val Loss:   5.9,  Val Acc: 30.30%, Val F1:  7.85% Time: 222.6896197795868 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 90.62%,Val Loss:   5.8,  Val Acc: 60.86%, Val F1: 51.51% Time: 314.0075492858887 
top-down:SEC: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 48.41%, Val F1: 30.90% Time: 314.0075492858887 
top-down:CONN: Iter:   2700,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   5.8,  Val Acc: 28.93%, Val F1:  7.85% Time: 314.0075492858887 
 
 
Train time usage: 360.33519744873047
Test time usage: 2.3333208560943604
TOP: Test Loss:   5.5,  Test Acc: 63.62%, Test F1: 52.83%
SEC: Test Loss:   5.5,  Test Acc: 49.76%, Test F1: 32.18%
CONN: Test Loss:   5.5,  Test Acc: 25.51%, Test F1:  8.19%
consistency_top_sec: 47.64%,  consistency_sec_conn: 20.98%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.6000    0.2647    0.3673        68
 Contingency     0.5615    0.5368    0.5489       272
  Comparison     0.5644    0.3958    0.4653       144
   Expansion     0.6790    0.7928    0.7315       555

    accuracy                         0.6362      1039
   macro avg     0.6012    0.4975    0.5283      1039
weighted avg     0.6272    0.6362    0.6230      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5758    0.3519    0.4368        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5374    0.5918    0.5633       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4904    0.3984    0.4397       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4167    0.6000    0.4918       200
    Expansion.Instantiation     0.6762    0.5966    0.6339       119
      Expansion.Restatement     0.4581    0.4387    0.4482       212
      Expansion.Alternative     0.5000    0.5556    0.5263         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4976      1039
                  macro avg     0.3322    0.3212    0.3218      1039
               weighted avg     0.4839    0.4976    0.4849      1039

Epoch [8/30]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   5.8,  Val Acc: 62.06%, Val F1: 51.53% Time: 46.18639779090881 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 49.27%, Val F1: 31.50% Time: 46.18639779090881 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   5.8,  Val Acc: 28.93%, Val F1:  7.78% Time: 46.18639779090881 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 62.15%, Val F1: 53.24% Time: 137.33362746238708 
top-down:SEC: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   6.0,  Val Acc: 48.84%, Val F1: 31.71% Time: 137.33362746238708 
top-down:CONN: Iter:   2900,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   6.0,  Val Acc: 28.84%, Val F1:  7.87% Time: 137.33362746238708 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.0,  Val Acc: 62.06%, Val F1: 51.50% Time: 230.04316878318787 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   6.0,  Val Acc: 48.76%, Val F1: 30.81% Time: 230.04316878318787 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.0,  Val Acc: 27.98%, Val F1:  7.85% Time: 230.04316878318787 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 61.89%, Val F1: 50.00% Time: 322.5686128139496 
top-down:SEC: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 47.73%, Val F1: 30.54% Time: 322.5686128139496 
top-down:CONN: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   6.1,  Val Acc: 28.50%, Val F1:  8.03% Time: 322.5686128139496 
 
 
Train time usage: 362.63558745384216
Test time usage: 2.426226854324341
TOP: Test Loss:   5.6,  Test Acc: 63.81%, Test F1: 52.89%
SEC: Test Loss:   5.6,  Test Acc: 49.37%, Test F1: 31.80%
CONN: Test Loss:   5.6,  Test Acc: 24.74%, Test F1:  8.17%
consistency_top_sec: 47.35%,  consistency_sec_conn: 20.02%, consistency_top_sec_conn: 19.25%
              precision    recall  f1-score   support

    Temporal     0.6000    0.2609    0.3636        69
 Contingency     0.6117    0.4212    0.4989       273
  Comparison     0.5469    0.4861    0.5147       144
   Expansion     0.6638    0.8318    0.7384       553

    accuracy                         0.6381      1039
   macro avg     0.6056    0.5000    0.5289      1039
weighted avg     0.6297    0.6381    0.6196      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.3273    0.4235        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5806    0.4737    0.5217       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4646    0.4609    0.4627       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4051    0.6400    0.4961       200
    Expansion.Instantiation     0.6667    0.6610    0.6638       118
      Expansion.Restatement     0.4505    0.4695    0.4598       213
      Expansion.Alternative     0.5000    0.4444    0.4706         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4937      1039
                  macro avg     0.3334    0.3161    0.3180      1039
               weighted avg     0.4880    0.4937    0.4822      1039

Epoch [9/30]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.1,  Val Acc: 62.23%, Val F1: 51.51% Time: 53.746212005615234 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 49.18%, Val F1: 30.90% Time: 53.746212005615234 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   6.1,  Val Acc: 28.15%, Val F1:  7.77% Time: 53.746212005615234 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 62.06%, Val F1: 50.63% Time: 145.336749792099 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   6.1,  Val Acc: 48.58%, Val F1: 30.35% Time: 145.336749792099 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.1,  Val Acc: 27.90%, Val F1:  7.48% Time: 145.336749792099 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 61.37%, Val F1: 51.60% Time: 236.77257895469666 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   6.2,  Val Acc: 48.84%, Val F1: 30.58% Time: 236.77257895469666 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   6.2,  Val Acc: 28.15%, Val F1:  7.69% Time: 236.77257895469666 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 62.23%, Val F1: 52.27% Time: 328.0268032550812 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 49.70%, Val F1: 31.82% Time: 328.0268032550812 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 53.12%,Val Loss:   6.1,  Val Acc: 28.15%, Val F1:  7.79% Time: 328.0268032550812 
 
 
Train time usage: 361.9818003177643
Test time usage: 2.344111204147339
TOP: Test Loss:   5.8,  Test Acc: 62.56%, Test F1: 53.48%
SEC: Test Loss:   5.8,  Test Acc: 48.51%, Test F1: 31.47%
CONN: Test Loss:   5.8,  Test Acc: 26.08%, Test F1:  8.98%
consistency_top_sec: 46.68%,  consistency_sec_conn: 20.60%, consistency_top_sec_conn: 19.83%
              precision    recall  f1-score   support

    Temporal     0.5106    0.3478    0.4138        69
 Contingency     0.5794    0.4542    0.5092       273
  Comparison     0.4930    0.4861    0.4895       144
   Expansion     0.6792    0.7812    0.7267       553

    accuracy                         0.6256      1039
   macro avg     0.5656    0.5173    0.5348      1039
weighted avg     0.6160    0.6256    0.6159      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4468    0.3818    0.4118        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5532    0.4869    0.5179       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4430    0.5156    0.4765       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4146    0.5100    0.4574       200
    Expansion.Instantiation     0.6759    0.6186    0.6460       118
      Expansion.Restatement     0.4473    0.5000    0.4722       212
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4851      1039
                  macro avg     0.3051    0.3345    0.3147      1039
               weighted avg     0.4715    0.4851    0.4755      1039

Epoch [10/30]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   6.3,  Val Acc: 61.12%, Val F1: 51.11% Time: 58.67352366447449 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   6.3,  Val Acc: 48.76%, Val F1: 30.90% Time: 58.67352366447449 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 37.50%,Val Loss:   6.3,  Val Acc: 28.41%, Val F1:  7.23% Time: 58.67352366447449 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 61.72%, Val F1: 51.96% Time: 149.73187732696533 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   6.3,  Val Acc: 48.67%, Val F1: 31.55% Time: 149.73187732696533 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   6.3,  Val Acc: 29.18%, Val F1:  8.51% Time: 149.73187732696533 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 63.18%, Val F1: 51.48% Time: 240.9945523738861 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.3,  Val Acc: 49.61%, Val F1: 31.78% Time: 240.9945523738861 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 43.75%,Val Loss:   6.3,  Val Acc: 28.76%, Val F1:  8.03% Time: 240.9945523738861 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.4,  Val Acc: 62.83%, Val F1: 51.49% Time: 332.24840688705444 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   6.4,  Val Acc: 49.18%, Val F1: 31.52% Time: 332.24840688705444 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   6.4,  Val Acc: 27.64%, Val F1:  7.68% Time: 332.24840688705444 
 
 
Train time usage: 359.78992462158203
Test time usage: 2.3472633361816406
TOP: Test Loss:   5.9,  Test Acc: 64.49%, Test F1: 54.04%
SEC: Test Loss:   5.9,  Test Acc: 49.86%, Test F1: 31.85%
CONN: Test Loss:   5.9,  Test Acc: 25.79%, Test F1:  9.45%
consistency_top_sec: 48.60%,  consistency_sec_conn: 21.56%, consistency_top_sec_conn: 21.17%
              precision    recall  f1-score   support

    Temporal     0.5294    0.2647    0.3529        68
 Contingency     0.6067    0.5311    0.5664       273
  Comparison     0.5182    0.4931    0.5053       144
   Expansion     0.6932    0.7870    0.7371       554

    accuracy                         0.6449      1039
   macro avg     0.5869    0.5190    0.5404      1039
weighted avg     0.6355    0.6449    0.6350      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4865    0.3273    0.3913        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5747    0.5597    0.5671       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4493    0.4844    0.4662       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4265    0.5950    0.4969       200
    Expansion.Instantiation     0.6522    0.6356    0.6438       118
      Expansion.Restatement     0.4607    0.4171    0.4378       211
      Expansion.Alternative     0.4000    0.6667    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4986      1039
                  macro avg     0.3136    0.3351    0.3185      1039
               weighted avg     0.4825    0.4986    0.4864      1039

Epoch [11/30]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 61.89%, Val F1: 50.97% Time: 64.89102554321289 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 81.25%,Val Loss:   6.6,  Val Acc: 48.15%, Val F1: 30.94% Time: 64.89102554321289 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 40.62%,Val Loss:   6.6,  Val Acc: 27.47%, Val F1:  7.94% Time: 64.89102554321289 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.6,  Val Acc: 62.32%, Val F1: 49.40% Time: 158.33665442466736 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   6.6,  Val Acc: 48.76%, Val F1: 30.50% Time: 158.33665442466736 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 37.50%,Val Loss:   6.6,  Val Acc: 29.10%, Val F1:  8.51% Time: 158.33665442466736 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   6.6,  Val Acc: 62.92%, Val F1: 51.15% Time: 250.61493682861328 
top-down:SEC: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   6.6,  Val Acc: 49.53%, Val F1: 30.65% Time: 250.61493682861328 
top-down:CONN: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   6.6,  Val Acc: 28.15%, Val F1:  8.34% Time: 250.61493682861328 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 63.35%, Val F1: 53.01% Time: 343.47009110450745 *
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.6,  Val Acc: 50.73%, Val F1: 32.25% Time: 343.47009110450745 *
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   6.6,  Val Acc: 28.33%, Val F1:  7.56% Time: 343.47009110450745 *
 
 
Train time usage: 364.59736371040344
Test time usage: 2.371156930923462
TOP: Test Loss:   6.2,  Test Acc: 63.33%, Test F1: 53.27%
SEC: Test Loss:   6.2,  Test Acc: 48.70%, Test F1: 30.91%
CONN: Test Loss:   6.2,  Test Acc: 26.18%, Test F1:  9.55%
consistency_top_sec: 47.16%,  consistency_sec_conn: 21.37%, consistency_top_sec_conn: 20.69%
              precision    recall  f1-score   support

    Temporal     0.5556    0.2899    0.3810        69
 Contingency     0.5936    0.4797    0.5306       271
  Comparison     0.4964    0.4792    0.4876       144
   Expansion     0.6806    0.7910    0.7317       555

    accuracy                         0.6333      1039
   macro avg     0.5815    0.5099    0.5327      1039
weighted avg     0.6241    0.6333    0.6221      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5135    0.3455    0.4130        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5672    0.5056    0.5347       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4406    0.4922    0.4649       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4135    0.5500    0.4721       200
    Expansion.Instantiation     0.6202    0.6780    0.6478       118
      Expansion.Restatement     0.4585    0.4434    0.4508       212
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4870      1039
                  macro avg     0.3043    0.3246    0.3091      1039
               weighted avg     0.4737    0.4870    0.4766      1039

Epoch [12/30]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 60.52%, Val F1: 50.71% Time: 71.01401734352112 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 47.73%, Val F1: 30.19% Time: 71.01401734352112 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   6.7,  Val Acc: 27.73%, Val F1:  7.83% Time: 71.01401734352112 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.8,  Val Acc: 60.94%, Val F1: 51.11% Time: 162.28126907348633 
top-down:SEC: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   6.8,  Val Acc: 48.24%, Val F1: 30.09% Time: 162.28126907348633 
top-down:CONN: Iter:   4500,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   6.8,  Val Acc: 27.47%, Val F1:  7.43% Time: 162.28126907348633 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   6.8,  Val Acc: 60.69%, Val F1: 50.85% Time: 253.59490823745728 
top-down:SEC: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 48.50%, Val F1: 31.40% Time: 253.59490823745728 
top-down:CONN: Iter:   4600,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   6.8,  Val Acc: 27.98%, Val F1:  8.05% Time: 253.59490823745728 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 63.69%, Val F1: 52.46% Time: 344.8716547489166 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.7,  Val Acc: 48.67%, Val F1: 31.87% Time: 344.8716547489166 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   6.7,  Val Acc: 28.07%, Val F1:  8.31% Time: 344.8716547489166 
 
 
Train time usage: 360.03932452201843
Test time usage: 2.3864803314208984
TOP: Test Loss:   6.3,  Test Acc: 64.87%, Test F1: 54.83%
SEC: Test Loss:   6.3,  Test Acc: 51.40%, Test F1: 32.91%
CONN: Test Loss:   6.3,  Test Acc: 26.76%, Test F1:  9.43%
consistency_top_sec: 49.86%,  consistency_sec_conn: 22.81%, consistency_top_sec_conn: 22.14%
              precision    recall  f1-score   support

    Temporal     0.5128    0.2899    0.3704        69
 Contingency     0.5891    0.5912    0.5902       274
  Comparison     0.5366    0.4583    0.4944       144
   Expansion     0.7076    0.7717    0.7383       552

    accuracy                         0.6487      1039
   macro avg     0.5865    0.5278    0.5483      1039
weighted avg     0.6397    0.6487    0.6410      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.4000    0.4444        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5754    0.6142    0.5942       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4797    0.4609    0.4701       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4808    0.5000    0.4902       200
    Expansion.Instantiation     0.6792    0.6154    0.6457       117
      Expansion.Restatement     0.4341    0.5258    0.4756       213
      Expansion.Alternative     0.4545    0.5556    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5140      1039
                  macro avg     0.3276    0.3338    0.3291      1039
               weighted avg     0.4954    0.5140    0.5030      1039

Epoch [13/30]
top-down:TOP: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   6.8,  Val Acc: 61.63%, Val F1: 50.80% Time: 77.07221722602844 
top-down:SEC: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.8,  Val Acc: 48.84%, Val F1: 31.77% Time: 77.07221722602844 
top-down:CONN: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 59.38%,Val Loss:   6.8,  Val Acc: 27.98%, Val F1:  8.04% Time: 77.07221722602844 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   6.8,  Val Acc: 61.80%, Val F1: 50.33% Time: 168.29932808876038 
top-down:SEC: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 47.90%, Val F1: 29.42% Time: 168.29932808876038 
top-down:CONN: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 53.12%,Val Loss:   6.8,  Val Acc: 26.70%, Val F1:  7.20% Time: 168.29932808876038 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.9,  Val Acc: 60.26%, Val F1: 49.64% Time: 259.56942439079285 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   6.9,  Val Acc: 47.73%, Val F1: 30.87% Time: 259.56942439079285 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   6.9,  Val Acc: 27.64%, Val F1:  7.96% Time: 259.56942439079285 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 62.49%, Val F1: 50.38% Time: 350.8270056247711 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   6.8,  Val Acc: 49.61%, Val F1: 32.63% Time: 350.8270056247711 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   6.8,  Val Acc: 27.98%, Val F1:  8.31% Time: 350.8270056247711 
 
 
Train time usage: 359.77820229530334
Test time usage: 2.3554327487945557
TOP: Test Loss:   6.6,  Test Acc: 62.27%, Test F1: 51.63%
SEC: Test Loss:   6.6,  Test Acc: 49.37%, Test F1: 31.72%
CONN: Test Loss:   6.6,  Test Acc: 25.60%, Test F1:  9.16%
consistency_top_sec: 47.06%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 19.83%
              precision    recall  f1-score   support

    Temporal     0.5833    0.3043    0.4000        69
 Contingency     0.5646    0.4338    0.4906       272
  Comparison     0.5234    0.3889    0.4462       144
   Expansion     0.6579    0.8159    0.7284       554

    accuracy                         0.6227      1039
   macro avg     0.5823    0.4857    0.5163      1039
weighted avg     0.6099    0.6227    0.6053      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5714    0.3636    0.4444        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5556    0.4887    0.5200       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4609    0.4141    0.4362       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4307    0.5900    0.4979       200
    Expansion.Instantiation     0.6364    0.6525    0.6444       118
      Expansion.Restatement     0.4557    0.5070    0.4800       213
      Expansion.Alternative     0.3333    0.7778    0.4667         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4937      1039
                  macro avg     0.3131    0.3449    0.3172      1039
               weighted avg     0.4807    0.4937    0.4819      1039

Epoch [14/30]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.0,  Val Acc: 61.46%, Val F1: 50.77% Time: 83.45571827888489 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 47.81%, Val F1: 31.08% Time: 83.45571827888489 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   7.0,  Val Acc: 27.21%, Val F1:  8.34% Time: 83.45571827888489 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.0,  Val Acc: 63.35%, Val F1: 50.39% Time: 174.81570100784302 
top-down:SEC: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   7.0,  Val Acc: 48.15%, Val F1: 30.35% Time: 174.81570100784302 
top-down:CONN: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   7.0,  Val Acc: 27.55%, Val F1:  8.07% Time: 174.81570100784302 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 100.00%,Val Loss:   7.0,  Val Acc: 61.97%, Val F1: 50.69% Time: 266.18562364578247 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 48.67%, Val F1: 31.48% Time: 266.18562364578247 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   7.0,  Val Acc: 27.98%, Val F1:  8.80% Time: 266.18562364578247 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   7.0,  Val Acc: 62.15%, Val F1: 50.96% Time: 357.5925624370575 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 90.62%,Val Loss:   7.0,  Val Acc: 49.27%, Val F1: 32.13% Time: 357.5925624370575 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 43.75%,Val Loss:   7.0,  Val Acc: 27.90%, Val F1:  8.26% Time: 357.5925624370575 
 
 
Train time usage: 360.3215675354004
Test time usage: 2.3588290214538574
TOP: Test Loss:   6.6,  Test Acc: 63.23%, Test F1: 52.86%
SEC: Test Loss:   6.6,  Test Acc: 48.80%, Test F1: 31.64%
CONN: Test Loss:   6.6,  Test Acc: 25.89%, Test F1:  8.99%
consistency_top_sec: 47.16%,  consistency_sec_conn: 21.66%, consistency_top_sec_conn: 20.89%
              precision    recall  f1-score   support

    Temporal     0.5208    0.3623    0.4274        69
 Contingency     0.5931    0.4432    0.5073       273
  Comparison     0.5400    0.3750    0.4426       144
   Expansion     0.6652    0.8264    0.7371       553

    accuracy                         0.6323      1039
   macro avg     0.5798    0.5017    0.5286      1039
weighted avg     0.6193    0.6323    0.6153      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4400    0.4000    0.4190        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5926    0.4794    0.5300       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4630    0.3906    0.4237       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4201    0.6050    0.4959       200
    Expansion.Instantiation     0.6923    0.6102    0.6486       118
      Expansion.Restatement     0.4252    0.5094    0.4635       212
      Expansion.Alternative     0.4000    0.6667    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4880      1039
                  macro avg     0.3121    0.3328    0.3164      1039
               weighted avg     0.4823    0.4880    0.4786      1039

Epoch [15/30]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 61.20%, Val F1: 50.93% Time: 89.61417555809021 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 48.33%, Val F1: 30.76% Time: 89.61417555809021 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   7.1,  Val Acc: 27.55%, Val F1:  8.17% Time: 89.61417555809021 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.2,  Val Acc: 61.97%, Val F1: 50.40% Time: 193.80587697029114 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 47.47%, Val F1: 29.82% Time: 193.80587697029114 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   7.2,  Val Acc: 27.98%, Val F1:  8.10% Time: 193.80587697029114 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.1,  Val Acc: 63.18%, Val F1: 50.21% Time: 326.6595182418823 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 48.50%, Val F1: 30.81% Time: 326.6595182418823 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.1,  Val Acc: 28.33%, Val F1:  9.06% Time: 326.6595182418823 
 
 
Train time usage: 449.94282031059265
Test time usage: 2.5663270950317383
TOP: Test Loss:   6.7,  Test Acc: 63.52%, Test F1: 55.39%
SEC: Test Loss:   6.7,  Test Acc: 49.09%, Test F1: 31.57%
CONN: Test Loss:   6.7,  Test Acc: 25.70%, Test F1:  9.19%
consistency_top_sec: 47.55%,  consistency_sec_conn: 20.69%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.5745    0.3913    0.4655        69
 Contingency     0.5703    0.5474    0.5587       274
  Comparison     0.5169    0.4236    0.4656       144
   Expansion     0.6907    0.7645    0.7257       552

    accuracy                         0.6352      1039
   macro avg     0.5881    0.5317    0.5539      1039
weighted avg     0.6271    0.6352    0.6283      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5238    0.4000    0.4536        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5535    0.5618    0.5576       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4706    0.4375    0.4534       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4418    0.5500    0.4900       200
    Expansion.Instantiation     0.6509    0.5897    0.6188       117
      Expansion.Restatement     0.4217    0.4554    0.4379       213
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4909      1039
                  macro avg     0.3105    0.3328    0.3157      1039
               weighted avg     0.4758    0.4909    0.4809      1039

Epoch [16/30]
top-down:TOP: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 59.57%, Val F1: 48.74% Time: 9.710646629333496 
top-down:SEC: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   7.2,  Val Acc: 48.07%, Val F1: 30.27% Time: 9.710646629333496 
top-down:CONN: Iter:   5900,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   7.2,  Val Acc: 27.12%, Val F1:  7.98% Time: 9.710646629333496 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.2,  Val Acc: 61.03%, Val F1: 49.44% Time: 119.58352088928223 
top-down:SEC: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 48.84%, Val F1: 32.19% Time: 119.58352088928223 
top-down:CONN: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.2,  Val Acc: 27.64%, Val F1:  8.53% Time: 119.58352088928223 
 
 
top-down:TOP: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.2,  Val Acc: 61.80%, Val F1: 51.32% Time: 210.88641953468323 
top-down:SEC: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.2,  Val Acc: 47.81%, Val F1: 29.66% Time: 210.88641953468323 
top-down:CONN: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 68.75%,Val Loss:   7.2,  Val Acc: 28.41%, Val F1:  8.75% Time: 210.88641953468323 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 60.77%, Val F1: 49.82% Time: 302.44009494781494 
top-down:SEC: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   7.1,  Val Acc: 48.50%, Val F1: 30.43% Time: 302.44009494781494 
top-down:CONN: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   7.1,  Val Acc: 27.64%, Val F1:  8.15% Time: 302.44009494781494 
 
 
Train time usage: 381.2342541217804
Test time usage: 2.306328773498535
TOP: Test Loss:   6.8,  Test Acc: 63.91%, Test F1: 54.60%
SEC: Test Loss:   6.8,  Test Acc: 49.57%, Test F1: 31.34%
CONN: Test Loss:   6.8,  Test Acc: 25.99%, Test F1:  9.78%
consistency_top_sec: 47.64%,  consistency_sec_conn: 20.98%, consistency_top_sec_conn: 20.21%
              precision    recall  f1-score   support

    Temporal     0.5217    0.3478    0.4174        69
 Contingency     0.5992    0.5182    0.5558       274
  Comparison     0.5039    0.4514    0.4762       144
   Expansion     0.6906    0.7844    0.7345       552

    accuracy                         0.6391      1039
   macro avg     0.5788    0.5255    0.5460      1039
weighted avg     0.6294    0.6391    0.6305      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3818    0.4330        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5703    0.5597    0.5650       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4496    0.4531    0.4514       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4312    0.5950    0.5000       200
    Expansion.Instantiation     0.6542    0.5983    0.6250       117
      Expansion.Restatement     0.4619    0.4292    0.4450       212
      Expansion.Alternative     0.3158    0.6667    0.4286         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4957      1039
                  macro avg     0.3075    0.3349    0.3134      1039
               weighted avg     0.4826    0.4957    0.4854      1039

Epoch [17/30]
top-down:TOP: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   7.2,  Val Acc: 62.06%, Val F1: 51.84% Time: 13.170665264129639 
top-down:SEC: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 48.41%, Val F1: 31.23% Time: 13.170665264129639 
top-down:CONN: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   7.2,  Val Acc: 28.67%, Val F1:  8.59% Time: 13.170665264129639 
 
 
top-down:TOP: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 60.00%, Val F1: 49.36% Time: 104.45428490638733 
top-down:SEC: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 49.10%, Val F1: 31.43% Time: 104.45428490638733 
top-down:CONN: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   7.3,  Val Acc: 28.24%, Val F1:  9.16% Time: 104.45428490638733 
 
 
top-down:TOP: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.5,  Val Acc: 59.31%, Val F1: 48.18% Time: 195.65785694122314 
top-down:SEC: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.5,  Val Acc: 47.98%, Val F1: 30.28% Time: 195.65785694122314 
top-down:CONN: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 28.41%, Val F1:  8.58% Time: 195.65785694122314 
 
 
top-down:TOP: Iter:   6600,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 61.72%, Val F1: 49.99% Time: 287.08669447898865 
top-down:SEC: Iter:   6600,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 49.36%, Val F1: 31.54% Time: 287.08669447898865 
top-down:CONN: Iter:   6600,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   7.4,  Val Acc: 28.07%, Val F1:  8.28% Time: 287.08669447898865 
 
 
Train time usage: 359.5841953754425
Test time usage: 2.346432685852051
TOP: Test Loss:   7.0,  Test Acc: 63.81%, Test F1: 54.84%
SEC: Test Loss:   7.0,  Test Acc: 49.66%, Test F1: 30.65%
CONN: Test Loss:   7.0,  Test Acc: 26.08%, Test F1:  9.36%
consistency_top_sec: 48.22%,  consistency_sec_conn: 21.56%, consistency_top_sec_conn: 20.98%
              precision    recall  f1-score   support

    Temporal     0.5417    0.3768    0.4444        69
 Contingency     0.5679    0.5803    0.5740       274
  Comparison     0.5185    0.3889    0.4444       144
   Expansion     0.6998    0.7645    0.7307       552

    accuracy                         0.6381      1039
   macro avg     0.5820    0.5276    0.5484      1039
weighted avg     0.6294    0.6381    0.6307      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5111    0.4182    0.4600        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5590    0.6007    0.5791       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4722    0.3984    0.4322       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4444    0.5800    0.5033       200
    Expansion.Instantiation     0.6545    0.6154    0.6344       117
      Expansion.Restatement     0.4384    0.4198    0.4289       212
      Expansion.Alternative     0.2667    0.4444    0.3333         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3042    0.3161    0.3065      1039
               weighted avg     0.4805    0.4966    0.4857      1039

Epoch [18/30]
top-down:TOP: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.2,  Val Acc: 62.06%, Val F1: 50.54% Time: 19.47863793373108 
top-down:SEC: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 47.04%, Val F1: 30.22% Time: 19.47863793373108 
top-down:CONN: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 62.50%,Val Loss:   7.2,  Val Acc: 27.55%, Val F1:  8.55% Time: 19.47863793373108 
 
 
top-down:TOP: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 61.29%, Val F1: 49.77% Time: 110.60949659347534 
top-down:SEC: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.4,  Val Acc: 47.47%, Val F1: 30.14% Time: 110.60949659347534 
top-down:CONN: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   7.4,  Val Acc: 27.47%, Val F1:  8.82% Time: 110.60949659347534 
 
 
top-down:TOP: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 96.88%,Val Loss:   7.4,  Val Acc: 61.55%, Val F1: 50.93% Time: 201.81463074684143 
top-down:SEC: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 48.24%, Val F1: 31.23% Time: 201.81463074684143 
top-down:CONN: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 75.00%,Val Loss:   7.4,  Val Acc: 28.84%, Val F1:  8.94% Time: 201.81463074684143 
 
 
top-down:TOP: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.4,  Val Acc: 62.15%, Val F1: 51.02% Time: 293.16268491744995 
top-down:SEC: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   7.4,  Val Acc: 47.81%, Val F1: 29.53% Time: 293.16268491744995 
top-down:CONN: Iter:   7000,  Train Loss: 2.5e+01,  Train Acc: 62.50%,Val Loss:   7.4,  Val Acc: 28.41%, Val F1:  8.61% Time: 293.16268491744995 
 
 
Train time usage: 359.34900641441345
Test time usage: 2.3672990798950195
TOP: Test Loss:   7.1,  Test Acc: 63.72%, Test F1: 54.15%
SEC: Test Loss:   7.1,  Test Acc: 49.28%, Test F1: 32.09%
CONN: Test Loss:   7.1,  Test Acc: 25.60%, Test F1:  9.50%
consistency_top_sec: 47.26%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 19.63%
              precision    recall  f1-score   support

    Temporal     0.5111    0.3333    0.4035        69
 Contingency     0.5841    0.4818    0.5280       274
  Comparison     0.5360    0.4653    0.4981       144
   Expansion     0.6843    0.7971    0.7364       552

    accuracy                         0.6372      1039
   macro avg     0.5789    0.5194    0.5415      1039
weighted avg     0.6258    0.6372    0.6263      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4667    0.3818    0.4200        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5826    0.5281    0.5540       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4574    0.4609    0.4591       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4221    0.5550    0.4795       200
    Expansion.Instantiation     0.6522    0.6410    0.6466       117
      Expansion.Restatement     0.4455    0.4601    0.4527       213
      Expansion.Alternative     0.3889    0.7778    0.5185         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4928      1039
                  macro avg     0.3105    0.3459    0.3209      1039
               weighted avg     0.4801    0.4928    0.4836      1039

Epoch [19/30]
top-down:TOP: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.5,  Val Acc: 61.12%, Val F1: 50.60% Time: 25.704604148864746 
top-down:SEC: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 47.81%, Val F1: 30.66% Time: 25.704604148864746 
top-down:CONN: Iter:   7100,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   7.5,  Val Acc: 28.67%, Val F1:  8.67% Time: 25.704604148864746 
 
 
top-down:TOP: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 96.88%,Val Loss:   7.5,  Val Acc: 62.23%, Val F1: 49.99% Time: 116.76574110984802 
top-down:SEC: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 84.38%,Val Loss:   7.5,  Val Acc: 47.55%, Val F1: 28.72% Time: 116.76574110984802 
top-down:CONN: Iter:   7200,  Train Loss: 2e+01,  Train Acc: 71.88%,Val Loss:   7.5,  Val Acc: 27.12%, Val F1:  8.82% Time: 116.76574110984802 
 
 
top-down:TOP: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 62.23%, Val F1: 52.03% Time: 208.0053584575653 
top-down:SEC: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 48.07%, Val F1: 31.21% Time: 208.0053584575653 
top-down:CONN: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   7.6,  Val Acc: 28.24%, Val F1:  8.82% Time: 208.0053584575653 
 
 
top-down:TOP: Iter:   7400,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.63%, Val F1: 50.59% Time: 299.40494298934937 
top-down:SEC: Iter:   7400,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 48.58%, Val F1: 31.77% Time: 299.40494298934937 
top-down:CONN: Iter:   7400,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   7.7,  Val Acc: 27.12%, Val F1:  8.46% Time: 299.40494298934937 
 
 
Train time usage: 359.74732542037964
Test time usage: 2.346142292022705
TOP: Test Loss:   7.0,  Test Acc: 63.52%, Test F1: 54.76%
SEC: Test Loss:   7.0,  Test Acc: 49.28%, Test F1: 31.55%
CONN: Test Loss:   7.0,  Test Acc: 24.64%, Test F1:  8.87%
consistency_top_sec: 47.55%,  consistency_sec_conn: 20.12%, consistency_top_sec_conn: 19.73%
              precision    recall  f1-score   support

    Temporal     0.5500    0.3235    0.4074        68
 Contingency     0.5632    0.5693    0.5662       274
  Comparison     0.5366    0.4583    0.4944       144
   Expansion     0.6945    0.7523    0.7222       553

    accuracy                         0.6352      1039
   macro avg     0.5861    0.5259    0.5476      1039
weighted avg     0.6285    0.6352    0.6289      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3636    0.4211        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5583    0.5918    0.5745       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4672    0.4453    0.4560       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4286    0.5700    0.4893       200
    Expansion.Instantiation     0.6379    0.6271    0.6325       118
      Expansion.Restatement     0.4457    0.3868    0.4141       212
      Expansion.Alternative     0.3500    0.7778    0.4828         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4928      1039
                  macro avg     0.3080    0.3420    0.3155      1039
               weighted avg     0.4764    0.4928    0.4808      1039

Epoch [20/30]
top-down:TOP: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 61.29%, Val F1: 50.00% Time: 31.833319902420044 
top-down:SEC: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 47.47%, Val F1: 30.85% Time: 31.833319902420044 
top-down:CONN: Iter:   7500,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   7.7,  Val Acc: 27.38%, Val F1:  8.83% Time: 31.833319902420044 
 
 
top-down:TOP: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.89%, Val F1: 49.76% Time: 122.96618962287903 
top-down:SEC: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   7.7,  Val Acc: 46.95%, Val F1: 30.88% Time: 122.96618962287903 
top-down:CONN: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.7,  Val Acc: 27.38%, Val F1:  9.31% Time: 122.96618962287903 
 
 
top-down:TOP: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 62.58%, Val F1: 51.54% Time: 214.05326676368713 
top-down:SEC: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.6,  Val Acc: 49.36%, Val F1: 31.35% Time: 214.05326676368713 
top-down:CONN: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   7.6,  Val Acc: 28.33%, Val F1:  8.97% Time: 214.05326676368713 
 
 
top-down:TOP: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.03%, Val F1: 50.93% Time: 305.4346718788147 
top-down:SEC: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 47.55%, Val F1: 30.34% Time: 305.4346718788147 
top-down:CONN: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 28.07%, Val F1:  8.96% Time: 305.4346718788147 
 
 
Train time usage: 359.629323720932
Test time usage: 2.360866069793701
TOP: Test Loss:   7.2,  Test Acc: 63.04%, Test F1: 52.85%
SEC: Test Loss:   7.2,  Test Acc: 50.14%, Test F1: 31.58%
CONN: Test Loss:   7.2,  Test Acc: 25.31%, Test F1:  9.55%
consistency_top_sec: 48.03%,  consistency_sec_conn: 20.40%, consistency_top_sec_conn: 19.83%
              precision    recall  f1-score   support

    Temporal     0.5135    0.2794    0.3619        68
 Contingency     0.5830    0.5000    0.5383       274
  Comparison     0.5000    0.4722    0.4857       144
   Expansion     0.6830    0.7794    0.7280       553

    accuracy                         0.6304      1039
   macro avg     0.5699    0.5078    0.5285      1039
weighted avg     0.6202    0.6304    0.6205      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5429    0.3455    0.4222        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5714    0.5393    0.5549       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4697    0.4844    0.4769       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4405    0.5550    0.4912       200
    Expansion.Instantiation     0.6990    0.6154    0.6545       117
      Expansion.Restatement     0.4496    0.5023    0.4745       213
      Expansion.Alternative     0.2857    0.6667    0.4000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5014      1039
                  macro avg     0.3144    0.3371    0.3158      1039
               weighted avg     0.4916    0.5014    0.4927      1039

Epoch [21/30]
top-down:TOP: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.12%, Val F1: 50.24% Time: 38.1029953956604 
top-down:SEC: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 46.35%, Val F1: 30.49% Time: 38.1029953956604 
top-down:CONN: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 71.88%,Val Loss:   7.7,  Val Acc: 27.73%, Val F1:  9.23% Time: 38.1029953956604 
 
 
top-down:TOP: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 61.46%, Val F1: 51.79% Time: 129.0953493118286 
top-down:SEC: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.6,  Val Acc: 46.61%, Val F1: 29.26% Time: 129.0953493118286 
top-down:CONN: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   7.6,  Val Acc: 28.24%, Val F1:  9.07% Time: 129.0953493118286 
 
 
top-down:TOP: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 61.03%, Val F1: 50.76% Time: 220.32856631278992 
top-down:SEC: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 47.98%, Val F1: 31.43% Time: 220.32856631278992 
top-down:CONN: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   7.7,  Val Acc: 27.90%, Val F1:  8.94% Time: 220.32856631278992 
 
 
top-down:TOP: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   7.6,  Val Acc: 61.63%, Val F1: 50.15% Time: 311.5913441181183 
top-down:SEC: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   7.6,  Val Acc: 48.24%, Val F1: 31.91% Time: 311.5913441181183 
top-down:CONN: Iter:   8200,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   7.6,  Val Acc: 27.30%, Val F1:  8.48% Time: 311.5913441181183 
 
 
Train time usage: 359.5749418735504
Test time usage: 2.3692498207092285
TOP: Test Loss:   7.3,  Test Acc: 63.14%, Test F1: 51.74%
SEC: Test Loss:   7.3,  Test Acc: 50.34%, Test F1: 31.94%
CONN: Test Loss:   7.3,  Test Acc: 25.22%, Test F1:  9.16%
consistency_top_sec: 47.83%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 20.40%
              precision    recall  f1-score   support

    Temporal     0.5556    0.2206    0.3158        68
 Contingency     0.5736    0.5421    0.5574       273
  Comparison     0.5210    0.4306    0.4715       144
   Expansion     0.6787    0.7780    0.7250       554

    accuracy                         0.6314      1039
   macro avg     0.5822    0.4928    0.5174      1039
weighted avg     0.6212    0.6314    0.6190      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6538    0.3148    0.4250        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5756    0.5843    0.5799       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4711    0.4453    0.4578       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4335    0.5700    0.4924       200
    Expansion.Instantiation     0.6869    0.5763    0.6267       118
      Expansion.Restatement     0.4487    0.4930    0.4698       213
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5034      1039
                  macro avg     0.3293    0.3318    0.3194      1039
               weighted avg     0.4964    0.5034    0.4938      1039

Epoch [22/30]
top-down:TOP: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 61.89%, Val F1: 50.72% Time: 44.20862793922424 
top-down:SEC: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 47.21%, Val F1: 31.03% Time: 44.20862793922424 
top-down:CONN: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   7.7,  Val Acc: 27.90%, Val F1:  8.93% Time: 44.20862793922424 
 
 
top-down:TOP: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 60.43%, Val F1: 49.86% Time: 135.1903018951416 
top-down:SEC: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.8,  Val Acc: 46.01%, Val F1: 29.87% Time: 135.1903018951416 
top-down:CONN: Iter:   8400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   7.8,  Val Acc: 26.01%, Val F1:  8.49% Time: 135.1903018951416 
 
 
top-down:TOP: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 61.20%, Val F1: 49.29% Time: 226.68068385124207 
top-down:SEC: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   7.8,  Val Acc: 46.78%, Val F1: 31.24% Time: 226.68068385124207 
top-down:CONN: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   7.8,  Val Acc: 27.47%, Val F1:  8.97% Time: 226.68068385124207 
 
 
top-down:TOP: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.8,  Val Acc: 61.63%, Val F1: 51.36% Time: 317.8824870586395 
top-down:SEC: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   7.8,  Val Acc: 49.27%, Val F1: 31.79% Time: 317.8824870586395 
top-down:CONN: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   7.8,  Val Acc: 27.47%, Val F1:  9.26% Time: 317.8824870586395 
 
 
Train time usage: 359.4854106903076
Test time usage: 2.371469259262085
TOP: Test Loss:   7.3,  Test Acc: 63.81%, Test F1: 53.88%
SEC: Test Loss:   7.3,  Test Acc: 49.09%, Test F1: 32.03%
CONN: Test Loss:   7.3,  Test Acc: 24.16%, Test F1:  9.15%
consistency_top_sec: 47.45%,  consistency_sec_conn: 18.96%, consistency_top_sec_conn: 18.48%
              precision    recall  f1-score   support

    Temporal     0.5238    0.3188    0.3964        69
 Contingency     0.5875    0.5146    0.5486       274
  Comparison     0.5446    0.4236    0.4766       144
   Expansion     0.6806    0.7953    0.7335       552

    accuracy                         0.6381      1039
   macro avg     0.5841    0.5131    0.5388      1039
weighted avg     0.6268    0.6381    0.6268      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5128    0.3636    0.4255        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5698    0.5485    0.5589       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4696    0.4219    0.4444       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4137    0.5150    0.4588       200
    Expansion.Instantiation     0.6549    0.6325    0.6435       117
      Expansion.Restatement     0.4435    0.5000    0.4701       212
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4909      1039
                  macro avg     0.3175    0.3317    0.3203      1039
               weighted avg     0.4795    0.4909    0.4827      1039

Epoch [23/30]
top-down:TOP: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 61.55%, Val F1: 50.14% Time: 50.13456916809082 
top-down:SEC: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 47.98%, Val F1: 30.85% Time: 50.13456916809082 
top-down:CONN: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   7.8,  Val Acc: 27.55%, Val F1:  9.01% Time: 50.13456916809082 
 
 
top-down:TOP: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 61.03%, Val F1: 50.60% Time: 141.2580235004425 
top-down:SEC: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 46.95%, Val F1: 30.03% Time: 141.2580235004425 
top-down:CONN: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   7.8,  Val Acc: 27.21%, Val F1:  8.88% Time: 141.2580235004425 
 
 
top-down:TOP: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 60.86%, Val F1: 50.07% Time: 232.61229276657104 
top-down:SEC: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 47.47%, Val F1: 31.12% Time: 232.61229276657104 
top-down:CONN: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   7.8,  Val Acc: 27.47%, Val F1:  8.93% Time: 232.61229276657104 
 
 
top-down:TOP: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 61.46%, Val F1: 50.95% Time: 323.82393884658813 
top-down:SEC: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 48.07%, Val F1: 31.39% Time: 323.82393884658813 
top-down:CONN: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   7.8,  Val Acc: 27.98%, Val F1:  8.92% Time: 323.82393884658813 
 
 
Train time usage: 359.2797751426697
Test time usage: 2.3736274242401123
TOP: Test Loss:   7.4,  Test Acc: 63.91%, Test F1: 53.58%
SEC: Test Loss:   7.4,  Test Acc: 49.09%, Test F1: 31.71%
CONN: Test Loss:   7.4,  Test Acc: 25.12%, Test F1:  9.32%
consistency_top_sec: 46.87%,  consistency_sec_conn: 20.12%, consistency_top_sec_conn: 19.44%
              precision    recall  f1-score   support

    Temporal     0.5641    0.3188    0.4074        69
 Contingency     0.6055    0.4835    0.5377       273
  Comparison     0.5421    0.4028    0.4622       144
   Expansion     0.6696    0.8174    0.7362       553

    accuracy                         0.6391      1039
   macro avg     0.5953    0.5056    0.5358      1039
weighted avg     0.6281    0.6391    0.6242      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5385    0.3818    0.4468        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5870    0.5056    0.5433       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4862    0.4141    0.4473       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4124    0.5650    0.4768       200
    Expansion.Instantiation     0.6441    0.6441    0.6441       118
      Expansion.Restatement     0.4398    0.5000    0.4680       212
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4909      1039
                  macro avg     0.3146    0.3343    0.3171      1039
               weighted avg     0.4846    0.4909    0.4828      1039

Epoch [24/30]
top-down:TOP: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 61.55%, Val F1: 49.44% Time: 56.59948205947876 
top-down:SEC: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 47.55%, Val F1: 31.62% Time: 56.59948205947876 
top-down:CONN: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 68.75%,Val Loss:   7.8,  Val Acc: 27.64%, Val F1:  9.15% Time: 56.59948205947876 
 
 
top-down:TOP: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 61.89%, Val F1: 50.25% Time: 147.78765439987183 
top-down:SEC: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 46.78%, Val F1: 31.37% Time: 147.78765439987183 
top-down:CONN: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 75.00%,Val Loss:   7.9,  Val Acc: 28.50%, Val F1:  9.59% Time: 147.78765439987183 
 
 
top-down:TOP: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.97%, Val F1: 50.11% Time: 238.8336591720581 
top-down:SEC: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 46.35%, Val F1: 31.30% Time: 238.8336591720581 
top-down:CONN: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 27.64%, Val F1:  9.32% Time: 238.8336591720581 
 
 
top-down:TOP: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.89%, Val F1: 50.85% Time: 330.1513702869415 
top-down:SEC: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 46.35%, Val F1: 29.79% Time: 330.1513702869415 
top-down:CONN: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 62.50%,Val Loss:   7.9,  Val Acc: 27.55%, Val F1:  8.99% Time: 330.1513702869415 
 
 
Train time usage: 359.5070815086365
Test time usage: 2.335108995437622
TOP: Test Loss:   7.4,  Test Acc: 63.81%, Test F1: 54.61%
SEC: Test Loss:   7.4,  Test Acc: 49.95%, Test F1: 32.43%
CONN: Test Loss:   7.4,  Test Acc: 24.45%, Test F1:  9.37%
consistency_top_sec: 47.74%,  consistency_sec_conn: 19.15%, consistency_top_sec_conn: 18.58%
              precision    recall  f1-score   support

    Temporal     0.5750    0.3333    0.4220        69
 Contingency     0.5882    0.5109    0.5469       274
  Comparison     0.5200    0.4514    0.4833       144
   Expansion     0.6840    0.7880    0.7323       552

    accuracy                         0.6381      1039
   macro avg     0.5918    0.5209    0.5461      1039
weighted avg     0.6288    0.6381    0.6283      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5263    0.3636    0.4301        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5906    0.5597    0.5747       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4609    0.4609    0.4609       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4195    0.5600    0.4797       200
    Expansion.Instantiation     0.6729    0.6154    0.6429       117
      Expansion.Restatement     0.4541    0.4670    0.4605       212
      Expansion.Alternative     0.3889    0.7778    0.5185         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4995      1039
                  macro avg     0.3194    0.3459    0.3243      1039
               weighted avg     0.4895    0.4995    0.4910      1039

Epoch [25/30]
top-down:TOP: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.97%, Val F1: 50.62% Time: 62.7812397480011 
top-down:SEC: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 48.24%, Val F1: 30.93% Time: 62.7812397480011 
top-down:CONN: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   7.9,  Val Acc: 27.81%, Val F1:  9.16% Time: 62.7812397480011 
 
 
top-down:TOP: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 61.29%, Val F1: 50.28% Time: 153.97422885894775 
top-down:SEC: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 46.87%, Val F1: 29.72% Time: 153.97422885894775 
top-down:CONN: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 27.73%, Val F1:  9.17% Time: 153.97422885894775 
 
 
top-down:TOP: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.55%, Val F1: 50.56% Time: 245.37371587753296 
top-down:SEC: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 46.18%, Val F1: 30.31% Time: 245.37371587753296 
top-down:CONN: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 59.38%,Val Loss:   8.0,  Val Acc: 27.98%, Val F1:  9.28% Time: 245.37371587753296 
 
 
top-down:TOP: Iter:   9800,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 62.23%, Val F1: 51.41% Time: 336.6643705368042 
top-down:SEC: Iter:   9800,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 47.64%, Val F1: 31.26% Time: 336.6643705368042 
top-down:CONN: Iter:   9800,  Train Loss: 2.3e+01,  Train Acc: 62.50%,Val Loss:   7.9,  Val Acc: 28.58%, Val F1:  9.05% Time: 336.6643705368042 
 
 
Train time usage: 359.721431016922
Test time usage: 2.349334239959717
TOP: Test Loss:   7.5,  Test Acc: 63.81%, Test F1: 54.35%
SEC: Test Loss:   7.5,  Test Acc: 49.37%, Test F1: 32.12%
CONN: Test Loss:   7.5,  Test Acc: 23.97%, Test F1:  9.24%
consistency_top_sec: 47.45%,  consistency_sec_conn: 18.67%, consistency_top_sec_conn: 18.00%
              precision    recall  f1-score   support

    Temporal     0.5610    0.3333    0.4182        69
 Contingency     0.5945    0.4725    0.5265       273
  Comparison     0.5366    0.4583    0.4944       144
   Expansion     0.6763    0.8047    0.7349       553

    accuracy                         0.6381      1039
   macro avg     0.5921    0.5172    0.5435      1039
weighted avg     0.6278    0.6381    0.6258      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5000    0.3636    0.4211        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5991    0.5000    0.5451       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4609    0.4609    0.4609       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4158    0.5800    0.4843       200
    Expansion.Instantiation     0.6230    0.6441    0.6333       118
      Expansion.Restatement     0.4615    0.4789    0.4700       213
      Expansion.Alternative     0.3889    0.7778    0.5185         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4937      1039
                  macro avg     0.3136    0.3459    0.3212      1039
               weighted avg     0.4854    0.4937    0.4846      1039

Epoch [26/30]
top-down:TOP: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.29%, Val F1: 51.24% Time: 69.14301490783691 
top-down:SEC: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 47.21%, Val F1: 31.74% Time: 69.14301490783691 
top-down:CONN: Iter:   9900,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   8.0,  Val Acc: 27.30%, Val F1:  8.93% Time: 69.14301490783691 
 
 
top-down:TOP: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.55%, Val F1: 50.59% Time: 160.2200963497162 
top-down:SEC: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 46.52%, Val F1: 30.20% Time: 160.2200963497162 
top-down:CONN: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   8.0,  Val Acc: 27.81%, Val F1:  9.22% Time: 160.2200963497162 
 
 
top-down:TOP: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.29%, Val F1: 50.91% Time: 251.65708446502686 
top-down:SEC: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 46.95%, Val F1: 30.71% Time: 251.65708446502686 
top-down:CONN: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   7.9,  Val Acc: 28.15%, Val F1:  9.04% Time: 251.65708446502686 
 
 
top-down:TOP: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.20%, Val F1: 50.18% Time: 342.62031722068787 
top-down:SEC: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 47.21%, Val F1: 31.05% Time: 342.62031722068787 
top-down:CONN: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   8.0,  Val Acc: 27.38%, Val F1:  8.96% Time: 342.62031722068787 
 
 
Train time usage: 359.66139364242554
Test time usage: 2.35744047164917
TOP: Test Loss:   7.5,  Test Acc: 64.20%, Test F1: 55.04%
SEC: Test Loss:   7.5,  Test Acc: 50.34%, Test F1: 32.54%
CONN: Test Loss:   7.5,  Test Acc: 24.45%, Test F1:  9.37%
consistency_top_sec: 48.51%,  consistency_sec_conn: 19.63%, consistency_top_sec_conn: 19.25%
              precision    recall  f1-score   support

    Temporal     0.5750    0.3333    0.4220        69
 Contingency     0.5882    0.5474    0.5671       274
  Comparison     0.5339    0.4375    0.4809       144
   Expansion     0.6885    0.7808    0.7317       552

    accuracy                         0.6420      1039
   macro avg     0.5964    0.5248    0.5504      1039
weighted avg     0.6331    0.6420    0.6330      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5556    0.3636    0.4396        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5779    0.5714    0.5747       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4831    0.4453    0.4634       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4198    0.5500    0.4762       200
    Expansion.Instantiation     0.6757    0.6356    0.6550       118
      Expansion.Restatement     0.4578    0.4836    0.4703       213
      Expansion.Alternative     0.4000    0.6667    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5034      1039
                  macro avg     0.3245    0.3378    0.3254      1039
               weighted avg     0.4917    0.5034    0.4943      1039

Epoch [27/30]
top-down:TOP: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.20%, Val F1: 50.25% Time: 75.17632865905762 
top-down:SEC: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.0,  Val Acc: 47.47%, Val F1: 30.43% Time: 75.17632865905762 
top-down:CONN: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   8.0,  Val Acc: 27.55%, Val F1:  9.22% Time: 75.17632865905762 
 
 
top-down:TOP: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.29%, Val F1: 50.06% Time: 166.19439482688904 
top-down:SEC: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 47.90%, Val F1: 31.33% Time: 166.19439482688904 
top-down:CONN: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   8.0,  Val Acc: 27.21%, Val F1:  8.87% Time: 166.19439482688904 
 
 
top-down:TOP: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.46%, Val F1: 50.70% Time: 257.42091488838196 
top-down:SEC: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 47.30%, Val F1: 31.19% Time: 257.42091488838196 
top-down:CONN: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   7.9,  Val Acc: 27.64%, Val F1:  9.10% Time: 257.42091488838196 
 
 
top-down:TOP: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 61.37%, Val F1: 50.54% Time: 348.5582013130188 
top-down:SEC: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   7.9,  Val Acc: 47.21%, Val F1: 30.69% Time: 348.5582013130188 
top-down:CONN: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 71.88%,Val Loss:   7.9,  Val Acc: 27.30%, Val F1:  8.84% Time: 348.5582013130188 
 
 
Train time usage: 359.2823283672333
Test time usage: 2.358915090560913
TOP: Test Loss:   7.5,  Test Acc: 64.00%, Test F1: 55.63%
SEC: Test Loss:   7.5,  Test Acc: 49.66%, Test F1: 32.40%
CONN: Test Loss:   7.5,  Test Acc: 24.93%, Test F1:  9.29%
consistency_top_sec: 47.35%,  consistency_sec_conn: 19.73%, consistency_top_sec_conn: 18.96%
              precision    recall  f1-score   support

    Temporal     0.6047    0.3768    0.4643        69
 Contingency     0.5941    0.5182    0.5536       274
  Comparison     0.5250    0.4375    0.4773       144
   Expansion     0.6813    0.7862    0.7300       552

    accuracy                         0.6400      1039
   macro avg     0.6013    0.5297    0.5563      1039
weighted avg     0.6316    0.6400    0.6308      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5385    0.3818    0.4468        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5771    0.5468    0.5615       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4672    0.4453    0.4560       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4129    0.5450    0.4698       200
    Expansion.Instantiation     0.6410    0.6410    0.6410       117
      Expansion.Restatement     0.4654    0.4742    0.4698       213
      Expansion.Alternative     0.3889    0.7778    0.5185         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3174    0.3465    0.3240      1039
               weighted avg     0.4848    0.4966    0.4876      1039

Epoch [28/30]
top-down:TOP: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.29%, Val F1: 49.48% Time: 81.40628004074097 
top-down:SEC: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 47.38%, Val F1: 30.08% Time: 81.40628004074097 
top-down:CONN: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   8.0,  Val Acc: 27.81%, Val F1:  8.97% Time: 81.40628004074097 
 
 
top-down:TOP: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.63%, Val F1: 51.15% Time: 172.51992273330688 
top-down:SEC: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.0,  Val Acc: 47.64%, Val F1: 31.55% Time: 172.51992273330688 
top-down:CONN: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   8.0,  Val Acc: 27.21%, Val F1:  8.89% Time: 172.51992273330688 
 
 
top-down:TOP: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.55%, Val F1: 50.92% Time: 263.8934500217438 
top-down:SEC: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 47.64%, Val F1: 30.78% Time: 263.8934500217438 
top-down:CONN: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   8.0,  Val Acc: 27.21%, Val F1:  8.91% Time: 263.8934500217438 
 
 
top-down:TOP: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 62.06%, Val F1: 50.97% Time: 354.8613886833191 
top-down:SEC: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 47.47%, Val F1: 30.89% Time: 354.8613886833191 
top-down:CONN: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 27.47%, Val F1:  9.13% Time: 354.8613886833191 
 
 
Train time usage: 359.3544807434082
Test time usage: 2.389111042022705
TOP: Test Loss:   7.5,  Test Acc: 64.20%, Test F1: 55.10%
SEC: Test Loss:   7.5,  Test Acc: 49.47%, Test F1: 31.23%
CONN: Test Loss:   7.5,  Test Acc: 23.48%, Test F1:  8.72%
consistency_top_sec: 47.55%,  consistency_sec_conn: 18.38%, consistency_top_sec_conn: 18.00%
              precision    recall  f1-score   support

    Temporal     0.5854    0.3478    0.4364        69
 Contingency     0.6079    0.5036    0.5509       274
  Comparison     0.5289    0.4444    0.4830       144
   Expansion     0.6785    0.7989    0.7338       552

    accuracy                         0.6420      1039
   macro avg     0.6002    0.5237    0.5510      1039
weighted avg     0.6330    0.6420    0.6310      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5263    0.3636    0.4301        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5907    0.5243    0.5556       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4567    0.4531    0.4549       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4126    0.5550    0.4733       200
    Expansion.Instantiation     0.6522    0.6410    0.6466       117
      Expansion.Restatement     0.4585    0.4930    0.4751       213
      Expansion.Alternative     0.3125    0.5556    0.4000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4947      1039
                  macro avg     0.3100    0.3260    0.3123      1039
               weighted avg     0.4855    0.4947    0.4864      1039

Epoch [29/30]
top-down:TOP: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 61.37%, Val F1: 50.62% Time: 87.688312292099 
top-down:SEC: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 47.38%, Val F1: 31.45% Time: 87.688312292099 
top-down:CONN: Iter:  11100,  Train Loss: 3.4e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 27.73%, Val F1:  9.15% Time: 87.688312292099 
 
 
top-down:TOP: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.55%, Val F1: 50.65% Time: 178.79349279403687 
top-down:SEC: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 47.30%, Val F1: 30.56% Time: 178.79349279403687 
top-down:CONN: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   8.0,  Val Acc: 27.55%, Val F1:  8.92% Time: 178.79349279403687 
 
 
top-down:TOP: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.89%, Val F1: 50.66% Time: 270.0368573665619 
top-down:SEC: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 47.47%, Val F1: 31.47% Time: 270.0368573665619 
top-down:CONN: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   8.0,  Val Acc: 27.73%, Val F1:  9.16% Time: 270.0368573665619 
 
 
Train time usage: 356.7311234474182
Test time usage: 2.3473050594329834
TOP: Test Loss:   7.5,  Test Acc: 64.49%, Test F1: 55.38%
SEC: Test Loss:   7.5,  Test Acc: 49.66%, Test F1: 32.19%
CONN: Test Loss:   7.5,  Test Acc: 23.58%, Test F1:  9.08%
consistency_top_sec: 48.03%,  consistency_sec_conn: 18.48%, consistency_top_sec_conn: 18.09%
              precision    recall  f1-score   support

    Temporal     0.5714    0.3478    0.4324        69
 Contingency     0.6104    0.5146    0.5584       274
  Comparison     0.5197    0.4583    0.4871       144
   Expansion     0.6870    0.7953    0.7372       552

    accuracy                         0.6449      1039
   macro avg     0.5971    0.5290    0.5538      1039
weighted avg     0.6359    0.6449    0.6351      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5263    0.3636    0.4301        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5949    0.5301    0.5606       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4504    0.4609    0.4556       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4186    0.5400    0.4716       200
    Expansion.Instantiation     0.6552    0.6441    0.6496       118
      Expansion.Restatement     0.4545    0.4930    0.4730       213
      Expansion.Alternative     0.3684    0.7778    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3153    0.3463    0.3219      1039
               weighted avg     0.4870    0.4966    0.4883      1039

Epoch [30/30]
top-down:TOP: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 61.46%, Val F1: 50.52% Time: 5.313251733779907 
top-down:SEC: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   8.0,  Val Acc: 46.52%, Val F1: 30.79% Time: 5.313251733779907 
top-down:CONN: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   8.0,  Val Acc: 27.55%, Val F1:  8.99% Time: 5.313251733779907 
 
 
top-down:TOP: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.55%, Val F1: 50.32% Time: 94.88483357429504 
top-down:SEC: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 47.21%, Val F1: 30.93% Time: 94.88483357429504 
top-down:CONN: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   8.0,  Val Acc: 27.64%, Val F1:  9.04% Time: 94.88483357429504 
 
 
top-down:TOP: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.72%, Val F1: 50.53% Time: 186.09381437301636 
top-down:SEC: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   8.0,  Val Acc: 46.78%, Val F1: 30.63% Time: 186.09381437301636 
top-down:CONN: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   8.0,  Val Acc: 27.90%, Val F1:  9.05% Time: 186.09381437301636 
 
 
top-down:TOP: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 61.37%, Val F1: 49.89% Time: 277.2691705226898 
top-down:SEC: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 46.61%, Val F1: 30.62% Time: 277.2691705226898 
top-down:CONN: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   8.0,  Val Acc: 27.64%, Val F1:  8.93% Time: 277.2691705226898 
 
 
Train time usage: 357.5909140110016
Test time usage: 2.3890740871429443
TOP: Test Loss:   7.6,  Test Acc: 64.10%, Test F1: 54.79%
SEC: Test Loss:   7.6,  Test Acc: 49.66%, Test F1: 31.85%
CONN: Test Loss:   7.6,  Test Acc: 23.48%, Test F1:  8.84%
consistency_top_sec: 48.12%,  consistency_sec_conn: 18.29%, consistency_top_sec_conn: 18.19%
              precision    recall  f1-score   support

    Temporal     0.5750    0.3333    0.4220        69
 Contingency     0.6070    0.5073    0.5527       274
  Comparison     0.5200    0.4514    0.4833       144
   Expansion     0.6806    0.7953    0.7335       552

    accuracy                         0.6410      1039
   macro avg     0.5957    0.5218    0.5479      1039
weighted avg     0.6319    0.6410    0.6305      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5263    0.3636    0.4301        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5907    0.5263    0.5567       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4524    0.4453    0.4488       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4183    0.5500    0.4752       200
    Expansion.Instantiation     0.6609    0.6441    0.6524       118
      Expansion.Restatement     0.4573    0.5023    0.4787       213
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3144    0.3362    0.3185      1039
               weighted avg     0.4872    0.4966    0.4883      1039

dev_best_acc_top: 63.35%,  dev_best_f1_top: 53.01%, 
dev_best_acc_sec: 50.73%,  dev_best_f1_sec: 32.25%, 
dev_best_acc_conn: 28.33%,  dev_best_f1_conn:  7.56%
