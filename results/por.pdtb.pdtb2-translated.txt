nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_pt/data/', 'log_file': 'data/pdtb_pt/log/', 'save_file': 'data/pdtb_pt/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 30, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March07-08:57:07', 'log': 'data/pdtb_pt/log/March07-08:57:07.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]109it [00:00, 1085.35it/s]302it [00:00, 1578.88it/s]491it [00:00, 1719.01it/s]695it [00:00, 1841.99it/s]899it [00:00, 1910.71it/s]1092it [00:00, 1916.85it/s]1294it [00:00, 1949.20it/s]1489it [00:00, 1948.44it/s]1684it [00:00, 1846.78it/s]1871it [00:01, 1851.38it/s]2068it [00:01, 1886.22it/s]2258it [00:01, 1878.14it/s]2447it [00:01, 1856.70it/s]2640it [00:01, 1877.82it/s]2829it [00:01, 1856.30it/s]3015it [00:01, 1857.20it/s]3201it [00:01, 1854.97it/s]3396it [00:01, 1882.36it/s]3585it [00:01, 1862.43it/s]3774it [00:02, 1869.23it/s]3962it [00:02, 1852.02it/s]4148it [00:02, 1848.77it/s]4333it [00:02, 1797.91it/s]4514it [00:02, 1759.44it/s]4695it [00:02, 1773.18it/s]4875it [00:02, 1776.68it/s]5061it [00:02, 1799.81it/s]5242it [00:03, 1247.58it/s]5390it [00:03, 1276.87it/s]5585it [00:03, 1438.77it/s]5771it [00:03, 1545.73it/s]5976it [00:03, 1678.79it/s]6161it [00:03, 1726.08it/s]6352it [00:03, 1776.17it/s]6536it [00:03, 1793.83it/s]6720it [00:03, 1779.49it/s]6902it [00:03, 1790.52it/s]7091it [00:04, 1818.64it/s]7281it [00:04, 1841.00it/s]7467it [00:04, 1721.43it/s]7650it [00:04, 1751.11it/s]7843it [00:04, 1800.98it/s]8025it [00:04, 1783.11it/s]8209it [00:04, 1798.62it/s]8391it [00:04, 1803.64it/s]8575it [00:04, 1812.54it/s]8765it [00:04, 1837.37it/s]8950it [00:05, 1823.54it/s]9142it [00:05, 1851.76it/s]9328it [00:05, 1784.47it/s]9511it [00:05, 1795.50it/s]9708it [00:05, 1844.54it/s]9899it [00:05, 1862.96it/s]10086it [00:05, 1863.26it/s]10273it [00:05, 1831.67it/s]10473it [00:05, 1879.74it/s]10662it [00:05, 1838.22it/s]10847it [00:06, 1832.70it/s]11039it [00:06, 1855.60it/s]11225it [00:06, 1801.72it/s]11411it [00:06, 1817.56it/s]11594it [00:06, 1803.35it/s]11775it [00:06, 1796.11it/s]11955it [00:06, 1788.03it/s]12134it [00:06, 1705.13it/s]12314it [00:06, 1730.27it/s]12488it [00:07, 1730.31it/s]12547it [00:07, 1777.57it/s]
0it [00:00, ?it/s]169it [00:00, 1685.44it/s]356it [00:00, 1786.49it/s]535it [00:00, 1754.87it/s]711it [00:00, 1750.78it/s]891it [00:00, 1767.38it/s]1068it [00:00, 1765.65it/s]1165it [00:00, 1756.61it/s]
0it [00:00, ?it/s]178it [00:00, 1769.64it/s]381it [00:00, 1919.22it/s]580it [00:00, 1950.16it/s]776it [00:00, 1914.12it/s]968it [00:00, 1908.54it/s]1039it [00:00, 1915.87it/s]
Time usage: 18.287198781967163
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/30]
top-down:TOP: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   7.5,  Val Acc: 55.88%, Val F1: 17.92% Time: 88.59306335449219 *
top-down:SEC: Iter:    100,  Train Loss: 3.1e+01,  Train Acc: 21.88%,Val Loss:   7.5,  Val Acc: 24.21%, Val F1:  3.54% Time: 88.59306335449219 *
top-down:CONN: Iter:    100,  Train Loss: 3.1e+01,  Train Acc:  0.00%,Val Loss:   7.5,  Val Acc: 12.53%, Val F1:  0.34% Time: 88.59306335449219 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   6.6,  Val Acc: 55.79%, Val F1: 17.91% Time: 169.69357180595398 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   6.6,  Val Acc: 28.58%, Val F1:  6.60% Time: 169.69357180595398 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 12.50%,Val Loss:   6.6,  Val Acc: 12.88%, Val F1:  0.34% Time: 169.69357180595398 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 65.62%,Val Loss:   6.4,  Val Acc: 56.39%, Val F1: 26.26% Time: 250.67354226112366 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 28.12%,Val Loss:   6.4,  Val Acc: 31.50%, Val F1: 10.00% Time: 250.67354226112366 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 12.50%,Val Loss:   6.4,  Val Acc: 15.62%, Val F1:  1.01% Time: 250.67354226112366 *
 
 
Train time usage: 323.96481919288635
Test time usage: 1.7341351509094238
TOP: Test Loss:   6.2,  Test Acc: 55.15%, Test F1: 23.71%
SEC: Test Loss:   6.2,  Test Acc: 34.55%, Test F1: 11.34%
CONN: Test Loss:   6.2,  Test Acc: 15.88%, Test F1:  1.15%
consistency_top_sec: 22.23%,  consistency_sec_conn: 12.99%, consistency_top_sec_conn:  7.51%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        68
 Contingency     0.5301    0.1624    0.2486       271
  Comparison     0.0000    0.0000    0.0000       144
   Expansion     0.5533    0.9514    0.6997       556

    accuracy                         0.5515      1039
   macro avg     0.2709    0.2785    0.2371      1039
weighted avg     0.4344    0.5515    0.4393      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4565    0.6455    0.5348       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.0000    0.0000    0.0000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4251    0.3550    0.3869       200
    Expansion.Instantiation     0.0000    0.0000    0.0000       118
      Expansion.Restatement     0.2333    0.5425    0.3262       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.3455      1039
                  macro avg     0.1014    0.1403    0.1134      1039
               weighted avg     0.2472    0.3455    0.2790      1039

Epoch [2/30]
top-down:TOP: Iter:    400,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.3,  Val Acc: 53.82%, Val F1: 26.56% Time: 9.024112939834595 *
top-down:SEC: Iter:    400,  Train Loss: 3e+01,  Train Acc: 25.00%,Val Loss:   6.3,  Val Acc: 34.76%, Val F1: 10.57% Time: 9.024112939834595 *
top-down:CONN: Iter:    400,  Train Loss: 3e+01,  Train Acc:  9.38%,Val Loss:   6.3,  Val Acc: 15.62%, Val F1:  1.27% Time: 9.024112939834595 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   6.2,  Val Acc: 50.82%, Val F1: 27.40% Time: 90.05541014671326 *
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.2,  Val Acc: 35.79%, Val F1: 13.11% Time: 90.05541014671326 *
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 15.62%,Val Loss:   6.2,  Val Acc: 20.26%, Val F1:  2.02% Time: 90.05541014671326 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   6.0,  Val Acc: 53.99%, Val F1: 30.44% Time: 171.04195547103882 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   6.0,  Val Acc: 39.06%, Val F1: 17.88% Time: 171.04195547103882 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 25.00%,Val Loss:   6.0,  Val Acc: 20.34%, Val F1:  2.49% Time: 171.04195547103882 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   5.9,  Val Acc: 54.59%, Val F1: 38.60% Time: 251.9782738685608 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   5.9,  Val Acc: 39.48%, Val F1: 17.21% Time: 251.9782738685608 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 21.88%,Val Loss:   5.9,  Val Acc: 21.29%, Val F1:  3.03% Time: 251.9782738685608 *
 
 
Train time usage: 319.70697140693665
Test time usage: 1.7429399490356445
TOP: Test Loss:   5.4,  Test Acc: 59.19%, Test F1: 45.14%
SEC: Test Loss:   5.4,  Test Acc: 45.43%, Test F1: 25.03%
CONN: Test Loss:   5.4,  Test Acc: 25.02%, Test F1:  4.01%
consistency_top_sec: 37.15%,  consistency_sec_conn: 21.37%, consistency_top_sec_conn: 17.90%
              precision    recall  f1-score   support

    Temporal     0.3789    0.5294    0.4417        68
 Contingency     0.5285    0.2399    0.3299       271
  Comparison     0.6304    0.2014    0.3053       144
   Expansion     0.6258    0.8723    0.7288       556

    accuracy                         0.5919      1039
   macro avg     0.5409    0.4607    0.4514      1039
weighted avg     0.5849    0.5919    0.5473      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3248    0.7037    0.4444        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5073    0.5206    0.5139       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5238    0.2578    0.3455       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4565    0.6300    0.5294       200
    Expansion.Instantiation     0.5926    0.6723    0.6299       119
      Expansion.Restatement     0.3218    0.2642    0.2902       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4543      1039
                  macro avg     0.2479    0.2771    0.2503      1039
               weighted avg     0.4332    0.4543    0.4310      1039

Epoch [3/30]
top-down:TOP: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   5.7,  Val Acc: 56.48%, Val F1: 47.90% Time: 14.46273946762085 *
top-down:SEC: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 42.58%, Val F1: 22.16% Time: 14.46273946762085 *
top-down:CONN: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 18.75%,Val Loss:   5.7,  Val Acc: 20.86%, Val F1:  3.45% Time: 14.46273946762085 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.6,  Val Acc: 59.57%, Val F1: 44.85% Time: 95.22596335411072 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 44.12%, Val F1: 22.37% Time: 95.22596335411072 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 15.62%,Val Loss:   5.6,  Val Acc: 24.12%, Val F1:  3.57% Time: 95.22596335411072 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   5.5,  Val Acc: 61.80%, Val F1: 42.51% Time: 175.8630931377411 *
top-down:SEC: Iter:   1000,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 46.52%, Val F1: 23.63% Time: 175.8630931377411 *
top-down:CONN: Iter:   1000,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   5.5,  Val Acc: 24.81%, Val F1:  4.20% Time: 175.8630931377411 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 61.20%, Val F1: 45.18% Time: 255.1531274318695 
top-down:SEC: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 44.38%, Val F1: 22.65% Time: 255.1531274318695 
top-down:CONN: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 24.03%, Val F1:  3.91% Time: 255.1531274318695 
 
 
Train time usage: 317.4369478225708
Test time usage: 1.7355751991271973
TOP: Test Loss:   5.2,  Test Acc: 62.85%, Test F1: 53.37%
SEC: Test Loss:   5.2,  Test Acc: 48.60%, Test F1: 26.90%
CONN: Test Loss:   5.2,  Test Acc: 22.62%, Test F1:  4.32%
consistency_top_sec: 43.89%,  consistency_sec_conn: 18.96%, consistency_top_sec_conn: 17.52%
              precision    recall  f1-score   support

    Temporal     0.4655    0.3971    0.4286        68
 Contingency     0.5862    0.4359    0.5000       273
  Comparison     0.5122    0.4375    0.4719       144
   Expansion     0.6779    0.8014    0.7345       554

    accuracy                         0.6285      1039
   macro avg     0.5604    0.5180    0.5337      1039
weighted avg     0.6169    0.6285    0.6165      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4098    0.4630    0.4348        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5096    0.5911    0.5473       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4406    0.4922    0.4649       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4495    0.6450    0.5298       200
    Expansion.Instantiation     0.7500    0.5085    0.6061       118
      Expansion.Restatement     0.4423    0.3270    0.3760       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4860      1039
                  macro avg     0.2729    0.2752    0.2690      1039
               weighted avg     0.4690    0.4860    0.4688      1039

Epoch [4/30]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.5,  Val Acc: 61.63%, Val F1: 50.92% Time: 19.854313611984253 *
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 48.50%, Val F1: 25.35% Time: 19.854313611984253 *
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 37.50%,Val Loss:   5.5,  Val Acc: 26.09%, Val F1:  4.49% Time: 19.854313611984253 *
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 64.38%, Val F1: 49.50% Time: 100.85915946960449 *
top-down:SEC: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 56.25%,Val Loss:   5.4,  Val Acc: 48.84%, Val F1: 28.87% Time: 100.85915946960449 *
top-down:CONN: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 46.88%,Val Loss:   5.4,  Val Acc: 26.35%, Val F1:  4.64% Time: 100.85915946960449 *
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 62.32%, Val F1: 52.89% Time: 181.83688354492188 *
top-down:SEC: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 49.18%, Val F1: 30.54% Time: 181.83688354492188 *
top-down:CONN: Iter:   1400,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   5.4,  Val Acc: 27.38%, Val F1:  5.17% Time: 181.83688354492188 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 61.63%, Val F1: 50.07% Time: 261.38727498054504 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 37.50%,Val Loss:   5.4,  Val Acc: 47.98%, Val F1: 29.76% Time: 261.38727498054504 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 26.52%, Val F1:  5.14% Time: 261.38727498054504 
 
 
Train time usage: 318.3594260215759
Test time usage: 1.7656323909759521
TOP: Test Loss:   5.1,  Test Acc: 64.00%, Test F1: 53.49%
SEC: Test Loss:   5.1,  Test Acc: 50.14%, Test F1: 30.16%
CONN: Test Loss:   5.1,  Test Acc: 23.77%, Test F1:  6.41%
consistency_top_sec: 46.49%,  consistency_sec_conn: 19.44%, consistency_top_sec_conn: 18.00%
              precision    recall  f1-score   support

    Temporal     0.6667    0.2647    0.3789        68
 Contingency     0.5611    0.4542    0.5020       273
  Comparison     0.5565    0.4792    0.5149       144
   Expansion     0.6807    0.8195    0.7437       554

    accuracy                         0.6400      1039
   macro avg     0.6162    0.5044    0.5349      1039
weighted avg     0.6311    0.6400    0.6246      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5625    0.3333    0.4186        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5081    0.5799    0.5417       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4812    0.5000    0.4904       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5333    0.5200    0.5266       200
    Expansion.Instantiation     0.6423    0.6695    0.6556       118
      Expansion.Restatement     0.4083    0.4645    0.4346       211
      Expansion.Alternative     0.2857    0.2222    0.2500         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5014      1039
                  macro avg     0.3110    0.2990    0.3016      1039
               weighted avg     0.4811    0.5014    0.4887      1039

Epoch [5/30]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   5.4,  Val Acc: 62.83%, Val F1: 51.32% Time: 23.562185525894165 
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   5.4,  Val Acc: 49.79%, Val F1: 27.10% Time: 23.562185525894165 
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   5.4,  Val Acc: 26.87%, Val F1:  5.54% Time: 23.562185525894165 
 
 
top-down:TOP: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   5.6,  Val Acc: 62.49%, Val F1: 52.70% Time: 102.76366758346558 
top-down:SEC: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 49.70%, Val F1: 30.65% Time: 102.76366758346558 
top-down:CONN: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 37.50%,Val Loss:   5.6,  Val Acc: 25.92%, Val F1:  5.19% Time: 102.76366758346558 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.5,  Val Acc: 61.03%, Val F1: 52.43% Time: 183.745197057724 *
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 49.61%, Val F1: 32.17% Time: 183.745197057724 *
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   5.5,  Val Acc: 26.78%, Val F1:  5.52% Time: 183.745197057724 *
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   5.4,  Val Acc: 63.18%, Val F1: 53.10% Time: 264.70177268981934 *
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.4,  Val Acc: 49.10%, Val F1: 31.44% Time: 264.70177268981934 *
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 27.90%, Val F1:  6.08% Time: 264.70177268981934 *
 
 
Train time usage: 316.3524844646454
Test time usage: 1.7345316410064697
TOP: Test Loss:   5.2,  Test Acc: 63.04%, Test F1: 53.43%
SEC: Test Loss:   5.2,  Test Acc: 50.43%, Test F1: 32.52%
CONN: Test Loss:   5.2,  Test Acc: 25.89%, Test F1:  7.42%
consistency_top_sec: 47.35%,  consistency_sec_conn: 21.37%, consistency_top_sec_conn: 20.02%
              precision    recall  f1-score   support

    Temporal     0.6176    0.3088    0.4118        68
 Contingency     0.5922    0.4453    0.5083       274
  Comparison     0.4412    0.5208    0.4777       144
   Expansion     0.6948    0.7902    0.7394       553

    accuracy                         0.6304      1039
   macro avg     0.5865    0.5163    0.5343      1039
weighted avg     0.6275    0.6304    0.6208      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5581    0.4444    0.4948        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5564    0.5316    0.5437       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4142    0.5469    0.4714       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5403    0.5700    0.5547       200
    Expansion.Instantiation     0.6449    0.5847    0.6133       118
      Expansion.Restatement     0.4231    0.4692    0.4449       211
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5043      1039
                  macro avg     0.3201    0.3366    0.3252      1039
               weighted avg     0.4906    0.5043    0.4953      1039

Epoch [6/30]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   5.5,  Val Acc: 64.12%, Val F1: 52.33% Time: 30.689287662506104 *
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 48.76%, Val F1: 30.64% Time: 30.689287662506104 *
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   5.5,  Val Acc: 28.76%, Val F1:  6.39% Time: 30.689287662506104 *
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   5.6,  Val Acc: 63.61%, Val F1: 51.35% Time: 109.95346808433533 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 56.25%,Val Loss:   5.6,  Val Acc: 49.87%, Val F1: 30.55% Time: 109.95346808433533 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   5.6,  Val Acc: 27.90%, Val F1:  6.20% Time: 109.95346808433533 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.5,  Val Acc: 63.26%, Val F1: 51.76% Time: 189.07157492637634 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 49.44%, Val F1: 29.17% Time: 189.07157492637634 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   5.5,  Val Acc: 28.41%, Val F1:  6.45% Time: 189.07157492637634 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 61.29%, Val F1: 51.23% Time: 268.44504833221436 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 68.75%,Val Loss:   5.6,  Val Acc: 48.15%, Val F1: 29.25% Time: 268.44504833221436 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 31.25%,Val Loss:   5.6,  Val Acc: 27.12%, Val F1:  6.65% Time: 268.44504833221436 
 
 
Train time usage: 314.4234824180603
Test time usage: 1.7206120491027832
TOP: Test Loss:   5.2,  Test Acc: 64.29%, Test F1: 55.20%
SEC: Test Loss:   5.2,  Test Acc: 49.66%, Test F1: 32.18%
CONN: Test Loss:   5.2,  Test Acc: 26.18%, Test F1:  7.70%
consistency_top_sec: 48.03%,  consistency_sec_conn: 21.37%, consistency_top_sec_conn: 20.89%
              precision    recall  f1-score   support

    Temporal     0.5349    0.3382    0.4144        68
 Contingency     0.6065    0.4781    0.5347       274
  Comparison     0.5286    0.5139    0.5211       144
   Expansion     0.6875    0.7957    0.7376       553

    accuracy                         0.6429      1039
   macro avg     0.5894    0.5315    0.5520      1039
weighted avg     0.6341    0.6429    0.6330      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4400    0.4074    0.4231        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5578    0.5224    0.5395       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4889    0.5156    0.5019       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4872    0.5700    0.5253       200
    Expansion.Instantiation     0.6396    0.6017    0.6201       118
      Expansion.Restatement     0.4016    0.4623    0.4298       212
      Expansion.Alternative     0.4545    0.5556    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3154    0.3304    0.3218      1039
               weighted avg     0.4793    0.4966    0.4866      1039

Epoch [7/30]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   5.8,  Val Acc: 62.75%, Val F1: 50.51% Time: 34.314409494400024 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   5.8,  Val Acc: 48.41%, Val F1: 30.28% Time: 34.314409494400024 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 21.88%,Val Loss:   5.8,  Val Acc: 26.87%, Val F1:  6.53% Time: 34.314409494400024 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   5.7,  Val Acc: 62.92%, Val F1: 52.06% Time: 113.44498991966248 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.7,  Val Acc: 49.01%, Val F1: 29.29% Time: 113.44498991966248 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 27.12%, Val F1:  6.09% Time: 113.44498991966248 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 62.92%, Val F1: 51.05% Time: 192.8413369655609 
top-down:SEC: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 47.64%, Val F1: 30.04% Time: 192.8413369655609 
top-down:CONN: Iter:   2600,  Train Loss: 3.3e+01,  Train Acc: 31.25%,Val Loss:   5.8,  Val Acc: 27.55%, Val F1:  6.01% Time: 192.8413369655609 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 75.00%,Val Loss:   5.8,  Val Acc: 63.09%, Val F1: 53.47% Time: 272.0959258079529 
top-down:SEC: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 62.50%,Val Loss:   5.8,  Val Acc: 47.55%, Val F1: 27.97% Time: 272.0959258079529 
top-down:CONN: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 40.62%,Val Loss:   5.8,  Val Acc: 27.90%, Val F1:  7.28% Time: 272.0959258079529 
 
 
Train time usage: 312.6123037338257
Test time usage: 1.7209768295288086
TOP: Test Loss:   5.3,  Test Acc: 65.45%, Test F1: 55.97%
SEC: Test Loss:   5.3,  Test Acc: 50.63%, Test F1: 31.03%
CONN: Test Loss:   5.3,  Test Acc: 26.08%, Test F1:  8.31%
consistency_top_sec: 48.89%,  consistency_sec_conn: 21.56%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.5333    0.3529    0.4248        68
 Contingency     0.5952    0.5455    0.5693       275
  Comparison     0.5323    0.4583    0.4925       144
   Expansion     0.7120    0.7971    0.7521       552

    accuracy                         0.6545      1039
   macro avg     0.5932    0.5385    0.5597      1039
weighted avg     0.6445    0.6545    0.6463      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4600    0.4259    0.4423        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5579    0.5911    0.5740       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4683    0.4609    0.4646       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5108    0.5900    0.5476       200
    Expansion.Instantiation     0.6404    0.6186    0.6293       118
      Expansion.Restatement     0.4136    0.4313    0.4223       211
      Expansion.Alternative     0.3333    0.3333    0.3333         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5063      1039
                  macro avg     0.3077    0.3137    0.3103      1039
               weighted avg     0.4840    0.5063    0.4943      1039

Epoch [8/30]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   5.8,  Val Acc: 63.00%, Val F1: 53.11% Time: 39.76534628868103 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 81.25%,Val Loss:   5.8,  Val Acc: 48.24%, Val F1: 28.47% Time: 39.76534628868103 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   5.8,  Val Acc: 28.33%, Val F1:  7.10% Time: 39.76534628868103 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.1,  Val Acc: 62.49%, Val F1: 50.82% Time: 118.97251415252686 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   6.1,  Val Acc: 46.18%, Val F1: 29.61% Time: 118.97251415252686 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   6.1,  Val Acc: 26.61%, Val F1:  6.78% Time: 118.97251415252686 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.0,  Val Acc: 60.94%, Val F1: 49.99% Time: 198.18014001846313 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 65.62%,Val Loss:   6.0,  Val Acc: 46.27%, Val F1: 29.02% Time: 198.18014001846313 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   6.0,  Val Acc: 26.87%, Val F1:  7.11% Time: 198.18014001846313 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 62.49%, Val F1: 52.38% Time: 277.4462311267853 
top-down:SEC: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   6.0,  Val Acc: 47.98%, Val F1: 29.50% Time: 277.4462311267853 
top-down:CONN: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 40.62%,Val Loss:   6.0,  Val Acc: 27.47%, Val F1:  6.96% Time: 277.4462311267853 
 
 
Train time usage: 312.666451215744
Test time usage: 1.7668557167053223
TOP: Test Loss:   5.8,  Test Acc: 62.85%, Test F1: 53.43%
SEC: Test Loss:   5.8,  Test Acc: 47.93%, Test F1: 29.20%
CONN: Test Loss:   5.8,  Test Acc: 25.70%, Test F1:  8.66%
consistency_top_sec: 46.58%,  consistency_sec_conn: 20.21%, consistency_top_sec_conn: 19.44%
              precision    recall  f1-score   support

    Temporal     0.5676    0.3088    0.4000        68
 Contingency     0.5969    0.4255    0.4968       275
  Comparison     0.4815    0.5417    0.5098       144
   Expansion     0.6786    0.7917    0.7308       552

    accuracy                         0.6285      1039
   macro avg     0.5811    0.5169    0.5343      1039
weighted avg     0.6224    0.6285    0.6166      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5405    0.3704    0.4396        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5650    0.4684    0.5122       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4551    0.5547    0.5000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4116    0.6050    0.4899       200
    Expansion.Instantiation     0.6333    0.6441    0.6387       118
      Expansion.Restatement     0.4316    0.3886    0.4090       211
      Expansion.Alternative     0.2222    0.2222    0.2222         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4793      1039
                  macro avg     0.2963    0.2958    0.2920      1039
               weighted avg     0.4712    0.4793    0.4689      1039

Epoch [9/30]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 60.43%, Val F1: 50.86% Time: 45.153820753097534 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.1,  Val Acc: 49.27%, Val F1: 31.33% Time: 45.153820753097534 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.1,  Val Acc: 27.73%, Val F1:  6.40% Time: 45.153820753097534 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 61.72%, Val F1: 50.03% Time: 124.199627161026 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.2,  Val Acc: 47.12%, Val F1: 29.46% Time: 124.199627161026 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   6.2,  Val Acc: 27.73%, Val F1:  7.46% Time: 124.199627161026 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 78.12%,Val Loss:   6.3,  Val Acc: 61.20%, Val F1: 51.55% Time: 203.57545852661133 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.3,  Val Acc: 46.52%, Val F1: 30.15% Time: 203.57545852661133 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   6.3,  Val Acc: 27.12%, Val F1:  7.58% Time: 203.57545852661133 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   6.3,  Val Acc: 63.69%, Val F1: 49.48% Time: 282.7880959510803 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   6.3,  Val Acc: 47.55%, Val F1: 27.88% Time: 282.7880959510803 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   6.3,  Val Acc: 26.95%, Val F1:  7.46% Time: 282.7880959510803 
 
 
Train time usage: 312.5248353481293
Test time usage: 1.7463884353637695
TOP: Test Loss:   5.9,  Test Acc: 62.66%, Test F1: 54.78%
SEC: Test Loss:   5.9,  Test Acc: 49.66%, Test F1: 32.92%
CONN: Test Loss:   5.9,  Test Acc: 24.54%, Test F1:  8.14%
consistency_top_sec: 48.12%,  consistency_sec_conn: 20.98%, consistency_top_sec_conn: 20.31%
              precision    recall  f1-score   support

    Temporal     0.4286    0.3971    0.4122        68
 Contingency     0.5573    0.5309    0.5438       275
  Comparison     0.5000    0.5278    0.5135       144
   Expansion     0.7153    0.7283    0.7217       552

    accuracy                         0.6266      1039
   macro avg     0.5503    0.5460    0.5478      1039
weighted avg     0.6249    0.6266    0.6255      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4444    0.4444    0.4444        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5376    0.5576    0.5474       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4792    0.5391    0.5074       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5048    0.5250    0.5147       200
    Expansion.Instantiation     0.6429    0.6102    0.6261       118
      Expansion.Restatement     0.4198    0.4218    0.4208       211
      Expansion.Alternative     0.4375    0.7778    0.5600         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4966      1039
                  macro avg     0.3151    0.3524    0.3292      1039
               weighted avg     0.4806    0.4966    0.4878      1039

Epoch [10/30]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.3,  Val Acc: 62.83%, Val F1: 52.02% Time: 50.64159083366394 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 81.25%,Val Loss:   6.3,  Val Acc: 47.81%, Val F1: 28.40% Time: 50.64159083366394 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 43.75%,Val Loss:   6.3,  Val Acc: 27.73%, Val F1:  6.79% Time: 50.64159083366394 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   6.5,  Val Acc: 62.66%, Val F1: 51.75% Time: 129.88339352607727 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   6.5,  Val Acc: 47.47%, Val F1: 28.81% Time: 129.88339352607727 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.5,  Val Acc: 26.78%, Val F1:  7.52% Time: 129.88339352607727 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.4,  Val Acc: 63.26%, Val F1: 49.65% Time: 209.28614830970764 
top-down:SEC: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   6.4,  Val Acc: 47.47%, Val F1: 28.83% Time: 209.28614830970764 
top-down:CONN: Iter:   3800,  Train Loss: 2.6e+01,  Train Acc: 40.62%,Val Loss:   6.4,  Val Acc: 26.78%, Val F1:  6.82% Time: 209.28614830970764 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 63.18%, Val F1: 49.06% Time: 288.43718576431274 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 46.27%, Val F1: 28.33% Time: 288.43718576431274 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 26.35%, Val F1:  6.46% Time: 288.43718576431274 
 
 
Train time usage: 312.8600649833679
Test time usage: 1.7541043758392334
TOP: Test Loss:   6.0,  Test Acc: 62.85%, Test F1: 54.86%
SEC: Test Loss:   6.0,  Test Acc: 50.24%, Test F1: 32.13%
CONN: Test Loss:   6.0,  Test Acc: 25.22%, Test F1:  8.47%
consistency_top_sec: 48.51%,  consistency_sec_conn: 21.46%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.5000    0.3382    0.4035        68
 Contingency     0.5253    0.5673    0.5455       275
  Comparison     0.5833    0.4828    0.5283       145
   Expansion     0.7014    0.7332    0.7169       551

    accuracy                         0.6285      1039
   macro avg     0.5775    0.5304    0.5486      1039
weighted avg     0.6251    0.6285    0.6247      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5122    0.3889    0.4421        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5113    0.5911    0.5483       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5167    0.4844    0.5000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4745    0.6050    0.5319       200
    Expansion.Instantiation     0.6529    0.6695    0.6611       118
      Expansion.Restatement     0.4491    0.3555    0.3968       211
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5024      1039
                  macro avg     0.3183    0.3318    0.3213      1039
               weighted avg     0.4827    0.5024    0.4885      1039

Epoch [11/30]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 62.58%, Val F1: 51.86% Time: 56.16929626464844 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   6.7,  Val Acc: 47.30%, Val F1: 28.88% Time: 56.16929626464844 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 27.98%, Val F1:  7.62% Time: 56.16929626464844 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 62.06%, Val F1: 50.03% Time: 135.8112030029297 
top-down:SEC: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   6.7,  Val Acc: 46.09%, Val F1: 27.16% Time: 135.8112030029297 
top-down:CONN: Iter:   4100,  Train Loss: 3.1e+01,  Train Acc: 34.38%,Val Loss:   6.7,  Val Acc: 27.55%, Val F1:  7.16% Time: 135.8112030029297 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 61.72%, Val F1: 50.80% Time: 215.38009977340698 
top-down:SEC: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   6.7,  Val Acc: 46.61%, Val F1: 28.51% Time: 215.38009977340698 
top-down:CONN: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 43.75%,Val Loss:   6.7,  Val Acc: 26.87%, Val F1:  7.29% Time: 215.38009977340698 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 60.94%, Val F1: 50.65% Time: 294.9482822418213 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 46.70%, Val F1: 27.50% Time: 294.9482822418213 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 40.62%,Val Loss:   6.7,  Val Acc: 27.47%, Val F1:  7.08% Time: 294.9482822418213 
 
 
Train time usage: 313.8889813423157
Test time usage: 1.76035737991333
TOP: Test Loss:   6.3,  Test Acc: 62.66%, Test F1: 54.28%
SEC: Test Loss:   6.3,  Test Acc: 48.51%, Test F1: 31.12%
CONN: Test Loss:   6.3,  Test Acc: 26.18%, Test F1:  8.72%
consistency_top_sec: 47.26%,  consistency_sec_conn: 21.56%, consistency_top_sec_conn: 21.46%
              precision    recall  f1-score   support

    Temporal     0.4464    0.3676    0.4032        68
 Contingency     0.5321    0.5127    0.5222       275
  Comparison     0.5504    0.4931    0.5201       144
   Expansion     0.7029    0.7500    0.7257       552

    accuracy                         0.6266      1039
   macro avg     0.5579    0.5309    0.5428      1039
weighted avg     0.6198    0.6266    0.6222      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4423    0.4259    0.4340        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5204    0.5204    0.5204       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5039    0.5078    0.5058       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4818    0.5300    0.5048       200
    Expansion.Instantiation     0.6417    0.6525    0.6471       118
      Expansion.Restatement     0.4009    0.4218    0.4111       211
      Expansion.Alternative     0.3636    0.4444    0.4000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4851      1039
                  macro avg     0.3050    0.3185    0.3112      1039
               weighted avg     0.4700    0.4851    0.4772      1039

Epoch [12/30]
top-down:TOP: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.8,  Val Acc: 61.46%, Val F1: 50.72% Time: 61.58050990104675 
top-down:SEC: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   6.8,  Val Acc: 46.44%, Val F1: 26.66% Time: 61.58050990104675 
top-down:CONN: Iter:   4400,  Train Loss: 3e+01,  Train Acc: 50.00%,Val Loss:   6.8,  Val Acc: 26.78%, Val F1:  7.22% Time: 61.58050990104675 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 61.72%, Val F1: 51.57% Time: 141.08508038520813 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   6.8,  Val Acc: 47.47%, Val F1: 28.54% Time: 141.08508038520813 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   6.8,  Val Acc: 27.55%, Val F1:  7.31% Time: 141.08508038520813 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 63.26%, Val F1: 49.87% Time: 220.4938576221466 
top-down:SEC: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   6.8,  Val Acc: 46.01%, Val F1: 27.15% Time: 220.4938576221466 
top-down:CONN: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 37.50%,Val Loss:   6.8,  Val Acc: 26.78%, Val F1:  7.46% Time: 220.4938576221466 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 62.32%, Val F1: 49.14% Time: 299.9900896549225 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.9,  Val Acc: 45.84%, Val F1: 27.03% Time: 299.9900896549225 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   6.9,  Val Acc: 26.87%, Val F1:  7.63% Time: 299.9900896549225 
 
 
Train time usage: 313.56026220321655
Test time usage: 1.7487454414367676
TOP: Test Loss:   6.4,  Test Acc: 61.50%, Test F1: 52.38%
SEC: Test Loss:   6.4,  Test Acc: 48.03%, Test F1: 30.72%
CONN: Test Loss:   6.4,  Test Acc: 26.85%, Test F1:  9.14%
consistency_top_sec: 46.78%,  consistency_sec_conn: 21.37%, consistency_top_sec_conn: 21.08%
              precision    recall  f1-score   support

    Temporal     0.3710    0.3382    0.3538        68
 Contingency     0.5312    0.4964    0.5132       274
  Comparison     0.5299    0.4931    0.5108       144
   Expansion     0.6968    0.7396    0.7175       553

    accuracy                         0.6150      1039
   macro avg     0.5322    0.5168    0.5238      1039
weighted avg     0.6087    0.6150    0.6112      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3729    0.4074    0.3894        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5344    0.5224    0.5283       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4812    0.5000    0.4904       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4950    0.5000    0.4975       200
    Expansion.Instantiation     0.6356    0.6356    0.6356       118
      Expansion.Restatement     0.3933    0.4434    0.4169       212
      Expansion.Alternative     0.4000    0.4444    0.4211         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4803      1039
                  macro avg     0.3011    0.3139    0.3072      1039
               weighted avg     0.4677    0.4803    0.4736      1039

Epoch [13/30]
top-down:TOP: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   6.9,  Val Acc: 62.23%, Val F1: 51.49% Time: 66.9697585105896 
top-down:SEC: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.9,  Val Acc: 46.95%, Val F1: 29.05% Time: 66.9697585105896 
top-down:CONN: Iter:   4800,  Train Loss: 2.5e+01,  Train Acc: 53.12%,Val Loss:   6.9,  Val Acc: 27.64%, Val F1:  7.77% Time: 66.9697585105896 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   7.0,  Val Acc: 62.75%, Val F1: 51.00% Time: 146.2243776321411 
top-down:SEC: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.0,  Val Acc: 46.18%, Val F1: 27.38% Time: 146.2243776321411 
top-down:CONN: Iter:   4900,  Train Loss: 2.5e+01,  Train Acc: 59.38%,Val Loss:   7.0,  Val Acc: 27.30%, Val F1:  7.43% Time: 146.2243776321411 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 61.80%, Val F1: 49.02% Time: 225.5047013759613 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 45.67%, Val F1: 28.05% Time: 225.5047013759613 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   7.1,  Val Acc: 26.61%, Val F1:  7.46% Time: 225.5047013759613 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   7.1,  Val Acc: 61.72%, Val F1: 51.13% Time: 304.82607865333557 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   7.1,  Val Acc: 46.70%, Val F1: 28.70% Time: 304.82607865333557 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 26.95%, Val F1:  7.61% Time: 304.82607865333557 
 
 
Train time usage: 312.91224098205566
Test time usage: 1.736647605895996
TOP: Test Loss:   6.7,  Test Acc: 63.62%, Test F1: 54.85%
SEC: Test Loss:   6.7,  Test Acc: 48.03%, Test F1: 32.59%
CONN: Test Loss:   6.7,  Test Acc: 25.51%, Test F1:  9.13%
consistency_top_sec: 46.87%,  consistency_sec_conn: 20.31%, consistency_top_sec_conn: 19.92%
              precision    recall  f1-score   support

    Temporal     0.4815    0.3824    0.4262        68
 Contingency     0.5916    0.4139    0.4871       273
  Comparison     0.5374    0.5486    0.5430       144
   Expansion     0.6847    0.7996    0.7377       554

    accuracy                         0.6362      1039
   macro avg     0.5738    0.5361    0.5485      1039
weighted avg     0.6265    0.6362    0.6245      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4565    0.3889    0.4200        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5781    0.4157    0.4837       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4694    0.5391    0.5018       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4453    0.5700    0.5000       200
    Expansion.Instantiation     0.6609    0.6441    0.6524       118
      Expansion.Restatement     0.4049    0.4695    0.4348       213
      Expansion.Alternative     0.4444    0.8889    0.5926         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4803      1039
                  macro avg     0.3145    0.3560    0.3259      1039
               weighted avg     0.4777    0.4803    0.4725      1039

Epoch [14/30]
top-down:TOP: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   7.1,  Val Acc: 62.49%, Val F1: 49.35% Time: 72.63704752922058 
top-down:SEC: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.1,  Val Acc: 45.75%, Val F1: 27.77% Time: 72.63704752922058 
top-down:CONN: Iter:   5200,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   7.1,  Val Acc: 28.58%, Val F1:  7.97% Time: 72.63704752922058 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 61.89%, Val F1: 50.25% Time: 152.00883269309998 
top-down:SEC: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 46.35%, Val F1: 27.38% Time: 152.00883269309998 
top-down:CONN: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   7.2,  Val Acc: 27.21%, Val F1:  7.77% Time: 152.00883269309998 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 62.06%, Val F1: 49.58% Time: 231.17924094200134 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 84.38%,Val Loss:   7.3,  Val Acc: 44.72%, Val F1: 27.13% Time: 231.17924094200134 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   7.3,  Val Acc: 25.49%, Val F1:  7.35% Time: 231.17924094200134 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 93.75%,Val Loss:   7.2,  Val Acc: 62.49%, Val F1: 49.13% Time: 310.2733051776886 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 87.50%,Val Loss:   7.2,  Val Acc: 46.27%, Val F1: 26.59% Time: 310.2733051776886 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 34.38%,Val Loss:   7.2,  Val Acc: 26.78%, Val F1:  7.59% Time: 310.2733051776886 
 
 
Train time usage: 312.9368405342102
Test time usage: 1.7460901737213135
TOP: Test Loss:   6.9,  Test Acc: 63.43%, Test F1: 53.01%
SEC: Test Loss:   6.9,  Test Acc: 46.97%, Test F1: 30.41%
CONN: Test Loss:   6.9,  Test Acc: 25.60%, Test F1:  9.20%
consistency_top_sec: 45.72%,  consistency_sec_conn: 20.69%, consistency_top_sec_conn: 20.40%
              precision    recall  f1-score   support

    Temporal     0.4211    0.3529    0.3840        68
 Contingency     0.6154    0.3810    0.4706       273
  Comparison     0.5591    0.4931    0.5240       144
   Expansion     0.6706    0.8303    0.7419       554

    accuracy                         0.6343      1039
   macro avg     0.5665    0.5143    0.5301      1039
weighted avg     0.6243    0.6343    0.6170      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4035    0.4259    0.4144        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5955    0.3985    0.4775       266
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5285    0.5078    0.5179       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4286    0.5700    0.4893       200
    Expansion.Instantiation     0.6727    0.6218    0.6463       119
      Expansion.Restatement     0.3723    0.4789    0.4189       213
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4697      1039
                  macro avg     0.3031    0.3134    0.3041      1039
               weighted avg     0.4773    0.4697    0.4650      1039

Epoch [15/30]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 62.23%, Val F1: 49.51% Time: 77.530677318573 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   7.3,  Val Acc: 45.67%, Val F1: 27.69% Time: 77.530677318573 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.3,  Val Acc: 28.15%, Val F1:  8.21% Time: 77.530677318573 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 63.18%, Val F1: 48.96% Time: 156.88441634178162 
top-down:SEC: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   7.2,  Val Acc: 46.61%, Val F1: 27.20% Time: 156.88441634178162 
top-down:CONN: Iter:   5700,  Train Loss: 2.7e+01,  Train Acc: 50.00%,Val Loss:   7.2,  Val Acc: 27.04%, Val F1:  7.57% Time: 156.88441634178162 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   7.2,  Val Acc: 62.06%, Val F1: 50.37% Time: 236.22550225257874 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.2,  Val Acc: 46.09%, Val F1: 27.75% Time: 236.22550225257874 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.2,  Val Acc: 26.95%, Val F1:  7.43% Time: 236.22550225257874 
 
 
Train time usage: 311.43987226486206
Test time usage: 1.7313802242279053
TOP: Test Loss:   6.8,  Test Acc: 62.95%, Test F1: 52.88%
SEC: Test Loss:   6.8,  Test Acc: 48.32%, Test F1: 31.53%
CONN: Test Loss:   6.8,  Test Acc: 25.22%, Test F1:  8.95%
consistency_top_sec: 46.49%,  consistency_sec_conn: 19.92%, consistency_top_sec_conn: 19.63%
              precision    recall  f1-score   support

    Temporal     0.4035    0.3382    0.3680        68
 Contingency     0.5829    0.4489    0.5072       274
  Comparison     0.5583    0.4653    0.5076       144
   Expansion     0.6774    0.7975    0.7326       553

    accuracy                         0.6295      1039
   macro avg     0.5555    0.5125    0.5288      1039
weighted avg     0.6181    0.6295    0.6181      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3636    0.2963    0.3265        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5701    0.4701    0.5153       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5321    0.4531    0.4895       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4250    0.5950    0.4958       200
    Expansion.Instantiation     0.6780    0.6780    0.6780       118
      Expansion.Restatement     0.4254    0.4575    0.4409       212
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4832      1039
                  macro avg     0.3112    0.3288    0.3153      1039
               weighted avg     0.4808    0.4832    0.4771      1039

Epoch [16/30]
top-down:TOP: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 61.20%, Val F1: 48.59% Time: 5.754907131195068 
top-down:SEC: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.3,  Val Acc: 44.98%, Val F1: 27.26% Time: 5.754907131195068 
top-down:CONN: Iter:   5900,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.3,  Val Acc: 25.24%, Val F1:  7.35% Time: 5.754907131195068 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 62.23%, Val F1: 51.70% Time: 85.67734885215759 
top-down:SEC: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   7.3,  Val Acc: 47.12%, Val F1: 31.28% Time: 85.67734885215759 
top-down:CONN: Iter:   6000,  Train Loss: 2.8e+01,  Train Acc: 37.50%,Val Loss:   7.3,  Val Acc: 26.95%, Val F1:  8.06% Time: 85.67734885215759 
 
 
top-down:TOP: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.5,  Val Acc: 61.29%, Val F1: 51.14% Time: 165.43387818336487 
top-down:SEC: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 87.50%,Val Loss:   7.5,  Val Acc: 46.35%, Val F1: 28.21% Time: 165.43387818336487 
top-down:CONN: Iter:   6100,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   7.5,  Val Acc: 26.52%, Val F1:  7.66% Time: 165.43387818336487 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.3,  Val Acc: 61.37%, Val F1: 50.77% Time: 244.90103363990784 
top-down:SEC: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.3,  Val Acc: 46.35%, Val F1: 29.69% Time: 244.90103363990784 
top-down:CONN: Iter:   6200,  Train Loss: 2.6e+01,  Train Acc: 68.75%,Val Loss:   7.3,  Val Acc: 26.01%, Val F1:  7.24% Time: 244.90103363990784 
 
 
Train time usage: 314.4668073654175
Test time usage: 1.7162365913391113
TOP: Test Loss:   7.1,  Test Acc: 63.23%, Test F1: 53.27%
SEC: Test Loss:   7.1,  Test Acc: 48.32%, Test F1: 32.17%
CONN: Test Loss:   7.1,  Test Acc: 25.12%, Test F1:  9.28%
consistency_top_sec: 46.68%,  consistency_sec_conn: 20.12%, consistency_top_sec_conn: 20.02%
              precision    recall  f1-score   support

    Temporal     0.4068    0.3529    0.3780        68
 Contingency     0.5960    0.4307    0.5000       274
  Comparison     0.5379    0.4931    0.5145       144
   Expansion     0.6831    0.8029    0.7382       553

    accuracy                         0.6323      1039
   macro avg     0.5559    0.5199    0.5327      1039
weighted avg     0.6219    0.6323    0.6208      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4182    0.4259    0.4220        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5769    0.4478    0.5042       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5333    0.5000    0.5161       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4207    0.5700    0.4841       200
    Expansion.Instantiation     0.7115    0.6271    0.6667       118
      Expansion.Restatement     0.4219    0.4717    0.4454       212
      Expansion.Alternative     0.3684    0.7778    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4832      1039
                  macro avg     0.3137    0.3473    0.3217      1039
               weighted avg     0.4873    0.4832    0.4797      1039

Epoch [17/30]
top-down:TOP: Iter:   6300,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   7.6,  Val Acc: 61.29%, Val F1: 52.12% Time: 11.160623550415039 
top-down:SEC: Iter:   6300,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   7.6,  Val Acc: 46.70%, Val F1: 29.71% Time: 11.160623550415039 
top-down:CONN: Iter:   6300,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   7.6,  Val Acc: 26.52%, Val F1:  7.37% Time: 11.160623550415039 
 
 
top-down:TOP: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   7.6,  Val Acc: 61.80%, Val F1: 49.98% Time: 90.4146237373352 
top-down:SEC: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   7.6,  Val Acc: 45.84%, Val F1: 28.29% Time: 90.4146237373352 
top-down:CONN: Iter:   6400,  Train Loss: 2.4e+01,  Train Acc: 68.75%,Val Loss:   7.6,  Val Acc: 25.67%, Val F1:  7.53% Time: 90.4146237373352 
 
 
top-down:TOP: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 61.80%, Val F1: 51.06% Time: 169.38987016677856 
top-down:SEC: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.6,  Val Acc: 45.49%, Val F1: 28.79% Time: 169.38987016677856 
top-down:CONN: Iter:   6500,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   7.6,  Val Acc: 26.44%, Val F1:  7.83% Time: 169.38987016677856 
 
 
top-down:TOP: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.6,  Val Acc: 61.12%, Val F1: 50.95% Time: 249.07394075393677 
top-down:SEC: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 44.81%, Val F1: 28.65% Time: 249.07394075393677 
top-down:CONN: Iter:   6600,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   7.6,  Val Acc: 27.04%, Val F1:  7.75% Time: 249.07394075393677 
 
 
Train time usage: 312.89720606803894
Test time usage: 1.734022617340088
TOP: Test Loss:   7.2,  Test Acc: 63.33%, Test F1: 53.36%
SEC: Test Loss:   7.2,  Test Acc: 47.64%, Test F1: 30.89%
CONN: Test Loss:   7.2,  Test Acc: 25.02%, Test F1:  8.26%
consistency_top_sec: 46.39%,  consistency_sec_conn: 20.40%, consistency_top_sec_conn: 20.02%
              precision    recall  f1-score   support

    Temporal     0.4865    0.2647    0.3429        68
 Contingency     0.5547    0.5385    0.5465       273
  Comparison     0.5068    0.5208    0.5137       144
   Expansion     0.7097    0.7545    0.7314       554

    accuracy                         0.6333      1039
   macro avg     0.5644    0.5196    0.5336      1039
weighted avg     0.6262    0.6333    0.6272      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5357    0.2778    0.3659        54
         Temporal.Synchrony     0.1250    0.0714    0.0909        14
          Contingency.Cause     0.5277    0.5356    0.5316       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4643    0.5078    0.4851       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4255    0.5000    0.4598       200
    Expansion.Instantiation     0.6696    0.6356    0.6522       118
      Expansion.Restatement     0.4319    0.4319    0.4319       213
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4764      1039
                  macro avg     0.3194    0.3095    0.3089      1039
               weighted avg     0.4717    0.4764    0.4710      1039

Epoch [18/30]
top-down:TOP: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   7.5,  Val Acc: 61.80%, Val F1: 50.71% Time: 16.51259422302246 
top-down:SEC: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   7.5,  Val Acc: 45.75%, Val F1: 29.38% Time: 16.51259422302246 
top-down:CONN: Iter:   6700,  Train Loss: 2.3e+01,  Train Acc: 62.50%,Val Loss:   7.5,  Val Acc: 25.92%, Val F1:  7.46% Time: 16.51259422302246 
 
 
top-down:TOP: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.6,  Val Acc: 61.97%, Val F1: 51.20% Time: 95.61624789237976 
top-down:SEC: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   7.6,  Val Acc: 45.84%, Val F1: 29.88% Time: 95.61624789237976 
top-down:CONN: Iter:   6800,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   7.6,  Val Acc: 25.49%, Val F1:  7.62% Time: 95.61624789237976 
 
 
top-down:TOP: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 93.75%,Val Loss:   7.6,  Val Acc: 61.29%, Val F1: 50.91% Time: 175.1153221130371 
top-down:SEC: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 90.62%,Val Loss:   7.6,  Val Acc: 46.95%, Val F1: 29.88% Time: 175.1153221130371 
top-down:CONN: Iter:   6900,  Train Loss: 2.1e+01,  Train Acc: 68.75%,Val Loss:   7.6,  Val Acc: 26.35%, Val F1:  7.76% Time: 175.1153221130371 
 
 
top-down:TOP: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 61.80%, Val F1: 48.78% Time: 254.23807096481323 
top-down:SEC: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   7.8,  Val Acc: 45.15%, Val F1: 29.56% Time: 254.23807096481323 
top-down:CONN: Iter:   7000,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   7.8,  Val Acc: 25.58%, Val F1:  7.69% Time: 254.23807096481323 
 
 
Train time usage: 312.6928651332855
Test time usage: 1.7675836086273193
TOP: Test Loss:   7.4,  Test Acc: 60.92%, Test F1: 52.47%
SEC: Test Loss:   7.4,  Test Acc: 47.16%, Test F1: 30.90%
CONN: Test Loss:   7.4,  Test Acc: 25.41%, Test F1:  9.21%
consistency_top_sec: 45.24%,  consistency_sec_conn: 20.79%, consistency_top_sec_conn: 20.21%
              precision    recall  f1-score   support

    Temporal     0.3333    0.4265    0.3742        68
 Contingency     0.5550    0.4432    0.4929       273
  Comparison     0.4968    0.5347    0.5151       144
   Expansion     0.7012    0.7329    0.7167       554

    accuracy                         0.6092      1039
   macro avg     0.5216    0.5343    0.5247      1039
weighted avg     0.6104    0.6092    0.6075      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3651    0.4259    0.3932        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5721    0.4739    0.5184       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4698    0.5469    0.5054       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4215    0.5100    0.4615       200
    Expansion.Instantiation     0.7188    0.5847    0.6449       118
      Expansion.Restatement     0.4393    0.4434    0.4413       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4716      1039
                  macro avg     0.3040    0.3219    0.3090      1039
               weighted avg     0.4799    0.4716    0.4723      1039

Epoch [19/30]
top-down:TOP: Iter:   7100,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 60.77%, Val F1: 49.90% Time: 22.123199939727783 
top-down:SEC: Iter:   7100,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.7,  Val Acc: 45.75%, Val F1: 29.13% Time: 22.123199939727783 
top-down:CONN: Iter:   7100,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   7.7,  Val Acc: 26.01%, Val F1:  7.69% Time: 22.123199939727783 
 
 
top-down:TOP: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 61.12%, Val F1: 47.72% Time: 101.7585027217865 
top-down:SEC: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 78.12%,Val Loss:   7.7,  Val Acc: 45.49%, Val F1: 28.04% Time: 101.7585027217865 
top-down:CONN: Iter:   7200,  Train Loss: 2.1e+01,  Train Acc: 59.38%,Val Loss:   7.7,  Val Acc: 25.92%, Val F1:  7.55% Time: 101.7585027217865 
 
 
top-down:TOP: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   7.7,  Val Acc: 60.94%, Val F1: 50.76% Time: 181.55767035484314 
top-down:SEC: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 45.24%, Val F1: 30.16% Time: 181.55767035484314 
top-down:CONN: Iter:   7300,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   7.7,  Val Acc: 26.44%, Val F1:  7.79% Time: 181.55767035484314 
 
 
top-down:TOP: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 62.32%, Val F1: 49.05% Time: 260.7763638496399 
top-down:SEC: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 45.67%, Val F1: 27.72% Time: 260.7763638496399 
top-down:CONN: Iter:   7400,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   7.8,  Val Acc: 25.75%, Val F1:  7.72% Time: 260.7763638496399 
 
 
Train time usage: 313.6698296070099
Test time usage: 1.7513656616210938
TOP: Test Loss:   7.4,  Test Acc: 61.79%, Test F1: 52.09%
SEC: Test Loss:   7.4,  Test Acc: 47.83%, Test F1: 30.74%
CONN: Test Loss:   7.4,  Test Acc: 25.60%, Test F1:  9.55%
consistency_top_sec: 45.91%,  consistency_sec_conn: 21.08%, consistency_top_sec_conn: 20.79%
              precision    recall  f1-score   support

    Temporal     0.3934    0.3529    0.3721        68
 Contingency     0.5578    0.4066    0.4703       273
  Comparison     0.5294    0.5000    0.5143       144
   Expansion     0.6765    0.7852    0.7268       554

    accuracy                         0.6179      1039
   macro avg     0.5393    0.5112    0.5209      1039
weighted avg     0.6064    0.6179    0.6068      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3846    0.3704    0.3774        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5646    0.4403    0.4948       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5078    0.5078    0.5078       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4328    0.5800    0.4957       200
    Expansion.Instantiation     0.7009    0.6356    0.6667       118
      Expansion.Restatement     0.4188    0.4623    0.4395       212
      Expansion.Alternative     0.3125    0.5556    0.4000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4783      1039
                  macro avg     0.3020    0.3229    0.3074      1039
               weighted avg     0.4793    0.4783    0.4741      1039

Epoch [20/30]
top-down:TOP: Iter:   7500,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   7.7,  Val Acc: 61.55%, Val F1: 50.25% Time: 27.434067010879517 
top-down:SEC: Iter:   7500,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   7.7,  Val Acc: 44.72%, Val F1: 28.95% Time: 27.434067010879517 
top-down:CONN: Iter:   7500,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   7.7,  Val Acc: 26.70%, Val F1:  8.28% Time: 27.434067010879517 
 
 
top-down:TOP: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.7,  Val Acc: 61.89%, Val F1: 50.28% Time: 107.02320075035095 
top-down:SEC: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.7,  Val Acc: 45.84%, Val F1: 29.25% Time: 107.02320075035095 
top-down:CONN: Iter:   7600,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   7.7,  Val Acc: 25.67%, Val F1:  7.61% Time: 107.02320075035095 
 
 
top-down:TOP: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 62.83%, Val F1: 49.17% Time: 186.26522779464722 
top-down:SEC: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 44.89%, Val F1: 29.11% Time: 186.26522779464722 
top-down:CONN: Iter:   7700,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   7.8,  Val Acc: 25.41%, Val F1:  7.83% Time: 186.26522779464722 
 
 
top-down:TOP: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 62.15%, Val F1: 49.48% Time: 265.1763415336609 
top-down:SEC: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 45.84%, Val F1: 29.30% Time: 265.1763415336609 
top-down:CONN: Iter:   7800,  Train Loss: 2.4e+01,  Train Acc: 56.25%,Val Loss:   7.8,  Val Acc: 25.75%, Val F1:  7.81% Time: 265.1763415336609 
 
 
Train time usage: 313.4364695549011
Test time usage: 1.8940908908843994
TOP: Test Loss:   7.3,  Test Acc: 62.66%, Test F1: 53.11%
SEC: Test Loss:   7.3,  Test Acc: 47.16%, Test F1: 30.80%
CONN: Test Loss:   7.3,  Test Acc: 25.31%, Test F1:  9.10%
consistency_top_sec: 45.43%,  consistency_sec_conn: 20.02%, consistency_top_sec_conn: 19.44%
              precision    recall  f1-score   support

    Temporal     0.4068    0.3529    0.3780        68
 Contingency     0.5735    0.4270    0.4895       274
  Comparison     0.5362    0.5139    0.5248       144
   Expansion     0.6834    0.7884    0.7322       553

    accuracy                         0.6266      1039
   macro avg     0.5500    0.5206    0.5311      1039
weighted avg     0.6159    0.6266    0.6163      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4167    0.3704    0.3922        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5622    0.4552    0.5031       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5081    0.4922    0.5000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4359    0.5100    0.4700       200
    Expansion.Instantiation     0.6900    0.5847    0.6330       118
      Expansion.Restatement     0.4082    0.5142    0.4551       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4716      1039
                  macro avg     0.3071    0.3166    0.3080      1039
               weighted avg     0.4779    0.4716    0.4707      1039

Epoch [21/30]
top-down:TOP: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 62.40%, Val F1: 50.75% Time: 40.487998485565186 
top-down:SEC: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 45.24%, Val F1: 28.64% Time: 40.487998485565186 
top-down:CONN: Iter:   7900,  Train Loss: 2.3e+01,  Train Acc: 71.88%,Val Loss:   7.8,  Val Acc: 26.44%, Val F1:  8.36% Time: 40.487998485565186 
 
 
top-down:TOP: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   7.8,  Val Acc: 61.80%, Val F1: 50.40% Time: 135.3120367527008 
top-down:SEC: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   7.8,  Val Acc: 46.09%, Val F1: 29.50% Time: 135.3120367527008 
top-down:CONN: Iter:   8000,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   7.8,  Val Acc: 26.70%, Val F1:  8.33% Time: 135.3120367527008 
 
 
top-down:TOP: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 62.49%, Val F1: 51.60% Time: 215.00905871391296 
top-down:SEC: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 47.21%, Val F1: 29.61% Time: 215.00905871391296 
top-down:CONN: Iter:   8100,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   7.8,  Val Acc: 26.78%, Val F1:  8.15% Time: 215.00905871391296 
 
 
top-down:TOP: Iter:   8200,  Train Loss: 3.2e+01,  Train Acc: 93.75%,Val Loss:   7.8,  Val Acc: 61.72%, Val F1: 50.68% Time: 295.51992201805115 
top-down:SEC: Iter:   8200,  Train Loss: 3.2e+01,  Train Acc: 81.25%,Val Loss:   7.8,  Val Acc: 45.49%, Val F1: 28.70% Time: 295.51992201805115 
top-down:CONN: Iter:   8200,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   7.8,  Val Acc: 26.70%, Val F1:  8.34% Time: 295.51992201805115 
 
 
Train time usage: 337.96813583374023
Test time usage: 1.7158915996551514
TOP: Test Loss:   7.3,  Test Acc: 63.52%, Test F1: 54.47%
SEC: Test Loss:   7.3,  Test Acc: 48.41%, Test F1: 31.67%
CONN: Test Loss:   7.3,  Test Acc: 26.08%, Test F1:  9.41%
consistency_top_sec: 46.78%,  consistency_sec_conn: 21.66%, consistency_top_sec_conn: 21.17%
              precision    recall  f1-score   support

    Temporal     0.4237    0.3676    0.3937        68
 Contingency     0.5789    0.4818    0.5259       274
  Comparison     0.5245    0.5208    0.5226       144
   Expansion     0.7028    0.7740    0.7367       553

    accuracy                         0.6352      1039
   macro avg     0.5575    0.5360    0.5447      1039
weighted avg     0.6272    0.6352    0.6290      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4565    0.3889    0.4200        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5708    0.5112    0.5394       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4889    0.5156    0.5019       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4350    0.4850    0.4586       200
    Expansion.Instantiation     0.6818    0.6356    0.6579       118
      Expansion.Restatement     0.4250    0.4811    0.4513       212
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4841      1039
                  macro avg     0.3130    0.3248    0.3167      1039
               weighted avg     0.4824    0.4841    0.4818      1039

Epoch [22/30]
top-down:TOP: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.8,  Val Acc: 61.29%, Val F1: 48.80% Time: 38.13293719291687 
top-down:SEC: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.8,  Val Acc: 45.58%, Val F1: 27.84% Time: 38.13293719291687 
top-down:CONN: Iter:   8300,  Train Loss: 2.5e+01,  Train Acc: 53.12%,Val Loss:   7.8,  Val Acc: 25.92%, Val F1:  7.78% Time: 38.13293719291687 
 
 
top-down:TOP: Iter:   8400,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 61.63%, Val F1: 49.92% Time: 122.9665584564209 
top-down:SEC: Iter:   8400,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   7.9,  Val Acc: 45.75%, Val F1: 29.20% Time: 122.9665584564209 
top-down:CONN: Iter:   8400,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   7.9,  Val Acc: 26.35%, Val F1:  8.01% Time: 122.9665584564209 
 
 
top-down:TOP: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 62.23%, Val F1: 49.90% Time: 219.15627574920654 
top-down:SEC: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   7.9,  Val Acc: 45.67%, Val F1: 28.90% Time: 219.15627574920654 
top-down:CONN: Iter:   8500,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   7.9,  Val Acc: 26.35%, Val F1:  7.92% Time: 219.15627574920654 
 
 
top-down:TOP: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 61.29%, Val F1: 50.38% Time: 315.4680771827698 
top-down:SEC: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   7.9,  Val Acc: 46.52%, Val F1: 29.78% Time: 315.4680771827698 
top-down:CONN: Iter:   8600,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   7.9,  Val Acc: 26.95%, Val F1:  8.24% Time: 315.4680771827698 
 
 
Train time usage: 359.6085102558136
Test time usage: 2.5991017818450928
TOP: Test Loss:   7.6,  Test Acc: 62.85%, Test F1: 53.04%
SEC: Test Loss:   7.6,  Test Acc: 46.58%, Test F1: 30.40%
CONN: Test Loss:   7.6,  Test Acc: 24.93%, Test F1:  9.03%
consistency_top_sec: 45.14%,  consistency_sec_conn: 19.44%, consistency_top_sec_conn: 19.25%
              precision    recall  f1-score   support

    Temporal     0.4035    0.3382    0.3680        68
 Contingency     0.5864    0.4103    0.4828       273
  Comparison     0.5232    0.5486    0.5356       144
   Expansion     0.6859    0.7924    0.7353       554

    accuracy                         0.6285      1039
   macro avg     0.5498    0.5224    0.5304      1039
weighted avg     0.6187    0.6285    0.6173      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4091    0.3333    0.3673        54
         Temporal.Synchrony     0.0588    0.0714    0.0645        14
          Contingency.Cause     0.5692    0.4157    0.4805       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4786    0.5234    0.5000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4200    0.5250    0.4667       200
    Expansion.Instantiation     0.7228    0.6186    0.6667       118
      Expansion.Restatement     0.4150    0.4930    0.4506       213
      Expansion.Alternative     0.2857    0.4444    0.3478         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4658      1039
                  macro avg     0.3054    0.3114    0.3040      1039
               weighted avg     0.4778    0.4658    0.4660      1039

Epoch [23/30]
top-down:TOP: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 62.49%, Val F1: 50.52% Time: 53.8712522983551 
top-down:SEC: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   7.9,  Val Acc: 45.58%, Val F1: 28.53% Time: 53.8712522983551 
top-down:CONN: Iter:   8700,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   7.9,  Val Acc: 27.47%, Val F1:  8.57% Time: 53.8712522983551 
 
 
top-down:TOP: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   7.9,  Val Acc: 61.97%, Val F1: 49.15% Time: 150.45078897476196 
top-down:SEC: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   7.9,  Val Acc: 46.52%, Val F1: 29.25% Time: 150.45078897476196 
top-down:CONN: Iter:   8800,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   7.9,  Val Acc: 27.47%, Val F1:  8.49% Time: 150.45078897476196 
 
 
top-down:TOP: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.58%, Val F1: 52.48% Time: 246.9159333705902 
top-down:SEC: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 46.09%, Val F1: 29.25% Time: 246.9159333705902 
top-down:CONN: Iter:   8900,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   8.0,  Val Acc: 27.21%, Val F1:  8.83% Time: 246.9159333705902 
 
 
top-down:TOP: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 62.49%, Val F1: 50.96% Time: 343.5888240337372 
top-down:SEC: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.0,  Val Acc: 45.84%, Val F1: 28.72% Time: 343.5888240337372 
top-down:CONN: Iter:   9000,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   8.0,  Val Acc: 25.84%, Val F1:  8.01% Time: 343.5888240337372 
 
 
Train time usage: 381.4493315219879
Test time usage: 2.573331356048584
TOP: Test Loss:   7.7,  Test Acc: 62.66%, Test F1: 52.58%
SEC: Test Loss:   7.7,  Test Acc: 46.39%, Test F1: 30.77%
CONN: Test Loss:   7.7,  Test Acc: 24.74%, Test F1:  9.26%
consistency_top_sec: 44.95%,  consistency_sec_conn: 19.25%, consistency_top_sec_conn: 19.06%
              precision    recall  f1-score   support

    Temporal     0.3966    0.3382    0.3651        68
 Contingency     0.5721    0.4359    0.4948       273
  Comparison     0.5344    0.4861    0.5091       144
   Expansion     0.6838    0.7924    0.7341       554

    accuracy                         0.6266      1039
   macro avg     0.5467    0.5132    0.5258      1039
weighted avg     0.6149    0.6266    0.6159      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4390    0.3333    0.3789        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5530    0.4478    0.4948       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4841    0.4766    0.4803       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4268    0.5250    0.4709       200
    Expansion.Instantiation     0.6549    0.6271    0.6407       118
      Expansion.Restatement     0.4033    0.4623    0.4308       212
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.0625    0.0833    0.0714        12

                   accuracy                         0.4639      1039
                  macro avg     0.3052    0.3192    0.3077      1039
               weighted avg     0.4675    0.4639    0.4622      1039

Epoch [24/30]
top-down:TOP: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 62.58%, Val F1: 50.14% Time: 59.97486996650696 
top-down:SEC: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 45.32%, Val F1: 28.48% Time: 59.97486996650696 
top-down:CONN: Iter:   9100,  Train Loss: 3.6e+01,  Train Acc: 71.88%,Val Loss:   8.1,  Val Acc: 26.35%, Val F1:  8.18% Time: 59.97486996650696 
 
 
top-down:TOP: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 62.32%, Val F1: 49.86% Time: 156.60520219802856 
top-down:SEC: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 87.50%,Val Loss:   8.1,  Val Acc: 45.67%, Val F1: 28.82% Time: 156.60520219802856 
top-down:CONN: Iter:   9200,  Train Loss: 2.2e+01,  Train Acc: 59.38%,Val Loss:   8.1,  Val Acc: 26.01%, Val F1:  7.88% Time: 156.60520219802856 
 
 
top-down:TOP: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 62.23%, Val F1: 49.94% Time: 237.1094036102295 
top-down:SEC: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   8.1,  Val Acc: 45.24%, Val F1: 28.61% Time: 237.1094036102295 
top-down:CONN: Iter:   9300,  Train Loss: 2.5e+01,  Train Acc: 59.38%,Val Loss:   8.1,  Val Acc: 25.75%, Val F1:  8.04% Time: 237.1094036102295 
 
 
top-down:TOP: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 62.23%, Val F1: 50.18% Time: 316.94032168388367 
top-down:SEC: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 44.55%, Val F1: 28.81% Time: 316.94032168388367 
top-down:CONN: Iter:   9400,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   8.1,  Val Acc: 26.52%, Val F1:  8.28% Time: 316.94032168388367 
 
 
Train time usage: 342.9247453212738
Test time usage: 1.7464854717254639
TOP: Test Loss:   7.6,  Test Acc: 61.50%, Test F1: 53.74%
SEC: Test Loss:   7.6,  Test Acc: 48.03%, Test F1: 30.54%
CONN: Test Loss:   7.6,  Test Acc: 25.70%, Test F1:  8.94%
consistency_top_sec: 45.62%,  consistency_sec_conn: 20.50%, consistency_top_sec_conn: 19.92%
              precision    recall  f1-score   support

    Temporal     0.3733    0.4118    0.3916        68
 Contingency     0.5529    0.5127    0.5321       275
  Comparison     0.5172    0.5172    0.5172       145
   Expansion     0.7004    0.7169    0.7085       551

    accuracy                         0.6150      1039
   macro avg     0.5360    0.5397    0.5374      1039
weighted avg     0.6144    0.6150    0.6144      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3774    0.3704    0.3738        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5625    0.5373    0.5496       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4889    0.5156    0.5019       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4597    0.4850    0.4720       200
    Expansion.Instantiation     0.6792    0.6102    0.6429       118
      Expansion.Restatement     0.4248    0.4528    0.4384       212
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4803      1039
                  macro avg     0.3023    0.3105    0.3054      1039
               weighted avg     0.4801    0.4803    0.4796      1039

Epoch [25/30]
top-down:TOP: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 62.66%, Val F1: 49.85% Time: 54.55813503265381 
top-down:SEC: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 46.52%, Val F1: 29.34% Time: 54.55813503265381 
top-down:CONN: Iter:   9500,  Train Loss: 2.8e+01,  Train Acc: 56.25%,Val Loss:   8.1,  Val Acc: 26.01%, Val F1:  7.99% Time: 54.55813503265381 
 
 
top-down:TOP: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 62.06%, Val F1: 50.96% Time: 134.1075267791748 
top-down:SEC: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   8.1,  Val Acc: 46.09%, Val F1: 29.02% Time: 134.1075267791748 
top-down:CONN: Iter:   9600,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   8.1,  Val Acc: 25.75%, Val F1:  7.83% Time: 134.1075267791748 
 
 
top-down:TOP: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 62.92%, Val F1: 51.98% Time: 213.8678638935089 
top-down:SEC: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 45.58%, Val F1: 29.11% Time: 213.8678638935089 
top-down:CONN: Iter:   9700,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   8.1,  Val Acc: 26.87%, Val F1:  8.37% Time: 213.8678638935089 
 
 
top-down:TOP: Iter:   9800,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 62.66%, Val F1: 50.65% Time: 293.21226930618286 
top-down:SEC: Iter:   9800,  Train Loss: 2.3e+01,  Train Acc: 90.62%,Val Loss:   8.1,  Val Acc: 46.01%, Val F1: 29.12% Time: 293.21226930618286 
top-down:CONN: Iter:   9800,  Train Loss: 2.3e+01,  Train Acc: 75.00%,Val Loss:   8.1,  Val Acc: 27.04%, Val F1:  8.50% Time: 293.21226930618286 
 
 
Train time usage: 313.9036741256714
Test time usage: 1.7610135078430176
TOP: Test Loss:   7.6,  Test Acc: 62.46%, Test F1: 54.11%
SEC: Test Loss:   7.6,  Test Acc: 48.99%, Test F1: 32.93%
CONN: Test Loss:   7.6,  Test Acc: 24.83%, Test F1:  9.11%
consistency_top_sec: 47.06%,  consistency_sec_conn: 19.83%, consistency_top_sec_conn: 19.44%
              precision    recall  f1-score   support

    Temporal     0.4154    0.3971    0.4060        68
 Contingency     0.5583    0.4891    0.5214       274
  Comparison     0.5103    0.5139    0.5121       144
   Expansion     0.7029    0.7486    0.7250       553

    accuracy                         0.6246      1039
   macro avg     0.5467    0.5372    0.5411      1039
weighted avg     0.6193    0.6246    0.6209      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4348    0.3704    0.4000        54
         Temporal.Synchrony     0.0556    0.0714    0.0625        14
          Contingency.Cause     0.5587    0.5149    0.5359       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4857    0.5312    0.5075       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4581    0.5200    0.4871       200
    Expansion.Instantiation     0.6916    0.6271    0.6578       118
      Expansion.Restatement     0.4375    0.4623    0.4495       212
      Expansion.Alternative     0.4286    0.6667    0.5217         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4899      1039
                  macro avg     0.3228    0.3422    0.3293      1039
               weighted avg     0.4870    0.4899    0.4871      1039

Epoch [26/30]
top-down:TOP: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.0,  Val Acc: 62.15%, Val F1: 49.90% Time: 65.34712052345276 
top-down:SEC: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   8.0,  Val Acc: 45.92%, Val F1: 28.52% Time: 65.34712052345276 
top-down:CONN: Iter:   9900,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   8.0,  Val Acc: 26.18%, Val F1:  8.17% Time: 65.34712052345276 
 
 
top-down:TOP: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 61.89%, Val F1: 49.83% Time: 166.1389617919922 
top-down:SEC: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   8.1,  Val Acc: 45.32%, Val F1: 28.48% Time: 166.1389617919922 
top-down:CONN: Iter:  10000,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   8.1,  Val Acc: 26.09%, Val F1:  8.56% Time: 166.1389617919922 
 
 
top-down:TOP: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 61.97%, Val F1: 49.97% Time: 267.84092712402344 
top-down:SEC: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 45.06%, Val F1: 28.44% Time: 267.84092712402344 
top-down:CONN: Iter:  10100,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   8.1,  Val Acc: 26.52%, Val F1:  8.13% Time: 267.84092712402344 
 
 
top-down:TOP: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 62.15%, Val F1: 48.98% Time: 368.194548368454 
top-down:SEC: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 44.89%, Val F1: 28.54% Time: 368.194548368454 
top-down:CONN: Iter:  10200,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   8.1,  Val Acc: 25.92%, Val F1:  8.15% Time: 368.194548368454 
 
 
Train time usage: 387.0709037780762
Test time usage: 2.6641438007354736
TOP: Test Loss:   7.6,  Test Acc: 63.62%, Test F1: 55.11%
SEC: Test Loss:   7.6,  Test Acc: 48.70%, Test F1: 31.92%
CONN: Test Loss:   7.6,  Test Acc: 25.41%, Test F1:  9.16%
consistency_top_sec: 47.06%,  consistency_sec_conn: 20.50%, consistency_top_sec_conn: 20.12%
              precision    recall  f1-score   support

    Temporal     0.4407    0.3824    0.4094        68
 Contingency     0.5703    0.5328    0.5509       274
  Comparison     0.5255    0.5000    0.5125       144
   Expansion     0.7104    0.7541    0.7316       553

    accuracy                         0.6362      1039
   macro avg     0.5617    0.5423    0.5511      1039
weighted avg     0.6302    0.6362    0.6325      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4390    0.3333    0.3789        54
         Temporal.Synchrony     0.0625    0.0714    0.0667        14
          Contingency.Cause     0.5635    0.5299    0.5462       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4724    0.4688    0.4706       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4573    0.5350    0.4931       200
    Expansion.Instantiation     0.6923    0.6102    0.6486       118
      Expansion.Restatement     0.4316    0.4764    0.4529       212
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4870      1039
                  macro avg     0.3185    0.3255    0.3192      1039
               weighted avg     0.4853    0.4870    0.4844      1039

Epoch [27/30]
top-down:TOP: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 61.55%, Val F1: 49.30% Time: 84.23938059806824 
top-down:SEC: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 45.06%, Val F1: 27.87% Time: 84.23938059806824 
top-down:CONN: Iter:  10300,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   8.1,  Val Acc: 25.92%, Val F1:  8.16% Time: 84.23938059806824 
 
 
top-down:TOP: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 61.97%, Val F1: 50.53% Time: 184.9510350227356 
top-down:SEC: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 44.72%, Val F1: 27.62% Time: 184.9510350227356 
top-down:CONN: Iter:  10400,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   8.1,  Val Acc: 26.44%, Val F1:  8.40% Time: 184.9510350227356 
 
 
top-down:TOP: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 61.97%, Val F1: 50.65% Time: 284.2710700035095 
top-down:SEC: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 87.50%,Val Loss:   8.1,  Val Acc: 45.06%, Val F1: 28.32% Time: 284.2710700035095 
top-down:CONN: Iter:  10500,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   8.1,  Val Acc: 26.61%, Val F1:  7.94% Time: 284.2710700035095 
 
 
top-down:TOP: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 61.72%, Val F1: 50.59% Time: 384.05347967147827 
top-down:SEC: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   8.1,  Val Acc: 45.67%, Val F1: 28.80% Time: 384.05347967147827 
top-down:CONN: Iter:  10600,  Train Loss: 3.3e+01,  Train Acc: 68.75%,Val Loss:   8.1,  Val Acc: 25.92%, Val F1:  8.58% Time: 384.05347967147827 
 
 
Train time usage: 395.44674134254456
Test time usage: 2.5114049911499023
TOP: Test Loss:   7.7,  Test Acc: 64.20%, Test F1: 54.38%
SEC: Test Loss:   7.7,  Test Acc: 48.60%, Test F1: 32.03%
CONN: Test Loss:   7.7,  Test Acc: 25.02%, Test F1:  9.60%
consistency_top_sec: 46.97%,  consistency_sec_conn: 19.83%, consistency_top_sec_conn: 19.44%
              precision    recall  f1-score   support

    Temporal     0.4259    0.3382    0.3770        68
 Contingency     0.5852    0.4891    0.5328       274
  Comparison     0.5328    0.5069    0.5196       144
   Expansion     0.7060    0.7902    0.7457       553

    accuracy                         0.6420      1039
   macro avg     0.5625    0.5311    0.5438      1039
weighted avg     0.6318    0.6420    0.6341      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4500    0.3333    0.3830        54
         Temporal.Synchrony     0.0667    0.0714    0.0690        14
          Contingency.Cause     0.5877    0.5000    0.5403       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5000    0.5078    0.5039       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4269    0.5400    0.4768       200
    Expansion.Instantiation     0.7115    0.6271    0.6667       118
      Expansion.Restatement     0.4274    0.4717    0.4484       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4860      1039
                  macro avg     0.3207    0.3279    0.3203      1039
               weighted avg     0.4908    0.4860    0.4850      1039

Epoch [28/30]
top-down:TOP: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 62.40%, Val F1: 50.67% Time: 89.0433886051178 
top-down:SEC: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   8.1,  Val Acc: 45.49%, Val F1: 28.38% Time: 89.0433886051178 
top-down:CONN: Iter:  10700,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   8.1,  Val Acc: 26.27%, Val F1:  8.17% Time: 89.0433886051178 
 
 
top-down:TOP: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 61.72%, Val F1: 50.72% Time: 187.7631003856659 
top-down:SEC: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 45.41%, Val F1: 29.37% Time: 187.7631003856659 
top-down:CONN: Iter:  10800,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   8.1,  Val Acc: 26.87%, Val F1:  8.04% Time: 187.7631003856659 
 
 
top-down:TOP: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 62.06%, Val F1: 50.80% Time: 287.2543933391571 
top-down:SEC: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   8.1,  Val Acc: 45.58%, Val F1: 28.77% Time: 287.2543933391571 
top-down:CONN: Iter:  10900,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   8.1,  Val Acc: 26.61%, Val F1:  8.55% Time: 287.2543933391571 
 
 
top-down:TOP: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 96.88%,Val Loss:   8.1,  Val Acc: 62.15%, Val F1: 50.64% Time: 388.24992060661316 
top-down:SEC: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   8.1,  Val Acc: 45.06%, Val F1: 29.32% Time: 388.24992060661316 
top-down:CONN: Iter:  11000,  Train Loss: 2.2e+01,  Train Acc: 78.12%,Val Loss:   8.1,  Val Acc: 26.61%, Val F1:  8.50% Time: 388.24992060661316 
 
 
Train time usage: 393.0619411468506
Test time usage: 2.632990598678589
TOP: Test Loss:   7.7,  Test Acc: 63.81%, Test F1: 54.49%
SEC: Test Loss:   7.7,  Test Acc: 48.12%, Test F1: 31.76%
CONN: Test Loss:   7.7,  Test Acc: 24.64%, Test F1:  9.09%
consistency_top_sec: 46.58%,  consistency_sec_conn: 19.44%, consistency_top_sec_conn: 19.06%
              precision    recall  f1-score   support

    Temporal     0.4167    0.3676    0.3906        68
 Contingency     0.5844    0.4927    0.5347       274
  Comparison     0.5294    0.5000    0.5143       144
   Expansion     0.7042    0.7794    0.7399       553

    accuracy                         0.6381      1039
   macro avg     0.5587    0.5349    0.5449      1039
weighted avg     0.6296    0.6381    0.6317      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4222    0.3519    0.3838        54
         Temporal.Synchrony     0.0526    0.0714    0.0606        14
          Contingency.Cause     0.5826    0.5000    0.5382       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4812    0.5000    0.4904       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4444    0.5000    0.4706       200
    Expansion.Instantiation     0.7009    0.6356    0.6667       118
      Expansion.Restatement     0.4198    0.4811    0.4484       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4812      1039
                  macro avg     0.3146    0.3269    0.3176      1039
               weighted avg     0.4861    0.4812    0.4815      1039

Epoch [29/30]
top-down:TOP: Iter:  11100,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 62.66%, Val F1: 51.19% Time: 96.14614415168762 
top-down:SEC: Iter:  11100,  Train Loss: 3.5e+01,  Train Acc: 84.38%,Val Loss:   8.1,  Val Acc: 44.98%, Val F1: 29.07% Time: 96.14614415168762 
top-down:CONN: Iter:  11100,  Train Loss: 3.5e+01,  Train Acc: 75.00%,Val Loss:   8.1,  Val Acc: 26.52%, Val F1:  8.55% Time: 96.14614415168762 
 
 
top-down:TOP: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 61.97%, Val F1: 50.97% Time: 196.42850875854492 
top-down:SEC: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   8.1,  Val Acc: 44.98%, Val F1: 29.27% Time: 196.42850875854492 
top-down:CONN: Iter:  11200,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   8.1,  Val Acc: 26.52%, Val F1:  8.42% Time: 196.42850875854492 
 
 
top-down:TOP: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 62.23%, Val F1: 50.04% Time: 296.02506470680237 
top-down:SEC: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 44.72%, Val F1: 27.80% Time: 296.02506470680237 
top-down:CONN: Iter:  11300,  Train Loss: 2.9e+01,  Train Acc: 71.88%,Val Loss:   8.2,  Val Acc: 25.84%, Val F1:  7.96% Time: 296.02506470680237 
 
 
Train time usage: 390.6316192150116
Test time usage: 2.6317379474639893
TOP: Test Loss:   7.7,  Test Acc: 64.29%, Test F1: 54.91%
SEC: Test Loss:   7.7,  Test Acc: 47.83%, Test F1: 31.67%
CONN: Test Loss:   7.7,  Test Acc: 25.02%, Test F1:  9.09%
consistency_top_sec: 46.58%,  consistency_sec_conn: 19.73%, consistency_top_sec_conn: 19.25%
              precision    recall  f1-score   support

    Temporal     0.4167    0.3676    0.3906        68
 Contingency     0.5840    0.5073    0.5430       274
  Comparison     0.5373    0.5000    0.5180       144
   Expansion     0.7117    0.7812    0.7448       553

    accuracy                         0.6429      1039
   macro avg     0.5624    0.5390    0.5491      1039
weighted avg     0.6346    0.6429    0.6370      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4286    0.3333    0.3750        54
         Temporal.Synchrony     0.0526    0.0714    0.0606        14
          Contingency.Cause     0.5819    0.5037    0.5400       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4701    0.4922    0.4809       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4454    0.5100    0.4755       200
    Expansion.Instantiation     0.7129    0.6102    0.6575       118
      Expansion.Restatement     0.4089    0.4764    0.4401       212
      Expansion.Alternative     0.3846    0.5556    0.4545         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4783      1039
                  macro avg     0.3168    0.3230    0.3167      1039
               weighted avg     0.4845    0.4783    0.4788      1039

Epoch [30/30]
top-down:TOP: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 61.89%, Val F1: 50.27% Time: 5.865394353866577 
top-down:SEC: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   8.2,  Val Acc: 44.89%, Val F1: 28.75% Time: 5.865394353866577 
top-down:CONN: Iter:  11400,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   8.2,  Val Acc: 26.52%, Val F1:  8.43% Time: 5.865394353866577 
 
 
top-down:TOP: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   8.2,  Val Acc: 62.06%, Val F1: 50.30% Time: 105.64677119255066 
top-down:SEC: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 44.64%, Val F1: 27.93% Time: 105.64677119255066 
top-down:CONN: Iter:  11500,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   8.2,  Val Acc: 26.18%, Val F1:  7.96% Time: 105.64677119255066 
 
 
top-down:TOP: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 62.23%, Val F1: 50.45% Time: 205.73433828353882 
top-down:SEC: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   8.2,  Val Acc: 45.24%, Val F1: 28.77% Time: 205.73433828353882 
top-down:CONN: Iter:  11600,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   8.2,  Val Acc: 26.44%, Val F1:  8.03% Time: 205.73433828353882 
 
 
top-down:TOP: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 62.06%, Val F1: 50.23% Time: 305.94318199157715 
top-down:SEC: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 96.88%,Val Loss:   8.2,  Val Acc: 45.06%, Val F1: 28.07% Time: 305.94318199157715 
top-down:CONN: Iter:  11700,  Train Loss: 2.5e+01,  Train Acc: 62.50%,Val Loss:   8.2,  Val Acc: 26.52%, Val F1:  8.31% Time: 305.94318199157715 
 
 
Train time usage: 395.3602349758148
Test time usage: 2.5339763164520264
TOP: Test Loss:   7.7,  Test Acc: 64.39%, Test F1: 55.03%
SEC: Test Loss:   7.7,  Test Acc: 47.83%, Test F1: 31.61%
CONN: Test Loss:   7.7,  Test Acc: 24.74%, Test F1:  8.97%
consistency_top_sec: 46.68%,  consistency_sec_conn: 19.44%, consistency_top_sec_conn: 19.06%
              precision    recall  f1-score   support

    Temporal     0.4262    0.3824    0.4031        68
 Contingency     0.5895    0.4927    0.5368       274
  Comparison     0.5379    0.4931    0.5145       144
   Expansion     0.7083    0.7902    0.7470       553

    accuracy                         0.6439      1039
   macro avg     0.5655    0.5396    0.5503      1039
weighted avg     0.6349    0.6439    0.6368      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4186    0.3333    0.3711        54
         Temporal.Synchrony     0.0556    0.0714    0.0625        14
          Contingency.Cause     0.5848    0.4888    0.5325       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4923    0.5000    0.4961       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4397    0.5100    0.4722       200
    Expansion.Instantiation     0.7115    0.6271    0.6667       118
      Expansion.Restatement     0.4080    0.4811    0.4416       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4783      1039
                  macro avg     0.3152    0.3243    0.3161      1039
               weighted avg     0.4858    0.4783    0.4791      1039

dev_best_acc_top: 64.12%,  dev_best_f1_top: 52.33%, 
dev_best_acc_sec: 48.76%,  dev_best_f1_sec: 30.64%, 
dev_best_acc_conn: 28.76%,  dev_best_f1_conn:  6.39%
