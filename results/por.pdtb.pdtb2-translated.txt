nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'pdtb_pt/data/', 'log_file': 'pdtb_pt/log/', 'save_file': 'pdtb_pt/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'February27-17:01:37', 'log': 'pdtb_pt/log/February27-17:01:37.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]108it [00:00, 1079.52it/s]299it [00:00, 1565.22it/s]473it [00:00, 1642.32it/s]672it [00:00, 1777.69it/s]869it [00:00, 1846.71it/s]1065it [00:00, 1883.14it/s]1261it [00:00, 1905.82it/s]1454it [00:00, 1913.14it/s]1646it [00:00, 1848.92it/s]1832it [00:01, 1849.43it/s]2032it [00:01, 1892.66it/s]2222it [00:01, 1862.78it/s]2409it [00:01, 1846.19it/s]2596it [00:01, 1852.00it/s]2788it [00:01, 1869.88it/s]2976it [00:01, 1854.33it/s]3164it [00:01, 1859.07it/s]3350it [00:01, 1847.38it/s]3535it [00:01, 1841.52it/s]3720it [00:02, 1840.55it/s]3905it [00:02, 1815.89it/s]4100it [00:02, 1854.71it/s]4286it [00:02, 1790.62it/s]4471it [00:02, 1805.16it/s]4652it [00:02, 1802.65it/s]4845it [00:02, 1837.32it/s]5031it [00:02, 1841.51it/s]5216it [00:02, 1307.19it/s]5377it [00:03, 1376.98it/s]5576it [00:03, 1528.66it/s]5764it [00:03, 1619.49it/s]5971it [00:03, 1737.98it/s]6158it [00:03, 1774.63it/s]6350it [00:03, 1814.56it/s]6537it [00:03, 1819.18it/s]6723it [00:03, 1812.86it/s]6907it [00:03, 1811.48it/s]7093it [00:03, 1825.09it/s]7284it [00:04, 1847.61it/s]7470it [00:04, 1840.27it/s]7655it [00:04, 1842.98it/s]7852it [00:04, 1880.59it/s]8041it [00:04, 1844.08it/s]8231it [00:04, 1858.90it/s]8418it [00:04, 1851.21it/s]8604it [00:04, 1847.63it/s]8801it [00:04, 1881.61it/s]8990it [00:05, 1861.41it/s]9185it [00:05, 1886.70it/s]9374it [00:05, 1788.97it/s]9562it [00:05, 1812.91it/s]9764it [00:05, 1871.81it/s]9952it [00:05, 1872.44it/s]10140it [00:05, 1852.71it/s]10331it [00:05, 1868.03it/s]10531it [00:05, 1906.05it/s]10722it [00:05, 1858.03it/s]10916it [00:06, 1881.11it/s]11108it [00:06, 1890.75it/s]11299it [00:06, 1894.93it/s]11496it [00:06, 1914.83it/s]11688it [00:06, 1887.41it/s]11878it [00:06, 1890.98it/s]12068it [00:06, 1830.15it/s]12252it [00:06, 1764.17it/s]12430it [00:06, 1761.93it/s]12547it [00:06, 1806.02it/s]
0it [00:00, ?it/s]177it [00:00, 1767.76it/s]367it [00:00, 1843.94it/s]553it [00:00, 1850.65it/s]739it [00:00, 1769.58it/s]917it [00:00, 1759.22it/s]1094it [00:00, 1755.03it/s]1165it [00:00, 1781.33it/s]
0it [00:00, ?it/s]177it [00:00, 1765.77it/s]377it [00:00, 1902.91it/s]571it [00:00, 1918.05it/s]763it [00:00, 1855.92it/s]954it [00:00, 1872.78it/s]1039it [00:00, 1883.53it/s]
Time usage: 17.68810200691223
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.8,  Val Acc: 55.88%, Val F1: 17.92% Time: 83.04475402832031 *
top-down:SEC: Iter:    100,  Train Loss: 3e+01,  Train Acc: 21.88%,Val Loss:   6.8,  Val Acc: 24.21%, Val F1:  3.54% Time: 83.04475402832031 *
top-down:CONN: Iter:    100,  Train Loss: 3e+01,  Train Acc:  3.12%,Val Loss:   6.8,  Val Acc: 12.88%, Val F1:  0.34% Time: 83.04475402832031 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   6.4,  Val Acc: 53.91%, Val F1: 25.36% Time: 159.41532230377197 *
top-down:SEC: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 28.12%,Val Loss:   6.4,  Val Acc: 30.39%, Val F1:  7.34% Time: 159.41532230377197 *
top-down:CONN: Iter:    200,  Train Loss: 3.1e+01,  Train Acc: 15.62%,Val Loss:   6.4,  Val Acc: 15.19%, Val F1:  0.83% Time: 159.41532230377197 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 4e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 55.97%, Val F1: 18.77% Time: 235.63533425331116 *
top-down:SEC: Iter:    300,  Train Loss: 4e+01,  Train Acc: 37.50%,Val Loss:   6.2,  Val Acc: 34.85%, Val F1: 11.44% Time: 235.63533425331116 *
top-down:CONN: Iter:    300,  Train Loss: 4e+01,  Train Acc: 18.75%,Val Loss:   6.2,  Val Acc: 15.19%, Val F1:  1.09% Time: 235.63533425331116 *
 
 
Train time usage: 304.63944578170776
Test time usage: 1.7497014999389648
TOP: Test Loss:   5.9,  Test Acc: 57.17%, Test F1: 29.70%
SEC: Test Loss:   5.9,  Test Acc: 38.69%, Test F1: 16.19%
CONN: Test Loss:   5.9,  Test Acc: 17.52%, Test F1:  1.86%
consistency_top_sec: 29.55%,  consistency_sec_conn: 15.01%, consistency_top_sec_conn: 12.03%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        68
 Contingency     0.5323    0.3653    0.4333       271
  Comparison     0.3636    0.0278    0.0516       144
   Expansion     0.5838    0.8831    0.7029       556

    accuracy                         0.5717      1039
   macro avg     0.3699    0.3190    0.2970      1039
weighted avg     0.5017    0.5717    0.4963      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4444    0.0741    0.1270        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.4428    0.6791    0.5361       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4286    0.0703    0.1208       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5208    0.3750    0.4360       200
    Expansion.Instantiation     0.8667    0.1102    0.1955       118
      Expansion.Restatement     0.2711    0.5613    0.3656       212
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.3869      1039
                  macro avg     0.2704    0.1700    0.1619      1039
               weighted avg     0.4441    0.3869    0.3405      1039

Epoch [2/15]
top-down:TOP: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.0,  Val Acc: 57.00%, Val F1: 35.66% Time: 8.598264217376709 *
top-down:SEC: Iter:    400,  Train Loss: 2.9e+01,  Train Acc: 31.25%,Val Loss:   6.0,  Val Acc: 37.42%, Val F1: 16.37% Time: 8.598264217376709 *
top-down:CONN: Iter:    400,  Train Loss: 2.9e+01,  Train Acc:  6.25%,Val Loss:   6.0,  Val Acc: 18.45%, Val F1:  2.05% Time: 8.598264217376709 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   5.9,  Val Acc: 54.25%, Val F1: 36.44% Time: 84.4681167602539 *
top-down:SEC: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   5.9,  Val Acc: 40.26%, Val F1: 18.48% Time: 84.4681167602539 *
top-down:CONN: Iter:    500,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   5.9,  Val Acc: 21.80%, Val F1:  3.07% Time: 84.4681167602539 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 56.57%, Val F1: 40.38% Time: 160.58851027488708 *
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 42.40%, Val F1: 20.89% Time: 160.58851027488708 *
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   5.7,  Val Acc: 22.23%, Val F1:  3.07% Time: 160.58851027488708 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   5.6,  Val Acc: 58.37%, Val F1: 43.31% Time: 236.87652826309204 *
top-down:SEC: Iter:    700,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   5.6,  Val Acc: 42.92%, Val F1: 19.82% Time: 236.87652826309204 *
top-down:CONN: Iter:    700,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   5.6,  Val Acc: 22.75%, Val F1:  3.37% Time: 236.87652826309204 *
 
 
Train time usage: 300.2721366882324
Test time usage: 1.7284767627716064
TOP: Test Loss:   5.2,  Test Acc: 61.02%, Test F1: 49.23%
SEC: Test Loss:   5.2,  Test Acc: 48.03%, Test F1: 25.94%
CONN: Test Loss:   5.2,  Test Acc: 23.87%, Test F1:  4.32%
consistency_top_sec: 41.19%,  consistency_sec_conn: 19.15%, consistency_top_sec_conn: 16.84%
              precision    recall  f1-score   support

    Temporal     0.4118    0.4118    0.4118        68
 Contingency     0.5288    0.3700    0.4353       273
  Comparison     0.5443    0.2986    0.3857       144
   Expansion     0.6591    0.8339    0.7363       554

    accuracy                         0.6102      1039
   macro avg     0.5360    0.4786    0.4923      1039
weighted avg     0.5927    0.6102    0.5874      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3333    0.4074    0.3667        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5000    0.6194    0.5533       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4881    0.3203    0.3868       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4646    0.5900    0.5198       200
    Expansion.Instantiation     0.6441    0.6387    0.6414       119
      Expansion.Restatement     0.4153    0.3602    0.3858       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4803      1039
                  macro avg     0.2587    0.2669    0.2594      1039
               weighted avg     0.4540    0.4803    0.4613      1039

Epoch [3/15]
top-down:TOP: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 56.91%, Val F1: 48.74% Time: 13.670444250106812 *
top-down:SEC: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 37.50%,Val Loss:   5.6,  Val Acc: 44.03%, Val F1: 24.40% Time: 13.670444250106812 *
top-down:CONN: Iter:    800,  Train Loss: 2.8e+01,  Train Acc: 25.00%,Val Loss:   5.6,  Val Acc: 23.18%, Val F1:  4.15% Time: 13.670444250106812 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   5.5,  Val Acc: 59.57%, Val F1: 47.37% Time: 90.14982175827026 *
top-down:SEC: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   5.5,  Val Acc: 46.09%, Val F1: 26.21% Time: 90.14982175827026 *
top-down:CONN: Iter:    900,  Train Loss: 2.9e+01,  Train Acc: 21.88%,Val Loss:   5.5,  Val Acc: 24.89%, Val F1:  4.38% Time: 90.14982175827026 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 62.23%, Val F1: 46.36% Time: 166.44743418693542 *
top-down:SEC: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   5.4,  Val Acc: 47.04%, Val F1: 24.42% Time: 166.44743418693542 *
top-down:CONN: Iter:   1000,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   5.4,  Val Acc: 24.81%, Val F1:  4.54% Time: 166.44743418693542 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 63.26%, Val F1: 50.48% Time: 242.9326684474945 *
top-down:SEC: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 47.47%, Val F1: 26.18% Time: 242.9326684474945 *
top-down:CONN: Iter:   1100,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   5.4,  Val Acc: 26.09%, Val F1:  4.81% Time: 242.9326684474945 *
 
 
Train time usage: 301.660621881485
Test time usage: 1.7203235626220703
TOP: Test Loss:   5.2,  Test Acc: 63.72%, Test F1: 51.38%
SEC: Test Loss:   5.2,  Test Acc: 49.18%, Test F1: 27.42%
CONN: Test Loss:   5.2,  Test Acc: 23.97%, Test F1:  5.10%
consistency_top_sec: 45.04%,  consistency_sec_conn: 19.83%, consistency_top_sec_conn: 18.29%
              precision    recall  f1-score   support

    Temporal     0.4000    0.3235    0.3577        68
 Contingency     0.6667    0.3516    0.4604       273
  Comparison     0.4926    0.4653    0.4786       144
   Expansion     0.6776    0.8610    0.7583       554

    accuracy                         0.6372      1039
   macro avg     0.5592    0.5004    0.5138      1039
weighted avg     0.6309    0.6372    0.6151      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3731    0.4630    0.4132        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5670    0.4721    0.5152       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4248    0.5078    0.4626       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4363    0.6850    0.5331       200
    Expansion.Instantiation     0.7087    0.6186    0.6606       118
      Expansion.Restatement     0.4719    0.3981    0.4319       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4918      1039
                  macro avg     0.2711    0.2859    0.2742      1039
               weighted avg     0.4788    0.4918    0.4772      1039

Epoch [4/15]
top-down:TOP: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 81.25%,Val Loss:   5.3,  Val Acc: 64.12%, Val F1: 52.98% Time: 18.769888877868652 *
top-down:SEC: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 75.00%,Val Loss:   5.3,  Val Acc: 49.70%, Val F1: 26.08% Time: 18.769888877868652 *
top-down:CONN: Iter:   1200,  Train Loss: 3.3e+01,  Train Acc: 40.62%,Val Loss:   5.3,  Val Acc: 27.30%, Val F1:  5.39% Time: 18.769888877868652 *
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 63.61%, Val F1: 49.86% Time: 93.7888331413269 
top-down:SEC: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 50.00%,Val Loss:   5.4,  Val Acc: 48.41%, Val F1: 26.99% Time: 93.7888331413269 
top-down:CONN: Iter:   1300,  Train Loss: 3.5e+01,  Train Acc: 43.75%,Val Loss:   5.4,  Val Acc: 27.04%, Val F1:  5.24% Time: 93.7888331413269 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   5.4,  Val Acc: 62.92%, Val F1: 54.12% Time: 170.37128162384033 *
top-down:SEC: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 48.50%, Val F1: 29.06% Time: 170.37128162384033 *
top-down:CONN: Iter:   1400,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   5.4,  Val Acc: 28.50%, Val F1:  6.49% Time: 170.37128162384033 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   5.5,  Val Acc: 60.60%, Val F1: 47.20% Time: 245.11584973335266 
top-down:SEC: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 43.75%,Val Loss:   5.5,  Val Acc: 46.95%, Val F1: 26.87% Time: 245.11584973335266 
top-down:CONN: Iter:   1500,  Train Loss: 3.3e+01,  Train Acc: 21.88%,Val Loss:   5.5,  Val Acc: 25.84%, Val F1:  5.75% Time: 245.11584973335266 
 
 
Train time usage: 298.8558027744293
Test time usage: 1.7302556037902832
TOP: Test Loss:   5.0,  Test Acc: 64.49%, Test F1: 53.25%
SEC: Test Loss:   5.0,  Test Acc: 51.59%, Test F1: 31.74%
CONN: Test Loss:   5.0,  Test Acc: 23.97%, Test F1:  6.70%
consistency_top_sec: 47.74%,  consistency_sec_conn: 19.63%, consistency_top_sec_conn: 18.48%
              precision    recall  f1-score   support

    Temporal     0.5806    0.2647    0.3636        68
 Contingency     0.6099    0.4036    0.4858       275
  Comparison     0.5441    0.5139    0.5286       144
   Expansion     0.6768    0.8460    0.7520       552

    accuracy                         0.6449      1039
   macro avg     0.6029    0.5071    0.5325      1039
weighted avg     0.6344    0.6449    0.6252      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4737    0.3333    0.3913        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5714    0.5224    0.5458       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5000    0.5469    0.5224       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4958    0.5900    0.5388       200
    Expansion.Instantiation     0.6909    0.6441    0.6667       118
      Expansion.Restatement     0.4319    0.5236    0.4733       212
      Expansion.Alternative     0.3750    0.3333    0.3529         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5159      1039
                  macro avg     0.3217    0.3176    0.3174      1039
               weighted avg     0.4989    0.5159    0.5046      1039

Epoch [5/15]
top-down:TOP: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 63.52%, Val F1: 51.63% Time: 22.248209238052368 
top-down:SEC: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   5.4,  Val Acc: 49.70%, Val F1: 31.99% Time: 22.248209238052368 
top-down:CONN: Iter:   1600,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   5.4,  Val Acc: 26.44%, Val F1:  5.51% Time: 22.248209238052368 
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   5.5,  Val Acc: 60.94%, Val F1: 50.93% Time: 96.51686072349548 
top-down:SEC: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.5,  Val Acc: 49.10%, Val F1: 31.53% Time: 96.51686072349548 
top-down:CONN: Iter:   1700,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   5.5,  Val Acc: 25.84%, Val F1:  5.84% Time: 96.51686072349548 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 59.91%, Val F1: 51.50% Time: 171.31208753585815 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.6,  Val Acc: 46.70%, Val F1: 30.06% Time: 171.31208753585815 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 56.25%,Val Loss:   5.6,  Val Acc: 25.67%, Val F1:  5.62% Time: 171.31208753585815 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 75.00%,Val Loss:   5.4,  Val Acc: 61.72%, Val F1: 51.24% Time: 246.05108189582825 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.4,  Val Acc: 48.50%, Val F1: 28.74% Time: 246.05108189582825 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 34.38%,Val Loss:   5.4,  Val Acc: 27.98%, Val F1:  6.53% Time: 246.05108189582825 
 
 
Train time usage: 294.5427601337433
Test time usage: 1.7532062530517578
TOP: Test Loss:   5.2,  Test Acc: 63.52%, Test F1: 53.71%
SEC: Test Loss:   5.2,  Test Acc: 51.30%, Test F1: 32.71%
CONN: Test Loss:   5.2,  Test Acc: 25.12%, Test F1:  7.49%
consistency_top_sec: 48.80%,  consistency_sec_conn: 20.40%, consistency_top_sec_conn: 19.63%
              precision    recall  f1-score   support

    Temporal     0.5500    0.3235    0.4074        68
 Contingency     0.6289    0.4436    0.5203       275
  Comparison     0.4194    0.5417    0.4727       144
   Expansion     0.7076    0.7935    0.7481       552

    accuracy                         0.6352      1039
   macro avg     0.5765    0.5256    0.5371      1039
weighted avg     0.6365    0.6352    0.6273      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5814    0.4630    0.5155        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5733    0.4813    0.5233       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.3968    0.5859    0.4732       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4979    0.5950    0.5421       200
    Expansion.Instantiation     0.7255    0.6271    0.6727       118
      Expansion.Restatement     0.4777    0.5047    0.4908       212
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5130      1039
                  macro avg     0.3260    0.3365    0.3271      1039
               weighted avg     0.5056    0.5130    0.5043      1039

Epoch [6/15]
top-down:TOP: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 61.80%, Val F1: 49.83% Time: 27.313690900802612 
top-down:SEC: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   5.5,  Val Acc: 48.24%, Val F1: 29.81% Time: 27.313690900802612 
top-down:CONN: Iter:   2000,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   5.5,  Val Acc: 28.33%, Val F1:  6.76% Time: 27.313690900802612 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   5.5,  Val Acc: 63.52%, Val F1: 52.12% Time: 101.97216081619263 
top-down:SEC: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   5.5,  Val Acc: 48.84%, Val F1: 30.21% Time: 101.97216081619263 
top-down:CONN: Iter:   2100,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   5.5,  Val Acc: 27.38%, Val F1:  6.97% Time: 101.97216081619263 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.5,  Val Acc: 61.63%, Val F1: 50.81% Time: 176.06288027763367 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   5.5,  Val Acc: 48.76%, Val F1: 28.61% Time: 176.06288027763367 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   5.5,  Val Acc: 29.27%, Val F1:  7.29% Time: 176.06288027763367 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 78.12%,Val Loss:   5.6,  Val Acc: 61.97%, Val F1: 51.49% Time: 250.45990300178528 
top-down:SEC: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 62.50%,Val Loss:   5.6,  Val Acc: 48.24%, Val F1: 30.00% Time: 250.45990300178528 
top-down:CONN: Iter:   2300,  Train Loss: 3.6e+01,  Train Acc: 40.62%,Val Loss:   5.6,  Val Acc: 28.24%, Val F1:  7.03% Time: 250.45990300178528 
 
 
Train time usage: 293.6038954257965
Test time usage: 1.7514266967773438
TOP: Test Loss:   5.2,  Test Acc: 64.77%, Test F1: 55.07%
SEC: Test Loss:   5.2,  Test Acc: 51.59%, Test F1: 32.42%
CONN: Test Loss:   5.2,  Test Acc: 25.51%, Test F1:  7.83%
consistency_top_sec: 49.37%,  consistency_sec_conn: 21.17%, consistency_top_sec_conn: 20.69%
              precision    recall  f1-score   support

    Temporal     0.4898    0.3529    0.4103        68
 Contingency     0.5887    0.4964    0.5386       274
  Comparison     0.5397    0.4722    0.5037       144
   Expansion     0.7030    0.8047    0.7504       553

    accuracy                         0.6477      1039
   macro avg     0.5803    0.5316    0.5507      1039
weighted avg     0.6363    0.6477    0.6381      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4600    0.4259    0.4423        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5657    0.5299    0.5472       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5200    0.5078    0.5138       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4836    0.5900    0.5315       200
    Expansion.Instantiation     0.7037    0.6441    0.6726       118
      Expansion.Restatement     0.4500    0.5094    0.4779       212
      Expansion.Alternative     0.3333    0.4444    0.3810         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5159      1039
                  macro avg     0.3197    0.3320    0.3242      1039
               weighted avg     0.5016    0.5159    0.5069      1039

Epoch [7/15]
top-down:TOP: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 75.00%,Val Loss:   5.7,  Val Acc: 63.18%, Val F1: 52.33% Time: 32.38331389427185 
top-down:SEC: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   5.7,  Val Acc: 47.38%, Val F1: 29.23% Time: 32.38331389427185 
top-down:CONN: Iter:   2400,  Train Loss: 3.2e+01,  Train Acc: 31.25%,Val Loss:   5.7,  Val Acc: 27.04%, Val F1:  6.67% Time: 32.38331389427185 
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.8,  Val Acc: 61.72%, Val F1: 50.14% Time: 107.28090333938599 
top-down:SEC: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   5.8,  Val Acc: 45.84%, Val F1: 26.98% Time: 107.28090333938599 
top-down:CONN: Iter:   2500,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   5.8,  Val Acc: 26.09%, Val F1:  6.74% Time: 107.28090333938599 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 87.50%,Val Loss:   5.9,  Val Acc: 62.49%, Val F1: 51.40% Time: 182.11037421226501 
top-down:SEC: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 47.04%, Val F1: 28.29% Time: 182.11037421226501 
top-down:CONN: Iter:   2600,  Train Loss: 3.4e+01,  Train Acc: 25.00%,Val Loss:   5.9,  Val Acc: 26.18%, Val F1:  6.02% Time: 182.11037421226501 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 61.89%, Val F1: 50.86% Time: 256.54029273986816 
top-down:SEC: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 62.50%,Val Loss:   5.8,  Val Acc: 47.21%, Val F1: 28.54% Time: 256.54029273986816 
top-down:CONN: Iter:   2700,  Train Loss: 3.4e+01,  Train Acc: 34.38%,Val Loss:   5.8,  Val Acc: 27.04%, Val F1:  7.31% Time: 256.54029273986816 
 
 
Train time usage: 294.4675323963165
Test time usage: 1.7241156101226807
TOP: Test Loss:   5.2,  Test Acc: 64.29%, Test F1: 54.14%
SEC: Test Loss:   5.2,  Test Acc: 51.88%, Test F1: 32.40%
CONN: Test Loss:   5.2,  Test Acc: 25.70%, Test F1:  7.64%
consistency_top_sec: 49.86%,  consistency_sec_conn: 21.85%, consistency_top_sec_conn: 21.08%
              precision    recall  f1-score   support

    Temporal     0.5135    0.2794    0.3619        68
 Contingency     0.5699    0.5636    0.5667       275
  Comparison     0.5271    0.4722    0.4982       144
   Expansion     0.7088    0.7717    0.7389       552

    accuracy                         0.6429      1039
   macro avg     0.5798    0.5218    0.5414      1039
weighted avg     0.6341    0.6429    0.6353      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4762    0.3704    0.4167        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5564    0.5709    0.5635       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4924    0.5078    0.5000       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5046    0.5500    0.5263       200
    Expansion.Instantiation     0.7143    0.6356    0.6726       118
      Expansion.Restatement     0.4480    0.5283    0.4848       212
      Expansion.Alternative     0.3636    0.4444    0.4000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5188      1039
                  macro avg     0.3232    0.3279    0.3240      1039
               weighted avg     0.5017    0.5188    0.5087      1039

Epoch [8/15]
top-down:TOP: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   5.9,  Val Acc: 62.06%, Val F1: 52.36% Time: 37.40093493461609 
top-down:SEC: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 68.75%,Val Loss:   5.9,  Val Acc: 47.90%, Val F1: 30.27% Time: 37.40093493461609 
top-down:CONN: Iter:   2800,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   5.9,  Val Acc: 26.44%, Val F1:  6.77% Time: 37.40093493461609 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.1,  Val Acc: 60.86%, Val F1: 50.07% Time: 111.75857877731323 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.1,  Val Acc: 44.55%, Val F1: 28.97% Time: 111.75857877731323 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 40.62%,Val Loss:   6.1,  Val Acc: 26.01%, Val F1:  7.15% Time: 111.75857877731323 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.0,  Val Acc: 62.66%, Val F1: 51.94% Time: 186.0775990486145 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 46.27%, Val F1: 27.12% Time: 186.0775990486145 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   6.0,  Val Acc: 26.95%, Val F1:  7.64% Time: 186.0775990486145 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.9,  Val Acc: 62.49%, Val F1: 51.74% Time: 260.560418844223 
top-down:SEC: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.9,  Val Acc: 46.61%, Val F1: 28.86% Time: 260.560418844223 
top-down:CONN: Iter:   3100,  Train Loss: 2.7e+01,  Train Acc: 46.88%,Val Loss:   5.9,  Val Acc: 26.87%, Val F1:  7.09% Time: 260.560418844223 
 
 
Train time usage: 293.577086687088
Test time usage: 1.7355287075042725
TOP: Test Loss:   5.6,  Test Acc: 65.54%, Test F1: 54.82%
SEC: Test Loss:   5.6,  Test Acc: 51.30%, Test F1: 32.03%
CONN: Test Loss:   5.6,  Test Acc: 25.22%, Test F1:  8.30%
consistency_top_sec: 50.34%,  consistency_sec_conn: 20.50%, consistency_top_sec_conn: 20.12%
              precision    recall  f1-score   support

    Temporal     0.4878    0.2941    0.3670        68
 Contingency     0.6219    0.4545    0.5252       275
  Comparison     0.6000    0.5000    0.5455       144
   Expansion     0.6854    0.8406    0.7551       552

    accuracy                         0.6554      1039
   macro avg     0.5988    0.5223    0.5482      1039
weighted avg     0.6438    0.6554    0.6398      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4524    0.3519    0.3958        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5853    0.4739    0.5237       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5078    0.5078    0.5078       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4797    0.6500    0.5520       200
    Expansion.Instantiation     0.6529    0.6695    0.6611       118
      Expansion.Restatement     0.4596    0.5094    0.4832       212
      Expansion.Alternative     0.3125    0.5556    0.4000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5130      1039
                  macro avg     0.3136    0.3380    0.3203      1039
               weighted avg     0.5000    0.5130    0.5016      1039

Epoch [9/15]
top-down:TOP: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   6.0,  Val Acc: 61.63%, Val F1: 51.47% Time: 42.499494314193726 
top-down:SEC: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.0,  Val Acc: 46.87%, Val F1: 29.53% Time: 42.499494314193726 
top-down:CONN: Iter:   3200,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   6.0,  Val Acc: 26.44%, Val F1:  6.49% Time: 42.499494314193726 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.1,  Val Acc: 62.75%, Val F1: 51.59% Time: 116.92891240119934 
top-down:SEC: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   6.1,  Val Acc: 46.44%, Val F1: 29.81% Time: 116.92891240119934 
top-down:CONN: Iter:   3300,  Train Loss: 3e+01,  Train Acc: 71.88%,Val Loss:   6.1,  Val Acc: 26.44%, Val F1:  6.76% Time: 116.92891240119934 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 63.09%, Val F1: 51.83% Time: 191.3180809020996 
top-down:SEC: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 46.52%, Val F1: 28.87% Time: 191.3180809020996 
top-down:CONN: Iter:   3400,  Train Loss: 2.5e+01,  Train Acc: 40.62%,Val Loss:   6.2,  Val Acc: 27.47%, Val F1:  7.41% Time: 191.3180809020996 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 62.66%, Val F1: 52.17% Time: 265.7519567012787 
top-down:SEC: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 84.38%,Val Loss:   6.1,  Val Acc: 46.35%, Val F1: 27.85% Time: 265.7519567012787 
top-down:CONN: Iter:   3500,  Train Loss: 3.3e+01,  Train Acc: 46.88%,Val Loss:   6.1,  Val Acc: 27.47%, Val F1:  7.74% Time: 265.7519567012787 
 
 
Train time usage: 293.6447443962097
Test time usage: 1.7352752685546875
TOP: Test Loss:   5.8,  Test Acc: 62.75%, Test F1: 52.74%
SEC: Test Loss:   5.8,  Test Acc: 50.91%, Test F1: 33.50%
CONN: Test Loss:   5.8,  Test Acc: 25.89%, Test F1:  8.73%
consistency_top_sec: 49.09%,  consistency_sec_conn: 20.89%, consistency_top_sec_conn: 20.12%
              precision    recall  f1-score   support

    Temporal     0.3818    0.3088    0.3415        68
 Contingency     0.5721    0.4635    0.5121       274
  Comparison     0.4878    0.5556    0.5195       144
   Expansion     0.7090    0.7667    0.7368       553

    accuracy                         0.6275      1039
   macro avg     0.5377    0.5237    0.5274      1039
weighted avg     0.6208    0.6275    0.6215      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4151    0.4074    0.4112        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5721    0.4888    0.5272       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4581    0.5547    0.5018       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4829    0.5650    0.5207       200
    Expansion.Instantiation     0.7054    0.6695    0.6870       118
      Expansion.Restatement     0.4569    0.5000    0.4775       212
      Expansion.Alternative     0.4375    0.7778    0.5600         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5091      1039
                  macro avg     0.3207    0.3603    0.3350      1039
               weighted avg     0.4956    0.5091    0.4997      1039

Epoch [10/15]
top-down:TOP: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 93.75%,Val Loss:   6.3,  Val Acc: 61.89%, Val F1: 51.35% Time: 47.62910008430481 
top-down:SEC: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 78.12%,Val Loss:   6.3,  Val Acc: 46.95%, Val F1: 27.96% Time: 47.62910008430481 
top-down:CONN: Iter:   3600,  Train Loss: 2.5e+01,  Train Acc: 37.50%,Val Loss:   6.3,  Val Acc: 27.04%, Val F1:  7.38% Time: 47.62910008430481 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 61.72%, Val F1: 51.88% Time: 122.1343309879303 
top-down:SEC: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   6.4,  Val Acc: 44.72%, Val F1: 28.06% Time: 122.1343309879303 
top-down:CONN: Iter:   3700,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.4,  Val Acc: 26.44%, Val F1:  7.72% Time: 122.1343309879303 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 62.75%, Val F1: 51.30% Time: 196.60758805274963 
top-down:SEC: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 46.70%, Val F1: 28.19% Time: 196.60758805274963 
top-down:CONN: Iter:   3800,  Train Loss: 2.7e+01,  Train Acc: 34.38%,Val Loss:   6.2,  Val Acc: 27.30%, Val F1:  7.14% Time: 196.60758805274963 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.3,  Val Acc: 62.92%, Val F1: 50.97% Time: 271.3014783859253 
top-down:SEC: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.3,  Val Acc: 46.44%, Val F1: 28.53% Time: 271.3014783859253 
top-down:CONN: Iter:   3900,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   6.3,  Val Acc: 26.61%, Val F1:  7.08% Time: 271.3014783859253 
 
 
Train time usage: 294.19929909706116
Test time usage: 1.7192013263702393
TOP: Test Loss:   5.9,  Test Acc: 64.77%, Test F1: 55.10%
SEC: Test Loss:   5.9,  Test Acc: 51.20%, Test F1: 32.62%
CONN: Test Loss:   5.9,  Test Acc: 25.31%, Test F1:  8.46%
consistency_top_sec: 49.47%,  consistency_sec_conn: 20.79%, consistency_top_sec_conn: 20.21%
              precision    recall  f1-score   support

    Temporal     0.5000    0.3088    0.3818        68
 Contingency     0.6055    0.4800    0.5355       275
  Comparison     0.5338    0.5486    0.5411       144
   Expansion     0.6989    0.7989    0.7456       552

    accuracy                         0.6477      1039
   macro avg     0.5845    0.5341    0.5510      1039
weighted avg     0.6383    0.6477    0.6378      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4878    0.3704    0.4211        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5852    0.5000    0.5392       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4894    0.5391    0.5130       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4468    0.6300    0.5228       200
    Expansion.Instantiation     0.7117    0.6695    0.6900       118
      Expansion.Restatement     0.4670    0.4670    0.4670       212
      Expansion.Alternative     0.3571    0.5556    0.4348         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5120      1039
                  macro avg     0.3223    0.3392    0.3262      1039
               weighted avg     0.5018    0.5120    0.5022      1039

Epoch [11/15]
top-down:TOP: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   6.4,  Val Acc: 62.75%, Val F1: 51.19% Time: 52.80251407623291 
top-down:SEC: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 81.25%,Val Loss:   6.4,  Val Acc: 45.41%, Val F1: 27.08% Time: 52.80251407623291 
top-down:CONN: Iter:   4000,  Train Loss: 3.2e+01,  Train Acc: 37.50%,Val Loss:   6.4,  Val Acc: 26.78%, Val F1:  7.21% Time: 52.80251407623291 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.5,  Val Acc: 61.72%, Val F1: 50.78% Time: 127.53908658027649 
top-down:SEC: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   6.5,  Val Acc: 45.75%, Val F1: 26.68% Time: 127.53908658027649 
top-down:CONN: Iter:   4100,  Train Loss: 3.2e+01,  Train Acc: 31.25%,Val Loss:   6.5,  Val Acc: 26.44%, Val F1:  7.40% Time: 127.53908658027649 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   6.4,  Val Acc: 61.89%, Val F1: 49.80% Time: 202.21914768218994 
top-down:SEC: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 62.50%,Val Loss:   6.4,  Val Acc: 45.41%, Val F1: 27.74% Time: 202.21914768218994 
top-down:CONN: Iter:   4200,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   6.4,  Val Acc: 26.52%, Val F1:  7.29% Time: 202.21914768218994 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   6.6,  Val Acc: 61.63%, Val F1: 51.19% Time: 276.6276898384094 
top-down:SEC: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.6,  Val Acc: 46.09%, Val F1: 28.38% Time: 276.6276898384094 
top-down:CONN: Iter:   4300,  Train Loss: 2.6e+01,  Train Acc: 40.62%,Val Loss:   6.6,  Val Acc: 25.84%, Val F1:  6.92% Time: 276.6276898384094 
 
 
Train time usage: 294.32793855667114
Test time usage: 1.710360050201416
TOP: Test Loss:   6.1,  Test Acc: 62.66%, Test F1: 52.04%
SEC: Test Loss:   6.1,  Test Acc: 49.57%, Test F1: 32.00%
CONN: Test Loss:   6.1,  Test Acc: 25.60%, Test F1:  8.76%
consistency_top_sec: 47.93%,  consistency_sec_conn: 21.08%, consistency_top_sec_conn: 20.50%
              precision    recall  f1-score   support

    Temporal     0.3846    0.2941    0.3333        68
 Contingency     0.5905    0.4526    0.5124       274
  Comparison     0.5070    0.5000    0.5035       144
   Expansion     0.6850    0.7866    0.7323       553

    accuracy                         0.6266      1039
   macro avg     0.5418    0.5083    0.5204      1039
weighted avg     0.6158    0.6266    0.6165      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3878    0.3519    0.3689        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5826    0.4757    0.5237       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.4928    0.5312    0.5113       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4681    0.5500    0.5057       200
    Expansion.Instantiation     0.6897    0.6723    0.6809       119
      Expansion.Restatement     0.4118    0.4953    0.4497       212
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.4957      1039
                  macro avg     0.3098    0.3403    0.3200      1039
               weighted avg     0.4869    0.4957    0.4880      1039

Epoch [12/15]
top-down:TOP: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.5,  Val Acc: 61.72%, Val F1: 51.10% Time: 57.66733193397522 
top-down:SEC: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 84.38%,Val Loss:   6.5,  Val Acc: 46.09%, Val F1: 28.06% Time: 57.66733193397522 
top-down:CONN: Iter:   4400,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   6.5,  Val Acc: 27.21%, Val F1:  7.59% Time: 57.66733193397522 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 61.37%, Val F1: 50.25% Time: 132.59163618087769 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   6.6,  Val Acc: 45.24%, Val F1: 28.45% Time: 132.59163618087769 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   6.6,  Val Acc: 26.61%, Val F1:  7.64% Time: 132.59163618087769 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   6.6,  Val Acc: 62.75%, Val F1: 51.62% Time: 207.04217338562012 
top-down:SEC: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 44.98%, Val F1: 27.48% Time: 207.04217338562012 
top-down:CONN: Iter:   4600,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   6.6,  Val Acc: 26.52%, Val F1:  7.42% Time: 207.04217338562012 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 61.72%, Val F1: 50.46% Time: 281.4155578613281 
top-down:SEC: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 81.25%,Val Loss:   6.7,  Val Acc: 45.49%, Val F1: 29.02% Time: 281.4155578613281 
top-down:CONN: Iter:   4700,  Train Loss: 3e+01,  Train Acc: 40.62%,Val Loss:   6.7,  Val Acc: 26.52%, Val F1:  7.64% Time: 281.4155578613281 
 
 
Train time usage: 294.0751054286957
Test time usage: 1.7370870113372803
TOP: Test Loss:   6.1,  Test Acc: 63.91%, Test F1: 53.82%
SEC: Test Loss:   6.1,  Test Acc: 51.40%, Test F1: 31.72%
CONN: Test Loss:   6.1,  Test Acc: 25.89%, Test F1:  9.31%
consistency_top_sec: 49.47%,  consistency_sec_conn: 22.23%, consistency_top_sec_conn: 21.37%
              precision    recall  f1-score   support

    Temporal     0.3889    0.3088    0.3443        68
 Contingency     0.5875    0.5146    0.5486       274
  Comparison     0.5368    0.5069    0.5214       144
   Expansion     0.7044    0.7758    0.7384       553

    accuracy                         0.6391      1039
   macro avg     0.5544    0.5265    0.5382      1039
weighted avg     0.6297    0.6391    0.6325      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4167    0.3704    0.3922        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5647    0.5393    0.5517       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5234    0.5234    0.5234       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5021    0.5950    0.5446       200
    Expansion.Instantiation     0.6838    0.6723    0.6780       119
      Expansion.Restatement     0.4329    0.4717    0.4515       212
      Expansion.Alternative     0.2857    0.4444    0.3478         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5140      1039
                  macro avg     0.3099    0.3288    0.3172      1039
               weighted avg     0.4970    0.5140    0.5043      1039

Epoch [13/15]
top-down:TOP: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.6,  Val Acc: 62.23%, Val F1: 50.95% Time: 62.72261571884155 
top-down:SEC: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 78.12%,Val Loss:   6.6,  Val Acc: 45.75%, Val F1: 28.49% Time: 62.72261571884155 
top-down:CONN: Iter:   4800,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   6.6,  Val Acc: 26.78%, Val F1:  7.75% Time: 62.72261571884155 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 62.40%, Val F1: 51.60% Time: 137.06060528755188 
top-down:SEC: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   6.6,  Val Acc: 45.84%, Val F1: 28.31% Time: 137.06060528755188 
top-down:CONN: Iter:   4900,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   6.6,  Val Acc: 26.61%, Val F1:  7.34% Time: 137.06060528755188 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.6,  Val Acc: 61.97%, Val F1: 50.44% Time: 211.45601677894592 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.6,  Val Acc: 46.61%, Val F1: 29.05% Time: 211.45601677894592 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   6.6,  Val Acc: 27.38%, Val F1:  8.12% Time: 211.45601677894592 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   6.7,  Val Acc: 62.15%, Val F1: 50.83% Time: 285.8203580379486 
top-down:SEC: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 46.35%, Val F1: 28.75% Time: 285.8203580379486 
top-down:CONN: Iter:   5100,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 26.09%, Val F1:  7.48% Time: 285.8203580379486 
 
 
Train time usage: 293.4565062522888
Test time usage: 1.7145130634307861
TOP: Test Loss:   6.2,  Test Acc: 63.23%, Test F1: 52.58%
SEC: Test Loss:   6.2,  Test Acc: 50.43%, Test F1: 32.05%
CONN: Test Loss:   6.2,  Test Acc: 25.31%, Test F1:  8.65%
consistency_top_sec: 48.80%,  consistency_sec_conn: 20.69%, consistency_top_sec_conn: 20.21%
              precision    recall  f1-score   support

    Temporal     0.3390    0.2941    0.3150        68
 Contingency     0.5973    0.4818    0.5333       274
  Comparison     0.5645    0.4861    0.5224       144
   Expansion     0.6850    0.7866    0.7323       553

    accuracy                         0.6323      1039
   macro avg     0.5465    0.5121    0.5258      1039
weighted avg     0.6225    0.6323    0.6234      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4000    0.3704    0.3846        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5641    0.4944    0.5269       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5537    0.5234    0.5382       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4743    0.6000    0.5298       200
    Expansion.Instantiation     0.6957    0.6723    0.6838       119
      Expansion.Restatement     0.4219    0.4717    0.4454       212
      Expansion.Alternative     0.3333    0.5556    0.4167         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5043      1039
                  macro avg     0.3130    0.3352    0.3205      1039
               weighted avg     0.4939    0.5043    0.4965      1039

Epoch [14/15]
top-down:TOP: Iter:   5200,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 61.46%, Val F1: 51.03% Time: 67.88938212394714 
top-down:SEC: Iter:   5200,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.7,  Val Acc: 46.78%, Val F1: 29.35% Time: 67.88938212394714 
top-down:CONN: Iter:   5200,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   6.7,  Val Acc: 27.04%, Val F1:  7.32% Time: 67.88938212394714 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 61.72%, Val F1: 50.75% Time: 142.30408596992493 
top-down:SEC: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 46.35%, Val F1: 28.58% Time: 142.30408596992493 
top-down:CONN: Iter:   5300,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   6.7,  Val Acc: 26.78%, Val F1:  7.81% Time: 142.30408596992493 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 96.88%,Val Loss:   6.7,  Val Acc: 62.66%, Val F1: 51.77% Time: 216.79455518722534 
top-down:SEC: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   6.7,  Val Acc: 46.78%, Val F1: 29.24% Time: 216.79455518722534 
top-down:CONN: Iter:   5400,  Train Loss: 3.3e+01,  Train Acc: 56.25%,Val Loss:   6.7,  Val Acc: 26.52%, Val F1:  7.72% Time: 216.79455518722534 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 100.00%,Val Loss:   6.7,  Val Acc: 61.80%, Val F1: 51.28% Time: 291.1610941886902 
top-down:SEC: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 90.62%,Val Loss:   6.7,  Val Acc: 46.27%, Val F1: 28.91% Time: 291.1610941886902 
top-down:CONN: Iter:   5500,  Train Loss: 3.5e+01,  Train Acc: 53.12%,Val Loss:   6.7,  Val Acc: 26.18%, Val F1:  7.54% Time: 291.1610941886902 
 
 
Train time usage: 293.6779987812042
Test time usage: 1.7151408195495605
TOP: Test Loss:   6.2,  Test Acc: 62.85%, Test F1: 52.46%
SEC: Test Loss:   6.2,  Test Acc: 50.43%, Test F1: 32.45%
CONN: Test Loss:   6.2,  Test Acc: 26.66%, Test F1:  9.40%
consistency_top_sec: 48.80%,  consistency_sec_conn: 22.04%, consistency_top_sec_conn: 21.27%
              precision    recall  f1-score   support

    Temporal     0.3448    0.2941    0.3175        68
 Contingency     0.5781    0.5000    0.5362       274
  Comparison     0.5294    0.5000    0.5143       144
   Expansion     0.6974    0.7667    0.7304       553

    accuracy                         0.6285      1039
   macro avg     0.5374    0.5152    0.5246      1039
weighted avg     0.6196    0.6285    0.6222      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.3846    0.3704    0.3774        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5610    0.5169    0.5380       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5115    0.5234    0.5174       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4812    0.5750    0.5239       200
    Expansion.Instantiation     0.7117    0.6639    0.6870       119
      Expansion.Restatement     0.4267    0.4670    0.4459       212
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5043      1039
                  macro avg     0.3138    0.3439    0.3245      1039
               weighted avg     0.4916    0.5043    0.4963      1039

Epoch [15/15]
top-down:TOP: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 62.58%, Val F1: 51.04% Time: 72.82604336738586 
top-down:SEC: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.7,  Val Acc: 46.09%, Val F1: 27.98% Time: 72.82604336738586 
top-down:CONN: Iter:   5600,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   6.7,  Val Acc: 26.52%, Val F1:  7.76% Time: 72.82604336738586 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.8,  Val Acc: 62.15%, Val F1: 51.17% Time: 147.20907282829285 
top-down:SEC: Iter:   5700,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   6.8,  Val Acc: 45.58%, Val F1: 28.45% Time: 147.20907282829285 
top-down:CONN: Iter:   5700,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   6.8,  Val Acc: 26.01%, Val F1:  7.55% Time: 147.20907282829285 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 84.38%,Val Loss:   6.7,  Val Acc: 62.58%, Val F1: 51.40% Time: 221.6095004081726 
top-down:SEC: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 75.00%,Val Loss:   6.7,  Val Acc: 46.01%, Val F1: 27.88% Time: 221.6095004081726 
top-down:CONN: Iter:   5800,  Train Loss: 2.4e+01,  Train Acc: 56.25%,Val Loss:   6.7,  Val Acc: 26.35%, Val F1:  7.58% Time: 221.6095004081726 
 
 
Train time usage: 291.73480439186096
Test time usage: 1.7273828983306885
TOP: Test Loss:   6.2,  Test Acc: 62.85%, Test F1: 52.61%
SEC: Test Loss:   6.2,  Test Acc: 50.91%, Test F1: 32.99%
CONN: Test Loss:   6.2,  Test Acc: 26.56%, Test F1:  9.49%
consistency_top_sec: 49.18%,  consistency_sec_conn: 22.04%, consistency_top_sec_conn: 21.08%
              precision    recall  f1-score   support

    Temporal     0.3621    0.3088    0.3333        68
 Contingency     0.5837    0.4964    0.5365       274
  Comparison     0.5145    0.4931    0.5035       144
   Expansion     0.6967    0.7685    0.7309       553

    accuracy                         0.6285      1039
   macro avg     0.5392    0.5167    0.5261      1039
weighted avg     0.6198    0.6285    0.6221      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.4118    0.3889    0.4000        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5633    0.5169    0.5391       267
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5077    0.5156    0.5116       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4876    0.5900    0.5339       200
    Expansion.Instantiation     0.7182    0.6639    0.6900       119
      Expansion.Restatement     0.4335    0.4764    0.4539       212
      Expansion.Alternative     0.4000    0.6667    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5091      1039
                  macro avg     0.3202    0.3471    0.3299      1039
               weighted avg     0.4967    0.5091    0.5011      1039

dev_best_acc_top: 62.92%,  dev_best_f1_top: 54.12%, 
dev_best_acc_sec: 48.50%,  dev_best_f1_sec: 29.06%, 
dev_best_acc_conn: 28.50%,  dev_best_f1_conn:  6.49%
