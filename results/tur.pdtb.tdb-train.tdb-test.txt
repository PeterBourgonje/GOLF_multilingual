nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'data/pdtb_tr_tdb_train_tdb_test/data/', 'log_file': 'data/pdtb_tr_tdb_train_tdb_test/log/', 'save_file': 'data/pdtb_tr_tdb_train_tdb_test/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March04-23:15:54', 'log': 'data/pdtb_tr_tdb_train_tdb_test/log/March04-23:15:54.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]183it [00:00, 1823.29it/s]435it [00:00, 2230.32it/s]693it [00:00, 2388.21it/s]932it [00:00, 2280.56it/s]1161it [00:00, 1936.05it/s]1362it [00:00, 1735.07it/s]1543it [00:00, 1506.18it/s]1702it [00:01, 1336.58it/s]1843it [00:01, 1259.66it/s]1973it [00:01, 1194.07it/s]2095it [00:01, 1179.42it/s]2215it [00:01, 1160.95it/s]2332it [00:01, 1120.88it/s]2445it [00:01, 1085.48it/s]2562it [00:01, 1106.67it/s]2684it [00:01, 1137.30it/s]2799it [00:02, 1135.30it/s]2913it [00:02, 1088.13it/s]3023it [00:02, 1065.11it/s]3130it [00:02, 1066.37it/s]3237it [00:02, 982.43it/s] 3355it [00:02, 1034.62it/s]3462it [00:02, 1044.12it/s]3568it [00:02, 1022.87it/s]3681it [00:02, 1052.40it/s]3787it [00:03, 991.31it/s] 3888it [00:03, 982.59it/s]3999it [00:03, 1017.42it/s]4113it [00:03, 1052.41it/s]4219it [00:03, 995.61it/s] 4326it [00:03, 1013.84it/s]4435it [00:03, 1033.54it/s]4539it [00:03, 1024.20it/s]4644it [00:03, 1028.41it/s]4764it [00:03, 1064.21it/s]4871it [00:04, 1060.07it/s]4978it [00:04, 1060.75it/s]5088it [00:04, 1067.65it/s]5195it [00:04, 651.90it/s] 5303it [00:04, 738.66it/s]5413it [00:04, 819.11it/s]5514it [00:04, 865.65it/s]5613it [00:05, 836.13it/s]5705it [00:05, 825.13it/s]5794it [00:05, 825.55it/s]5881it [00:05, 820.49it/s]5966it [00:05, 794.48it/s]6086it [00:05, 904.42it/s]6193it [00:05, 937.81it/s]6289it [00:05, 697.95it/s]6369it [00:05, 698.61it/s]6476it [00:06, 788.10it/s]6586it [00:06, 867.57it/s]6680it [00:06, 843.15it/s]6819it [00:06, 988.06it/s]6935it [00:06, 1033.44it/s]7043it [00:06, 957.84it/s] 7169it [00:06, 1037.82it/s]7282it [00:06, 1062.29it/s]7391it [00:06, 1054.48it/s]7499it [00:07, 1057.14it/s]7606it [00:07, 1046.78it/s]7712it [00:07, 1030.20it/s]7816it [00:07, 1028.62it/s]7920it [00:07, 1004.72it/s]8028it [00:07, 1025.16it/s]8138it [00:07, 1045.99it/s]8247it [00:07, 1056.28it/s]8353it [00:07, 1055.30it/s]8459it [00:07, 1030.11it/s]8579it [00:08, 1079.17it/s]8694it [00:08, 1099.51it/s]8805it [00:08, 1071.48it/s]8913it [00:08, 1043.42it/s]9020it [00:08, 1049.41it/s]9132it [00:08, 1069.79it/s]9255it [00:08, 1113.67it/s]9367it [00:08, 1088.05it/s]9477it [00:08, 1027.44it/s]9591it [00:09, 1056.65it/s]9698it [00:09, 1050.66it/s]9804it [00:09, 994.25it/s] 9905it [00:09, 996.70it/s]10023it [00:09, 1048.73it/s]10137it [00:09, 1073.08it/s]10248it [00:09, 1083.47it/s]10357it [00:09, 1084.49it/s]10468it [00:09, 1090.66it/s]10578it [00:09, 1073.26it/s]10686it [00:10, 1053.18it/s]10792it [00:10, 908.23it/s] 10903it [00:10, 960.42it/s]11006it [00:10, 978.03it/s]11132it [00:10, 1056.05it/s]11247it [00:10, 1080.19it/s]11357it [00:10, 1075.42it/s]11480it [00:10, 1118.45it/s]11593it [00:11, 946.24it/s] 11707it [00:11, 996.48it/s]11824it [00:11, 1042.76it/s]11938it [00:11, 1069.36it/s]12048it [00:11, 1029.48it/s]12161it [00:11, 1056.55it/s]12275it [00:11, 1080.35it/s]12393it [00:11, 1107.05it/s]12505it [00:11, 1042.67it/s]12611it [00:11, 1030.71it/s]12715it [00:12, 973.90it/s] 12838it [00:12, 1043.19it/s]12944it [00:12, 1045.30it/s]13053it [00:12, 1055.72it/s]13163it [00:12, 1068.23it/s]13271it [00:12, 1031.09it/s]13375it [00:12, 1014.11it/s]13477it [00:12, 952.09it/s] 13586it [00:12, 987.98it/s]13686it [00:13, 966.62it/s]13784it [00:13, 958.94it/s]13881it [00:13, 948.05it/s]13895it [00:13, 1048.55it/s]
0it [00:00, ?it/s]44it [00:00, 437.66it/s]88it [00:00, 429.04it/s]150it [00:00, 503.52it/s]193it [00:00, 399.31it/s]
0it [00:00, ?it/s]129it [00:00, 1288.25it/s]258it [00:00, 1003.33it/s]268it [00:00, 1052.27it/s]
Time usage: 27.55309534072876
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3.5e+01,  Train Acc: 56.25%,Val Loss:   4.6,  Val Acc: 45.08%, Val F1: 15.54% Time: 103.98594164848328 *
top-down:SEC: Iter:    100,  Train Loss: 3.5e+01,  Train Acc: 37.50%,Val Loss:   4.6,  Val Acc: 41.45%, Val F1: 10.55% Time: 103.98594164848328 *
top-down:CONN: Iter:    100,  Train Loss: 3.5e+01,  Train Acc:  9.38%,Val Loss:   4.6,  Val Acc: 100.00%, Val F1: 100.00% Time: 103.98594164848328 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.4e+01,  Train Acc: 53.12%,Val Loss:   3.7,  Val Acc: 45.08%, Val F1: 15.54% Time: 195.82488441467285 
top-down:SEC: Iter:    200,  Train Loss: 3.4e+01,  Train Acc: 21.88%,Val Loss:   3.7,  Val Acc: 35.75%, Val F1: 11.86% Time: 195.82488441467285 
top-down:CONN: Iter:    200,  Train Loss: 3.4e+01,  Train Acc: 12.50%,Val Loss:   3.7,  Val Acc: 100.00%, Val F1: 100.00% Time: 195.82488441467285 
 
 
top-down:TOP: Iter:    300,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   3.0,  Val Acc: 45.08%, Val F1: 16.86% Time: 289.52526021003723 
top-down:SEC: Iter:    300,  Train Loss: 3.1e+01,  Train Acc: 31.25%,Val Loss:   3.0,  Val Acc: 40.93%, Val F1: 10.54% Time: 289.52526021003723 
top-down:CONN: Iter:    300,  Train Loss: 3.1e+01,  Train Acc: 25.00%,Val Loss:   3.0,  Val Acc: 98.96%, Val F1: 49.74% Time: 289.52526021003723 
 
 
top-down:TOP: Iter:    400,  Train Loss: 3.4e+01,  Train Acc: 59.38%,Val Loss:   2.8,  Val Acc: 48.19%, Val F1: 28.19% Time: 393.2178056240082 
top-down:SEC: Iter:    400,  Train Loss: 3.4e+01,  Train Acc: 37.50%,Val Loss:   2.8,  Val Acc: 41.97%, Val F1: 15.83% Time: 393.2178056240082 
top-down:CONN: Iter:    400,  Train Loss: 3.4e+01,  Train Acc: 34.38%,Val Loss:   2.8,  Val Acc: 99.48%, Val F1: 49.87% Time: 393.2178056240082 
 
 
Train time usage: 428.693190574646
Test time usage: 0.5846583843231201
TOP: Test Loss:   2.9,  Test Acc: 45.15%, Test F1: 25.81%
SEC: Test Loss:   2.9,  Test Acc: 38.81%, Test F1: 11.06%
CONN: Test Loss:   2.9,  Test Acc: 99.25%, Test F1: 49.81%
consistency_top_sec:  8.85%,  consistency_sec_conn: 10.01%, consistency_top_sec_conn:  8.85%
              precision    recall  f1-score   support

    Temporal     0.5000    0.2542    0.3371        59
 Contingency     0.0000    0.0000    0.0000        41
  Comparison     0.5000    0.0566    0.1017        53
   Expansion     0.4440    0.8957    0.5937       115

    accuracy                         0.4515       268
   macro avg     0.3610    0.3016    0.2581       268
weighted avg     0.3995    0.4515    0.3491       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6667    0.0339    0.0645        59
         Temporal.Synchrony     0.0833    0.0244    0.0377        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.3992    0.9439    0.5611       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.3881       268
                  macro avg     0.1915    0.1670    0.1106       268
               weighted avg     0.3189    0.3881    0.2440       268

Epoch [2/15]
top-down:TOP: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   2.7,  Val Acc: 46.63%, Val F1: 24.29% Time: 67.03712129592896 
top-down:SEC: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 40.62%,Val Loss:   2.7,  Val Acc: 37.31%, Val F1: 14.02% Time: 67.03712129592896 
top-down:CONN: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   2.7,  Val Acc: 98.45%, Val F1: 49.61% Time: 67.03712129592896 
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 40.62%,Val Loss:   2.5,  Val Acc: 47.15%, Val F1: 23.35% Time: 163.1567075252533 
top-down:SEC: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   2.5,  Val Acc: 43.01%, Val F1: 11.75% Time: 163.1567075252533 
top-down:CONN: Iter:    600,  Train Loss: 3.2e+01,  Train Acc: 21.88%,Val Loss:   2.5,  Val Acc: 99.48%, Val F1: 49.87% Time: 163.1567075252533 
 
 
top-down:TOP: Iter:    700,  Train Loss: 3.3e+01,  Train Acc: 62.50%,Val Loss:   2.6,  Val Acc: 46.11%, Val F1: 19.56% Time: 258.9211058616638 
top-down:SEC: Iter:    700,  Train Loss: 3.3e+01,  Train Acc: 50.00%,Val Loss:   2.6,  Val Acc: 43.01%, Val F1: 10.89% Time: 258.9211058616638 
top-down:CONN: Iter:    700,  Train Loss: 3.3e+01,  Train Acc: 18.75%,Val Loss:   2.6,  Val Acc: 98.96%, Val F1: 49.74% Time: 258.9211058616638 
 
 
top-down:TOP: Iter:    800,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   2.3,  Val Acc: 51.30%, Val F1: 36.87% Time: 369.01058650016785 
top-down:SEC: Iter:    800,  Train Loss: 3e+01,  Train Acc: 34.38%,Val Loss:   2.3,  Val Acc: 47.15%, Val F1: 20.15% Time: 369.01058650016785 
top-down:CONN: Iter:    800,  Train Loss: 3e+01,  Train Acc: 31.25%,Val Loss:   2.3,  Val Acc: 99.48%, Val F1: 49.87% Time: 369.01058650016785 
 
 
Train time usage: 447.47687339782715
Test time usage: 0.6311652660369873
TOP: Test Loss:   2.3,  Test Acc: 57.46%, Test F1: 46.67%
SEC: Test Loss:   2.3,  Test Acc: 51.87%, Test F1: 30.65%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 12.13%,  consistency_sec_conn: 13.38%, consistency_top_sec_conn: 12.13%
              precision    recall  f1-score   support

    Temporal     0.5529    0.7966    0.6528        59
 Contingency     0.1667    0.0244    0.0426        41
  Comparison     0.5814    0.4717    0.5208        53
   Expansion     0.6045    0.7043    0.6506       115

    accuracy                         0.5746       268
   macro avg     0.4764    0.4993    0.4667       268
weighted avg     0.5216    0.5746    0.5324       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6111    0.7458    0.6718        59
         Temporal.Synchrony     0.4118    0.1707    0.2414        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.4706    0.2500    0.3265        32
        Comparison.Contrast     0.5000    0.7477    0.5993       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5187       268
                  macro avg     0.3322    0.3190    0.3065       268
               weighted avg     0.4533    0.5187    0.4631       268

Epoch [3/15]
top-down:TOP: Iter:    900,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   2.3,  Val Acc: 52.33%, Val F1: 44.58% Time: 36.78499221801758 *
top-down:SEC: Iter:    900,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   2.3,  Val Acc: 48.19%, Val F1: 27.62% Time: 36.78499221801758 *
top-down:CONN: Iter:    900,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   2.3,  Val Acc: 99.48%, Val F1: 49.87% Time: 36.78499221801758 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 3.4e+01,  Train Acc: 68.75%,Val Loss:   2.2,  Val Acc: 54.92%, Val F1: 42.01% Time: 152.86858558654785 *
top-down:SEC: Iter:   1000,  Train Loss: 3.4e+01,  Train Acc: 43.75%,Val Loss:   2.2,  Val Acc: 49.22%, Val F1: 24.50% Time: 152.86858558654785 *
top-down:CONN: Iter:   1000,  Train Loss: 3.4e+01,  Train Acc: 31.25%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 152.86858558654785 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 3.3e+01,  Train Acc: 59.38%,Val Loss:   2.2,  Val Acc: 54.40%, Val F1: 44.32% Time: 263.4124913215637 
top-down:SEC: Iter:   1100,  Train Loss: 3.3e+01,  Train Acc: 56.25%,Val Loss:   2.2,  Val Acc: 51.30%, Val F1: 28.19% Time: 263.4124913215637 
top-down:CONN: Iter:   1100,  Train Loss: 3.3e+01,  Train Acc: 37.50%,Val Loss:   2.2,  Val Acc: 99.48%, Val F1: 49.87% Time: 263.4124913215637 
 
 
top-down:TOP: Iter:   1200,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   2.2,  Val Acc: 52.33%, Val F1: 41.02% Time: 371.3250160217285 
top-down:SEC: Iter:   1200,  Train Loss: 3.2e+01,  Train Acc: 43.75%,Val Loss:   2.2,  Val Acc: 47.15%, Val F1: 28.55% Time: 371.3250160217285 
top-down:CONN: Iter:   1200,  Train Loss: 3.2e+01,  Train Acc: 21.88%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 371.3250160217285 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 3e+01,  Train Acc: 56.25%,Val Loss:   2.1,  Val Acc: 60.10%, Val F1: 49.73% Time: 482.324693441391 *
top-down:SEC: Iter:   1300,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   2.1,  Val Acc: 52.33%, Val F1: 27.60% Time: 482.324693441391 *
top-down:CONN: Iter:   1300,  Train Loss: 3e+01,  Train Acc: 28.12%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 482.324693441391 *
 
 
Train time usage: 489.94366908073425
Test time usage: 0.5581872463226318
TOP: Test Loss:   2.0,  Test Acc: 63.06%, Test F1: 56.71%
SEC: Test Loss:   2.0,  Test Acc: 57.46%, Test F1: 35.49%
CONN: Test Loss:   2.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.67%,  consistency_sec_conn: 14.82%, consistency_top_sec_conn: 13.67%
              precision    recall  f1-score   support

    Temporal     0.6456    0.8644    0.7391        59
 Contingency     0.4444    0.1951    0.2712        41
  Comparison     0.6512    0.5283    0.5833        53
   Expansion     0.6406    0.7130    0.6749       115

    accuracy                         0.6306       268
   macro avg     0.5955    0.5752    0.5671       268
weighted avg     0.6138    0.6306    0.6092       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6712    0.8305    0.7424        59
         Temporal.Synchrony     0.4667    0.1707    0.2500        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.6190    0.4062    0.4906        32
        Comparison.Contrast     0.5449    0.7944    0.6464       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5746       268
                  macro avg     0.3836    0.3670    0.3549       268
               weighted avg     0.5106    0.5746    0.5183       268

Epoch [4/15]
top-down:TOP: Iter:   1400,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   2.1,  Val Acc: 54.40%, Val F1: 50.09% Time: 102.43913984298706 
top-down:SEC: Iter:   1400,  Train Loss: 3.1e+01,  Train Acc: 50.00%,Val Loss:   2.1,  Val Acc: 52.33%, Val F1: 31.50% Time: 102.43913984298706 
top-down:CONN: Iter:   1400,  Train Loss: 3.1e+01,  Train Acc: 25.00%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 102.43913984298706 
 
 
top-down:TOP: Iter:   1500,  Train Loss: 3.2e+01,  Train Acc: 56.25%,Val Loss:   2.1,  Val Acc: 60.10%, Val F1: 53.74% Time: 211.97884273529053 *
top-down:SEC: Iter:   1500,  Train Loss: 3.2e+01,  Train Acc: 43.75%,Val Loss:   2.1,  Val Acc: 53.37%, Val F1: 31.78% Time: 211.97884273529053 *
top-down:CONN: Iter:   1500,  Train Loss: 3.2e+01,  Train Acc: 34.38%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 211.97884273529053 *
 
 
top-down:TOP: Iter:   1600,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 61.14%, Val F1: 52.27% Time: 320.3753442764282 *
top-down:SEC: Iter:   1600,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 55.96%, Val F1: 33.98% Time: 320.3753442764282 *
top-down:CONN: Iter:   1600,  Train Loss: 2.9e+01,  Train Acc: 40.62%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 320.3753442764282 *
 
 
top-down:TOP: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   2.0,  Val Acc: 58.55%, Val F1: 53.65% Time: 427.2559030056 *
top-down:SEC: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 54.92%, Val F1: 36.74% Time: 427.2559030056 *
top-down:CONN: Iter:   1700,  Train Loss: 3e+01,  Train Acc: 46.88%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 427.2559030056 *
 
 
Train time usage: 474.66426706314087
Test time usage: 0.5514340400695801
TOP: Test Loss:   2.0,  Test Acc: 65.67%, Test F1: 59.71%
SEC: Test Loss:   2.0,  Test Acc: 60.82%, Test F1: 42.32%
CONN: Test Loss:   2.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.21%,  consistency_sec_conn: 15.69%, consistency_top_sec_conn: 15.21%
              precision    recall  f1-score   support

    Temporal     0.6220    0.8644    0.7234        59
 Contingency     0.5238    0.2683    0.3548        41
  Comparison     0.8065    0.4717    0.5952        53
   Expansion     0.6642    0.7739    0.7149       115

    accuracy                         0.6567       268
   macro avg     0.6541    0.5946    0.5971       268
weighted avg     0.6615    0.6567    0.6380       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6329    0.8475    0.7246        59
         Temporal.Synchrony     0.5238    0.2683    0.3548        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.6154    0.5000    0.5517        32
        Comparison.Contrast     0.6028    0.7944    0.6855       107
      Comparison.Concession     1.0000    0.1250    0.2222         8

                   accuracy                         0.6082       268
                  macro avg     0.5625    0.4225    0.4232       268
               weighted avg     0.5635    0.6082    0.5600       268

Epoch [5/15]
top-down:TOP: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   2.0,  Val Acc: 59.07%, Val F1: 54.39% Time: 66.12148070335388 
top-down:SEC: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 54.92%, Val F1: 35.39% Time: 66.12148070335388 
top-down:CONN: Iter:   1800,  Train Loss: 2.7e+01,  Train Acc: 34.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 66.12148070335388 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   1.9,  Val Acc: 62.18%, Val F1: 57.45% Time: 182.04332876205444 *
top-down:SEC: Iter:   1900,  Train Loss: 3.2e+01,  Train Acc: 65.62%,Val Loss:   1.9,  Val Acc: 55.44%, Val F1: 36.66% Time: 182.04332876205444 *
top-down:CONN: Iter:   1900,  Train Loss: 3.2e+01,  Train Acc: 53.12%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 182.04332876205444 *
 
 
top-down:TOP: Iter:   2000,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   2.0,  Val Acc: 58.55%, Val F1: 54.31% Time: 289.12078976631165 
top-down:SEC: Iter:   2000,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   2.0,  Val Acc: 54.40%, Val F1: 38.77% Time: 289.12078976631165 
top-down:CONN: Iter:   2000,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 289.12078976631165 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 3.6e+01,  Train Acc: 81.25%,Val Loss:   2.0,  Val Acc: 58.55%, Val F1: 54.86% Time: 399.40957283973694 
top-down:SEC: Iter:   2100,  Train Loss: 3.6e+01,  Train Acc: 50.00%,Val Loss:   2.0,  Val Acc: 54.40%, Val F1: 41.62% Time: 399.40957283973694 
top-down:CONN: Iter:   2100,  Train Loss: 3.6e+01,  Train Acc: 43.75%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 399.40957283973694 
 
 
Train time usage: 482.7500846385956
Test time usage: 0.5179295539855957
TOP: Test Loss:   1.9,  Test Acc: 66.79%, Test F1: 64.09%
SEC: Test Loss:   1.9,  Test Acc: 61.94%, Test F1: 48.77%
CONN: Test Loss:   1.9,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.21%,  consistency_sec_conn: 15.98%, consistency_top_sec_conn: 15.21%
              precision    recall  f1-score   support

    Temporal     0.7424    0.8305    0.7840        59
 Contingency     0.5000    0.4146    0.4533        41
  Comparison     0.6809    0.6038    0.6400        53
   Expansion     0.6694    0.7043    0.6864       115

    accuracy                         0.6679       268
   macro avg     0.6482    0.6383    0.6409       268
weighted avg     0.6618    0.6679    0.6631       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7541    0.7797    0.7667        59
         Temporal.Synchrony     0.5128    0.4878    0.5000        41
          Contingency.Cause     0.2500    0.1429    0.1818        21
Contingency.Pragmatic cause     0.6538    0.5312    0.5862        32
        Comparison.Contrast     0.6124    0.7383    0.6695       107
      Comparison.Concession     1.0000    0.1250    0.2222         8

                   accuracy                         0.6194       268
                  macro avg     0.6305    0.4675    0.4877       268
               weighted avg     0.6165    0.6194    0.6034       268

Epoch [6/15]
top-down:TOP: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   1.9,  Val Acc: 61.66%, Val F1: 57.41% Time: 26.27087664604187 
top-down:SEC: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   1.9,  Val Acc: 55.96%, Val F1: 34.83% Time: 26.27087664604187 
top-down:CONN: Iter:   2200,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 26.27087664604187 
 
 
top-down:TOP: Iter:   2300,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   2.0,  Val Acc: 61.66%, Val F1: 55.63% Time: 139.77902150154114 *
top-down:SEC: Iter:   2300,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   2.0,  Val Acc: 58.03%, Val F1: 45.91% Time: 139.77902150154114 *
top-down:CONN: Iter:   2300,  Train Loss: 2.9e+01,  Train Acc: 34.38%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 139.77902150154114 *
 
 
top-down:TOP: Iter:   2400,  Train Loss: 2.5e+01,  Train Acc: 71.88%,Val Loss:   1.9,  Val Acc: 63.21%, Val F1: 58.64% Time: 248.05008149147034 *
top-down:SEC: Iter:   2400,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   1.9,  Val Acc: 56.99%, Val F1: 42.84% Time: 248.05008149147034 *
top-down:CONN: Iter:   2400,  Train Loss: 2.5e+01,  Train Acc: 46.88%,Val Loss:   1.9,  Val Acc: 100.00%, Val F1: 100.00% Time: 248.05008149147034 *
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   2.0,  Val Acc: 60.62%, Val F1: 55.73% Time: 359.4732403755188 
top-down:SEC: Iter:   2500,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   2.0,  Val Acc: 56.48%, Val F1: 37.50% Time: 359.4732403755188 
top-down:CONN: Iter:   2500,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 359.4732403755188 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 3e+01,  Train Acc: 68.75%,Val Loss:   2.0,  Val Acc: 61.14%, Val F1: 57.38% Time: 460.86016511917114 
top-down:SEC: Iter:   2600,  Train Loss: 3e+01,  Train Acc: 62.50%,Val Loss:   2.0,  Val Acc: 56.48%, Val F1: 42.59% Time: 460.86016511917114 
top-down:CONN: Iter:   2600,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 460.86016511917114 
 
 
Train time usage: 472.59127974510193
Test time usage: 0.5305187702178955
TOP: Test Loss:   1.9,  Test Acc: 67.16%, Test F1: 64.04%
SEC: Test Loss:   1.9,  Test Acc: 61.57%, Test F1: 45.40%
CONN: Test Loss:   1.9,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.69%,  consistency_sec_conn: 15.88%, consistency_top_sec_conn: 15.69%
              precision    recall  f1-score   support

    Temporal     0.7500    0.8136    0.7805        59
 Contingency     0.4722    0.4146    0.4416        41
  Comparison     0.7632    0.5472    0.6374        53
   Expansion     0.6615    0.7478    0.7020       115

    accuracy                         0.6716       268
   macro avg     0.6617    0.6308    0.6404       268
weighted avg     0.6721    0.6716    0.6667       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7581    0.7966    0.7769        59
         Temporal.Synchrony     0.4571    0.3902    0.4211        41
          Contingency.Cause     0.3333    0.0476    0.0833        21
Contingency.Pragmatic cause     0.5625    0.5625    0.5625        32
        Comparison.Contrast     0.6119    0.7664    0.6805       107
      Comparison.Concession     0.5000    0.1250    0.2000         8

                   accuracy                         0.6157       268
                  macro avg     0.5372    0.4481    0.4540       268
               weighted avg     0.5894    0.6157    0.5868       268

Epoch [7/15]
top-down:TOP: Iter:   2700,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   2.0,  Val Acc: 59.59%, Val F1: 55.74% Time: 95.96051478385925 
top-down:SEC: Iter:   2700,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   2.0,  Val Acc: 56.99%, Val F1: 46.66% Time: 95.96051478385925 
top-down:CONN: Iter:   2700,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 95.96051478385925 
 
 
top-down:TOP: Iter:   2800,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   2.0,  Val Acc: 64.25%, Val F1: 60.38% Time: 198.07226037979126 *
top-down:SEC: Iter:   2800,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   2.0,  Val Acc: 58.55%, Val F1: 44.54% Time: 198.07226037979126 *
top-down:CONN: Iter:   2800,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 198.07226037979126 *
 
 
top-down:TOP: Iter:   2900,  Train Loss: 3.3e+01,  Train Acc: 78.12%,Val Loss:   2.1,  Val Acc: 61.14%, Val F1: 54.93% Time: 303.08581495285034 
top-down:SEC: Iter:   2900,  Train Loss: 3.3e+01,  Train Acc: 68.75%,Val Loss:   2.1,  Val Acc: 55.44%, Val F1: 40.44% Time: 303.08581495285034 
top-down:CONN: Iter:   2900,  Train Loss: 3.3e+01,  Train Acc: 28.12%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 303.08581495285034 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 84.38%,Val Loss:   2.0,  Val Acc: 60.62%, Val F1: 56.43% Time: 404.82952785491943 
top-down:SEC: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 75.00%,Val Loss:   2.0,  Val Acc: 57.51%, Val F1: 47.60% Time: 404.82952785491943 
top-down:CONN: Iter:   3000,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 404.82952785491943 
 
 
Train time usage: 453.72767186164856
Test time usage: 0.5970327854156494
TOP: Test Loss:   2.0,  Test Acc: 65.67%, Test F1: 62.03%
SEC: Test Loss:   2.0,  Test Acc: 61.19%, Test F1: 45.09%
CONN: Test Loss:   2.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.50%,  consistency_sec_conn: 15.78%, consistency_top_sec_conn: 15.50%
              precision    recall  f1-score   support

    Temporal     0.7460    0.7966    0.7705        59
 Contingency     0.4848    0.3902    0.4324        41
  Comparison     0.7812    0.4717    0.5882        53
   Expansion     0.6286    0.7652    0.6902       115

    accuracy                         0.6567       268
   macro avg     0.6602    0.6059    0.6203       268
weighted avg     0.6626    0.6567    0.6483       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7627    0.7627    0.7627        59
         Temporal.Synchrony     0.4839    0.3659    0.4167        41
          Contingency.Cause     0.2500    0.0476    0.0800        21
Contingency.Pragmatic cause     0.6667    0.5000    0.5714        32
        Comparison.Contrast     0.5811    0.8037    0.6745       107
      Comparison.Concession     0.5000    0.1250    0.2000         8

                   accuracy                         0.6119       268
                  macro avg     0.5407    0.4342    0.4509       268
               weighted avg     0.5881    0.6119    0.5814       268

Epoch [8/15]
top-down:TOP: Iter:   3100,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   2.0,  Val Acc: 59.59%, Val F1: 56.01% Time: 55.995728969573975 
top-down:SEC: Iter:   3100,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   2.0,  Val Acc: 57.51%, Val F1: 44.29% Time: 55.995728969573975 
top-down:CONN: Iter:   3100,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 55.995728969573975 
 
 
top-down:TOP: Iter:   3200,  Train Loss: 2.9e+01,  Train Acc: 81.25%,Val Loss:   2.1,  Val Acc: 60.62%, Val F1: 54.94% Time: 162.93556904792786 
top-down:SEC: Iter:   3200,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   2.1,  Val Acc: 54.40%, Val F1: 35.31% Time: 162.93556904792786 
top-down:CONN: Iter:   3200,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 162.93556904792786 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   2.0,  Val Acc: 61.66%, Val F1: 57.90% Time: 269.35424995422363 
top-down:SEC: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   2.0,  Val Acc: 55.44%, Val F1: 37.74% Time: 269.35424995422363 
top-down:CONN: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 269.35424995422363 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   2.0,  Val Acc: 61.14%, Val F1: 55.85% Time: 374.34494805336 
top-down:SEC: Iter:   3400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   2.0,  Val Acc: 53.89%, Val F1: 35.29% Time: 374.34494805336 
top-down:CONN: Iter:   3400,  Train Loss: 2.8e+01,  Train Acc: 37.50%,Val Loss:   2.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 374.34494805336 
 
 
Train time usage: 459.54660987854004
Test time usage: 0.5387575626373291
TOP: Test Loss:   2.1,  Test Acc: 63.81%, Test F1: 60.66%
SEC: Test Loss:   2.1,  Test Acc: 59.70%, Test F1: 48.46%
CONN: Test Loss:   2.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.73%,  consistency_sec_conn: 15.40%, consistency_top_sec_conn: 14.73%
              precision    recall  f1-score   support

    Temporal     0.7581    0.7966    0.7769        59
 Contingency     0.4375    0.3415    0.3836        41
  Comparison     0.6327    0.5849    0.6078        53
   Expansion     0.6320    0.6870    0.6583       115

    accuracy                         0.6381       268
   macro avg     0.6151    0.6025    0.6066       268
weighted avg     0.6301    0.6381    0.6324       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7705    0.7966    0.7833        59
         Temporal.Synchrony     0.4571    0.3902    0.4211        41
          Contingency.Cause     0.2308    0.1429    0.1765        21
Contingency.Pragmatic cause     0.5161    0.5000    0.5079        32
        Comparison.Contrast     0.6080    0.7103    0.6552       107
      Comparison.Concession     0.6667    0.2500    0.3636         8

                   accuracy                         0.5970       268
                  macro avg     0.5415    0.4650    0.4846       268
               weighted avg     0.5819    0.5970    0.5838       268

Epoch [9/15]
top-down:TOP: Iter:   3500,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   2.1,  Val Acc: 61.14%, Val F1: 56.94% Time: 23.98584532737732 
top-down:SEC: Iter:   3500,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   2.1,  Val Acc: 58.03%, Val F1: 48.74% Time: 23.98584532737732 
top-down:CONN: Iter:   3500,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 23.98584532737732 
 
 
top-down:TOP: Iter:   3600,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   2.1,  Val Acc: 61.66%, Val F1: 56.55% Time: 128.23909187316895 
top-down:SEC: Iter:   3600,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   2.1,  Val Acc: 54.40%, Val F1: 33.50% Time: 128.23909187316895 
top-down:CONN: Iter:   3600,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 128.23909187316895 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   2.1,  Val Acc: 62.18%, Val F1: 56.20% Time: 234.3860902786255 
top-down:SEC: Iter:   3700,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   2.1,  Val Acc: 55.96%, Val F1: 43.46% Time: 234.3860902786255 
top-down:CONN: Iter:   3700,  Train Loss: 3.1e+01,  Train Acc: 46.88%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 234.3860902786255 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   2.1,  Val Acc: 61.66%, Val F1: 56.13% Time: 337.2915463447571 
top-down:SEC: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   2.1,  Val Acc: 52.85%, Val F1: 34.22% Time: 337.2915463447571 
top-down:CONN: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 337.2915463447571 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   2.1,  Val Acc: 61.14%, Val F1: 56.26% Time: 439.5508964061737 
top-down:SEC: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   2.1,  Val Acc: 57.51%, Val F1: 43.41% Time: 439.5508964061737 
top-down:CONN: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 37.50%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 439.5508964061737 
 
 
Train time usage: 458.57979369163513
Test time usage: 0.6763279438018799
TOP: Test Loss:   2.1,  Test Acc: 65.67%, Test F1: 62.44%
SEC: Test Loss:   2.1,  Test Acc: 58.58%, Test F1: 43.39%
CONN: Test Loss:   2.1,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.53%,  consistency_sec_conn: 15.11%, consistency_top_sec_conn: 14.53%
              precision    recall  f1-score   support

    Temporal     0.7586    0.7458    0.7521        59
 Contingency     0.5333    0.3902    0.4507        41
  Comparison     0.6744    0.5472    0.6042        53
   Expansion     0.6350    0.7565    0.6905       115

    accuracy                         0.6567       268
   macro avg     0.6504    0.6099    0.6244       268
weighted avg     0.6545    0.6567    0.6503       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7407    0.6780    0.7080        59
         Temporal.Synchrony     0.5333    0.3902    0.4507        41
          Contingency.Cause     0.1250    0.0476    0.0690        21
Contingency.Pragmatic cause     0.5769    0.4688    0.5172        32
        Comparison.Contrast     0.5676    0.7850    0.6588       107
      Comparison.Concession     0.5000    0.1250    0.2000         8

                   accuracy                         0.5858       268
                  macro avg     0.5073    0.4158    0.4339       268
               weighted avg     0.5649    0.5858    0.5610       268

Epoch [10/15]
top-down:TOP: Iter:   4000,  Train Loss: 2.5e+01,  Train Acc: 71.88%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 56.34% Time: 90.24970769882202 
top-down:SEC: Iter:   4000,  Train Loss: 2.5e+01,  Train Acc: 65.62%,Val Loss:   2.2,  Val Acc: 56.48%, Val F1: 45.14% Time: 90.24970769882202 
top-down:CONN: Iter:   4000,  Train Loss: 2.5e+01,  Train Acc: 53.12%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 90.24970769882202 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   2.1,  Val Acc: 62.69%, Val F1: 56.70% Time: 195.22322845458984 
top-down:SEC: Iter:   4100,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   2.1,  Val Acc: 59.07%, Val F1: 47.12% Time: 195.22322845458984 
top-down:CONN: Iter:   4100,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 195.22322845458984 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   2.1,  Val Acc: 63.21%, Val F1: 57.36% Time: 300.81261229515076 
top-down:SEC: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   2.1,  Val Acc: 56.48%, Val F1: 37.18% Time: 300.81261229515076 
top-down:CONN: Iter:   4200,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 300.81261229515076 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   2.1,  Val Acc: 60.62%, Val F1: 57.41% Time: 404.3045129776001 
top-down:SEC: Iter:   4300,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   2.1,  Val Acc: 56.48%, Val F1: 43.28% Time: 404.3045129776001 
top-down:CONN: Iter:   4300,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 404.3045129776001 
 
 
Train time usage: 456.2478594779968
Test time usage: 0.568427562713623
TOP: Test Loss:   2.2,  Test Acc: 65.67%, Test F1: 61.92%
SEC: Test Loss:   2.2,  Test Acc: 61.19%, Test F1: 48.26%
CONN: Test Loss:   2.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.30%,  consistency_sec_conn: 15.78%, consistency_top_sec_conn: 15.30%
              precision    recall  f1-score   support

    Temporal     0.7963    0.7288    0.7611        59
 Contingency     0.4828    0.3415    0.4000        41
  Comparison     0.6596    0.5849    0.6200        53
   Expansion     0.6377    0.7652    0.6957       115

    accuracy                         0.6567       268
   macro avg     0.6441    0.6051    0.6192       268
weighted avg     0.6532    0.6567    0.6499       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7963    0.7288    0.7611        59
         Temporal.Synchrony     0.5000    0.3659    0.4225        41
          Contingency.Cause     0.2222    0.0952    0.1333        21
Contingency.Pragmatic cause     0.5625    0.5625    0.5625        32
        Comparison.Contrast     0.6043    0.7850    0.6829       107
      Comparison.Concession     0.5000    0.2500    0.3333         8

                   accuracy                         0.6119       268
                  macro avg     0.5309    0.4646    0.4826       268
               weighted avg     0.5926    0.6119    0.5924       268

Epoch [11/15]
top-down:TOP: Iter:   4400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   2.3,  Val Acc: 60.62%, Val F1: 53.99% Time: 55.75771522521973 
top-down:SEC: Iter:   4400,  Train Loss: 2.8e+01,  Train Acc: 65.62%,Val Loss:   2.3,  Val Acc: 53.89%, Val F1: 38.53% Time: 55.75771522521973 
top-down:CONN: Iter:   4400,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   2.3,  Val Acc: 100.00%, Val F1: 100.00% Time: 55.75771522521973 
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   2.1,  Val Acc: 63.21%, Val F1: 58.67% Time: 155.16464042663574 
top-down:SEC: Iter:   4500,  Train Loss: 2.6e+01,  Train Acc: 81.25%,Val Loss:   2.1,  Val Acc: 57.51%, Val F1: 43.53% Time: 155.16464042663574 
top-down:CONN: Iter:   4500,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 155.16464042663574 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   2.1,  Val Acc: 62.69%, Val F1: 58.34% Time: 261.54078340530396 
top-down:SEC: Iter:   4600,  Train Loss: 3e+01,  Train Acc: 78.12%,Val Loss:   2.1,  Val Acc: 55.96%, Val F1: 43.78% Time: 261.54078340530396 
top-down:CONN: Iter:   4600,  Train Loss: 3e+01,  Train Acc: 53.12%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 261.54078340530396 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 55.66% Time: 366.11904096603394 
top-down:SEC: Iter:   4700,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   2.2,  Val Acc: 56.99%, Val F1: 44.59% Time: 366.11904096603394 
top-down:CONN: Iter:   4700,  Train Loss: 2.6e+01,  Train Acc: 46.88%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 366.11904096603394 
 
 
Train time usage: 457.1652054786682
Test time usage: 0.599571943283081
TOP: Test Loss:   2.2,  Test Acc: 64.18%, Test F1: 62.54%
SEC: Test Loss:   2.2,  Test Acc: 59.70%, Test F1: 49.03%
CONN: Test Loss:   2.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.01%,  consistency_sec_conn: 15.40%, consistency_top_sec_conn: 15.01%
              precision    recall  f1-score   support

    Temporal     0.7627    0.7627    0.7627        59
 Contingency     0.4375    0.5122    0.4719        41
  Comparison     0.6522    0.5660    0.6061        53
   Expansion     0.6609    0.6609    0.6609       115

    accuracy                         0.6418       268
   macro avg     0.6283    0.6255    0.6254       268
weighted avg     0.6474    0.6418    0.6435       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7500    0.7627    0.7563        59
         Temporal.Synchrony     0.4375    0.5122    0.4719        41
          Contingency.Cause     0.2308    0.1429    0.1765        21
Contingency.Pragmatic cause     0.6154    0.5000    0.5517        32
        Comparison.Contrast     0.6239    0.6822    0.6518       107
      Comparison.Concession     0.5000    0.2500    0.3333         8

                   accuracy                         0.5970       268
                  macro avg     0.5263    0.4750    0.4903       268
               weighted avg     0.5876    0.5970    0.5886       268

Epoch [12/15]
top-down:TOP: Iter:   4800,  Train Loss: 2.8e+01,  Train Acc: 96.88%,Val Loss:   2.1,  Val Acc: 63.21%, Val F1: 59.31% Time: 19.793071269989014 *
top-down:SEC: Iter:   4800,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   2.1,  Val Acc: 59.59%, Val F1: 50.72% Time: 19.793071269989014 *
top-down:CONN: Iter:   4800,  Train Loss: 2.8e+01,  Train Acc: 59.38%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 19.793071269989014 *
 
 
top-down:TOP: Iter:   4900,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   2.1,  Val Acc: 61.66%, Val F1: 57.87% Time: 126.26335406303406 
top-down:SEC: Iter:   4900,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   2.1,  Val Acc: 56.99%, Val F1: 49.71% Time: 126.26335406303406 
top-down:CONN: Iter:   4900,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 126.26335406303406 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   2.2,  Val Acc: 62.69%, Val F1: 57.00% Time: 231.10029768943787 
top-down:SEC: Iter:   5000,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   2.2,  Val Acc: 56.48%, Val F1: 45.71% Time: 231.10029768943787 
top-down:CONN: Iter:   5000,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 231.10029768943787 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   2.1,  Val Acc: 64.25%, Val F1: 58.88% Time: 333.48789644241333 
top-down:SEC: Iter:   5100,  Train Loss: 2.5e+01,  Train Acc: 68.75%,Val Loss:   2.1,  Val Acc: 59.59%, Val F1: 46.98% Time: 333.48789644241333 
top-down:CONN: Iter:   5100,  Train Loss: 2.5e+01,  Train Acc: 56.25%,Val Loss:   2.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 333.48789644241333 
 
 
top-down:TOP: Iter:   5200,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   2.2,  Val Acc: 62.69%, Val F1: 58.68% Time: 435.71728467941284 
top-down:SEC: Iter:   5200,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   2.2,  Val Acc: 58.03%, Val F1: 46.76% Time: 435.71728467941284 
top-down:CONN: Iter:   5200,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 435.71728467941284 
 
 
Train time usage: 458.56496834754944
Test time usage: 0.5330495834350586
TOP: Test Loss:   2.2,  Test Acc: 64.55%, Test F1: 62.64%
SEC: Test Loss:   2.2,  Test Acc: 60.45%, Test F1: 48.50%
CONN: Test Loss:   2.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.01%,  consistency_sec_conn: 15.59%, consistency_top_sec_conn: 15.01%
              precision    recall  f1-score   support

    Temporal     0.7857    0.7458    0.7652        59
 Contingency     0.4524    0.4634    0.4578        41
  Comparison     0.6818    0.5660    0.6186        53
   Expansion     0.6349    0.6957    0.6639       115

    accuracy                         0.6455       268
   macro avg     0.6387    0.6177    0.6264       268
weighted avg     0.6495    0.6455    0.6457       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7818    0.7288    0.7544        59
         Temporal.Synchrony     0.4750    0.4634    0.4691        41
          Contingency.Cause     0.2000    0.0952    0.1290        21
Contingency.Pragmatic cause     0.5862    0.5312    0.5574        32
        Comparison.Contrast     0.6077    0.7383    0.6667       107
      Comparison.Concession     0.5000    0.2500    0.3333         8

                   accuracy                         0.6045       268
                  macro avg     0.5251    0.4678    0.4850       268
               weighted avg     0.5880    0.6045    0.5906       268

Epoch [13/15]
top-down:TOP: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   2.2,  Val Acc: 62.69%, Val F1: 56.94% Time: 79.98510718345642 
top-down:SEC: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 68.75%,Val Loss:   2.2,  Val Acc: 57.51%, Val F1: 41.72% Time: 79.98510718345642 
top-down:CONN: Iter:   5300,  Train Loss: 2.7e+01,  Train Acc: 40.62%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 79.98510718345642 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   2.2,  Val Acc: 62.69%, Val F1: 58.65% Time: 179.48201394081116 
top-down:SEC: Iter:   5400,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   2.2,  Val Acc: 56.99%, Val F1: 46.03% Time: 179.48201394081116 
top-down:CONN: Iter:   5400,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 179.48201394081116 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 57.13% Time: 282.513720035553 
top-down:SEC: Iter:   5500,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   2.2,  Val Acc: 57.51%, Val F1: 48.98% Time: 282.513720035553 
top-down:CONN: Iter:   5500,  Train Loss: 2.6e+01,  Train Acc: 53.12%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 282.513720035553 
 
 
top-down:TOP: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   2.2,  Val Acc: 63.73%, Val F1: 59.69% Time: 387.5833411216736 
top-down:SEC: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   2.2,  Val Acc: 59.59%, Val F1: 48.97% Time: 387.5833411216736 
top-down:CONN: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 387.5833411216736 
 
 
Train time usage: 448.78068709373474
Test time usage: 0.5226545333862305
TOP: Test Loss:   2.3,  Test Acc: 65.30%, Test F1: 62.34%
SEC: Test Loss:   2.3,  Test Acc: 61.19%, Test F1: 46.75%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.30%,  consistency_sec_conn: 15.78%, consistency_top_sec_conn: 15.30%
              precision    recall  f1-score   support

    Temporal     0.7377    0.7627    0.7500        59
 Contingency     0.4857    0.4146    0.4474        41
  Comparison     0.6905    0.5472    0.6105        53
   Expansion     0.6462    0.7304    0.6857       115

    accuracy                         0.6530       268
   macro avg     0.6400    0.6137    0.6234       268
weighted avg     0.6505    0.6530    0.6485       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7500    0.7627    0.7563        59
         Temporal.Synchrony     0.5143    0.4390    0.4737        41
          Contingency.Cause     0.2222    0.0952    0.1333        21
Contingency.Pragmatic cause     0.6207    0.5625    0.5902        32
        Comparison.Contrast     0.6061    0.7477    0.6695       107
      Comparison.Concession     0.3333    0.1250    0.1818         8

                   accuracy                         0.6119       268
                  macro avg     0.5078    0.4554    0.4675       268
               weighted avg     0.5872    0.6119    0.5926       268

Epoch [14/15]
top-down:TOP: Iter:   5700,  Train Loss: 2.3e+01,  Train Acc: 87.50%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 58.38% Time: 47.22662663459778 
top-down:SEC: Iter:   5700,  Train Loss: 2.3e+01,  Train Acc: 75.00%,Val Loss:   2.2,  Val Acc: 57.51%, Val F1: 47.05% Time: 47.22662663459778 
top-down:CONN: Iter:   5700,  Train Loss: 2.3e+01,  Train Acc: 40.62%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 47.22662663459778 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 87.50%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 57.07% Time: 149.80814218521118 
top-down:SEC: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 68.75%,Val Loss:   2.2,  Val Acc: 58.03%, Val F1: 46.25% Time: 149.80814218521118 
top-down:CONN: Iter:   5800,  Train Loss: 2.3e+01,  Train Acc: 62.50%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 149.80814218521118 
 
 
top-down:TOP: Iter:   5900,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   2.2,  Val Acc: 62.18%, Val F1: 58.08% Time: 254.92123222351074 
top-down:SEC: Iter:   5900,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   2.2,  Val Acc: 59.59%, Val F1: 50.39% Time: 254.92123222351074 
top-down:CONN: Iter:   5900,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 254.92123222351074 
 
 
top-down:TOP: Iter:   6000,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   2.2,  Val Acc: 62.69%, Val F1: 58.70% Time: 358.10199451446533 
top-down:SEC: Iter:   6000,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   2.2,  Val Acc: 58.03%, Val F1: 49.18% Time: 358.10199451446533 
top-down:CONN: Iter:   6000,  Train Loss: 2.7e+01,  Train Acc: 62.50%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 358.10199451446533 
 
 
Train time usage: 449.6650080680847
Test time usage: 0.5521829128265381
TOP: Test Loss:   2.3,  Test Acc: 64.18%, Test F1: 61.27%
SEC: Test Loss:   2.3,  Test Acc: 60.45%, Test F1: 48.07%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.92%,  consistency_sec_conn: 15.59%, consistency_top_sec_conn: 14.92%
              precision    recall  f1-score   support

    Temporal     0.7458    0.7458    0.7458        59
 Contingency     0.4706    0.3902    0.4267        41
  Comparison     0.6522    0.5660    0.6061        53
   Expansion     0.6357    0.7130    0.6721       115

    accuracy                         0.6418       268
   macro avg     0.6260    0.6038    0.6127       268
weighted avg     0.6379    0.6418    0.6377       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7544    0.7288    0.7414        59
         Temporal.Synchrony     0.5000    0.3902    0.4384        41
          Contingency.Cause     0.1818    0.0952    0.1250        21
Contingency.Pragmatic cause     0.5806    0.5625    0.5714        32
        Comparison.Contrast     0.6090    0.7570    0.6750       107
      Comparison.Concession     0.5000    0.2500    0.3333         8

                   accuracy                         0.6045       268
                  macro avg     0.5210    0.4640    0.4807       268
               weighted avg     0.5842    0.6045    0.5877       268

Epoch [15/15]
top-down:TOP: Iter:   6100,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   2.2,  Val Acc: 62.69%, Val F1: 57.62% Time: 11.010429382324219 
top-down:SEC: Iter:   6100,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   2.2,  Val Acc: 59.07%, Val F1: 49.22% Time: 11.010429382324219 
top-down:CONN: Iter:   6100,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 11.010429382324219 
 
 
top-down:TOP: Iter:   6200,  Train Loss: 3e+01,  Train Acc: 96.88%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 56.43% Time: 114.95014595985413 
top-down:SEC: Iter:   6200,  Train Loss: 3e+01,  Train Acc: 87.50%,Val Loss:   2.2,  Val Acc: 58.03%, Val F1: 48.42% Time: 114.95014595985413 
top-down:CONN: Iter:   6200,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 114.95014595985413 
 
 
top-down:TOP: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 56.99% Time: 219.39684009552002 
top-down:SEC: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   2.2,  Val Acc: 58.55%, Val F1: 46.75% Time: 219.39684009552002 
top-down:CONN: Iter:   6300,  Train Loss: 2.9e+01,  Train Acc: 50.00%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 219.39684009552002 
 
 
top-down:TOP: Iter:   6400,  Train Loss: 2.5e+01,  Train Acc: 84.38%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 56.93% Time: 327.6743016242981 
top-down:SEC: Iter:   6400,  Train Loss: 2.5e+01,  Train Acc: 75.00%,Val Loss:   2.2,  Val Acc: 58.03%, Val F1: 45.95% Time: 327.6743016242981 
top-down:CONN: Iter:   6400,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 327.6743016242981 
 
 
top-down:TOP: Iter:   6500,  Train Loss: 2.2e+01,  Train Acc: 100.00%,Val Loss:   2.2,  Val Acc: 61.66%, Val F1: 56.93% Time: 430.13642835617065 
top-down:SEC: Iter:   6500,  Train Loss: 2.2e+01,  Train Acc: 84.38%,Val Loss:   2.2,  Val Acc: 58.03%, Val F1: 45.95% Time: 430.13642835617065 
top-down:CONN: Iter:   6500,  Train Loss: 2.2e+01,  Train Acc: 50.00%,Val Loss:   2.2,  Val Acc: 100.00%, Val F1: 100.00% Time: 430.13642835617065 
 
 
Train time usage: 458.69627714157104
Test time usage: 0.5686078071594238
TOP: Test Loss:   2.3,  Test Acc: 65.30%, Test F1: 62.39%
SEC: Test Loss:   2.3,  Test Acc: 61.94%, Test F1: 50.19%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.30%,  consistency_sec_conn: 15.98%, consistency_top_sec_conn: 15.30%
              precision    recall  f1-score   support

    Temporal     0.7818    0.7288    0.7544        59
 Contingency     0.5000    0.3902    0.4384        41
  Comparison     0.6596    0.5849    0.6200        53
   Expansion     0.6343    0.7391    0.6827       115

    accuracy                         0.6530       268
   macro avg     0.6439    0.6108    0.6239       268
weighted avg     0.6512    0.6530    0.6487       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7818    0.7288    0.7544        59
         Temporal.Synchrony     0.5000    0.3659    0.4225        41
          Contingency.Cause     0.3077    0.1905    0.2353        21
Contingency.Pragmatic cause     0.5806    0.5625    0.5714        32
        Comparison.Contrast     0.6222    0.7850    0.6942       107
      Comparison.Concession     0.5000    0.2500    0.3333         8

                   accuracy                         0.6194       268
                  macro avg     0.5487    0.4804    0.5019       268
               weighted avg     0.6054    0.6194    0.6045       268

dev_best_acc_top: 63.21%,  dev_best_f1_top: 59.31%, 
dev_best_acc_sec: 59.59%,  dev_best_f1_sec: 50.72%, 
dev_best_acc_conn: 100.00%,  dev_best_f1_conn: 100.00%
