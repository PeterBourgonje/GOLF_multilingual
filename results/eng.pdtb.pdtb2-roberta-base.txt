nohup: ignoring input and appending output to 'nohup.out'
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]tokenizer_config.json: 100%|██████████| 25.0/25.0 [00:00<00:00, 107kB/s]
{'cuda': 0, 'seed': 0, 'data_file': 'data/PDTB/Ji/data/', 'log_file': 'data/PDTB/Ji//log/', 'save_file': 'data/PDTB/Ji//saved_dict/', 'model_name_or_path': 'roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 15, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'March03-13:02:00', 'log': 'data/PDTB/Ji//log/March03-13:02:00.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]62it [00:00, 612.47it/s]158it [00:00, 813.31it/s]245it [00:00, 831.68it/s]338it [00:00, 869.47it/s]438it [00:00, 914.17it/s]535it [00:00, 931.30it/s]640it [00:00, 968.05it/s]757it [00:00, 1031.14it/s]870it [00:00, 1059.11it/s]978it [00:01, 1061.77it/s]1094it [00:01, 1089.62it/s]1221it [00:01, 1143.72it/s]1348it [00:01, 1181.67it/s]1467it [00:01, 1168.66it/s]1584it [00:01, 1146.54it/s]1699it [00:01, 1128.15it/s]1822it [00:01, 1156.17it/s]1948it [00:01, 1184.71it/s]2068it [00:01, 1188.53it/s]2188it [00:02, 1190.17it/s]2309it [00:02, 1193.82it/s]2438it [00:02, 1220.32it/s]2565it [00:02, 1234.82it/s]2689it [00:02, 1234.47it/s]2814it [00:02, 1238.67it/s]2939it [00:02, 1241.75it/s]3064it [00:02, 1212.60it/s]3187it [00:02, 1217.59it/s]3309it [00:02, 1213.18it/s]3438it [00:03, 1235.11it/s]3562it [00:03, 1220.76it/s]3692it [00:03, 1240.52it/s]3819it [00:03, 1247.33it/s]3944it [00:03, 1216.26it/s]4069it [00:03, 1224.44it/s]4192it [00:03, 1207.04it/s]4313it [00:03, 1171.15it/s]4431it [00:03, 1161.34it/s]4561it [00:03, 1199.09it/s]4682it [00:04, 1188.65it/s]4820it [00:04, 1243.66it/s]4945it [00:04, 1240.35it/s]5083it [00:04, 1281.52it/s]5217it [00:04, 1295.80it/s]5347it [00:04, 1259.90it/s]5486it [00:04, 1296.67it/s]5624it [00:04, 1316.95it/s]5756it [00:04, 1300.50it/s]5911it [00:04, 1371.97it/s]6049it [00:05, 1333.04it/s]6183it [00:05, 1329.67it/s]6319it [00:05, 1337.89it/s]6453it [00:05, 1328.18it/s]6586it [00:05, 1327.99it/s]6722it [00:05, 1336.10it/s]6856it [00:05, 1336.14it/s]6990it [00:05, 1332.44it/s]7124it [00:05, 1302.72it/s]7264it [00:06, 1330.61it/s]7398it [00:06, 1323.62it/s]7531it [00:06, 1323.91it/s]7664it [00:06, 1312.53it/s]7796it [00:06, 1310.46it/s]7942it [00:06, 1354.38it/s]8078it [00:06, 1325.15it/s]8219it [00:06, 1348.30it/s]8355it [00:06, 1341.04it/s]8490it [00:06, 1308.22it/s]8622it [00:07, 895.33it/s] 8763it [00:07, 1008.91it/s]8896it [00:07, 1084.06it/s]9031it [00:07, 1150.45it/s]9169it [00:07, 1210.96it/s]9310it [00:07, 1264.31it/s]9451it [00:07, 1305.13it/s]9591it [00:07, 1332.38it/s]9741it [00:08, 1378.24it/s]9882it [00:08, 1373.20it/s]10022it [00:08, 1371.83it/s]10161it [00:08, 1358.53it/s]10298it [00:08, 1353.61it/s]10448it [00:08, 1394.22it/s]10588it [00:08, 1381.55it/s]10727it [00:08, 1379.58it/s]10872it [00:08, 1399.08it/s]11013it [00:08, 1391.82it/s]11158it [00:09, 1407.17it/s]11301it [00:09, 1412.31it/s]11443it [00:09, 1395.69it/s]11583it [00:09, 1396.35it/s]11723it [00:09, 1383.51it/s]11862it [00:09, 1376.97it/s]12000it [00:09, 1367.68it/s]12137it [00:09, 1351.57it/s]12275it [00:09, 1357.76it/s]12411it [00:09, 1332.60it/s]12545it [00:10, 1324.07it/s]12547it [00:10, 1246.68it/s]
0it [00:00, ?it/s]138it [00:00, 1372.98it/s]284it [00:00, 1420.04it/s]427it [00:00, 1416.62it/s]569it [00:00, 1389.87it/s]709it [00:00, 1366.73it/s]846it [00:00, 1354.17it/s]984it [00:00, 1360.51it/s]1127it [00:00, 1378.55it/s]1165it [00:00, 1383.68it/s]
0it [00:00, ?it/s]130it [00:00, 1294.22it/s]266it [00:00, 1328.21it/s]417it [00:00, 1410.54it/s]562it [00:00, 1423.78it/s]709it [00:00, 1437.46it/s]853it [00:00, 1412.58it/s]1005it [00:00, 1443.51it/s]1039it [00:00, 1417.92it/s]
Time usage: 23.420384407043457
https://huggingface.co:443 "HEAD /roberta-base/resolve/main/config.json HTTP/1.1" 200 0
https://huggingface.co:443 "HEAD /roberta-base/resolve/main/model.safetensors HTTP/1.1" 302 0
Epoch [1/15]
top-down:TOP: Iter:    100,  Train Loss: 3e+01,  Train Acc: 43.75%,Val Loss:   6.6,  Val Acc: 55.88%, Val F1: 17.92% Time: 85.59130811691284 *
top-down:SEC: Iter:    100,  Train Loss: 3e+01,  Train Acc: 15.62%,Val Loss:   6.6,  Val Acc: 28.93%, Val F1:  9.29% Time: 85.59130811691284 *
top-down:CONN: Iter:    100,  Train Loss: 3e+01,  Train Acc: 25.00%,Val Loss:   6.6,  Val Acc: 15.36%, Val F1:  0.77% Time: 85.59130811691284 *
 
 
top-down:TOP: Iter:    200,  Train Loss: 3.5e+01,  Train Acc: 62.50%,Val Loss:   6.1,  Val Acc: 58.63%, Val F1: 35.41% Time: 162.4525089263916 *
top-down:SEC: Iter:    200,  Train Loss: 3.5e+01,  Train Acc: 56.25%,Val Loss:   6.1,  Val Acc: 36.57%, Val F1: 12.30% Time: 162.4525089263916 *
top-down:CONN: Iter:    200,  Train Loss: 3.5e+01,  Train Acc: 21.88%,Val Loss:   6.1,  Val Acc: 18.37%, Val F1:  1.75% Time: 162.4525089263916 *
 
 
top-down:TOP: Iter:    300,  Train Loss: 2.9e+01,  Train Acc: 46.88%,Val Loss:   5.3,  Val Acc: 64.12%, Val F1: 51.87% Time: 239.53468489646912 *
top-down:SEC: Iter:    300,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.3,  Val Acc: 50.99%, Val F1: 25.45% Time: 239.53468489646912 *
top-down:CONN: Iter:    300,  Train Loss: 2.9e+01,  Train Acc: 28.12%,Val Loss:   5.3,  Val Acc: 26.09%, Val F1:  3.81% Time: 239.53468489646912 *
 
 
Train time usage: 309.2788963317871
Test time usage: 1.6678457260131836
TOP: Test Loss:   4.8,  Test Acc: 67.85%, Test F1: 59.68%
SEC: Test Loss:   4.8,  Test Acc: 57.07%, Test F1: 31.36%
CONN: Test Loss:   4.8,  Test Acc: 28.49%, Test F1:  6.18%
consistency_top_sec: 53.71%,  consistency_sec_conn: 23.77%, consistency_top_sec_conn: 23.10%
              precision    recall  f1-score   support

    Temporal     0.6364    0.3088    0.4158        68
 Contingency     0.5850    0.6533    0.6172       274
  Comparison     0.5732    0.6207    0.5960       145
   Expansion     0.7643    0.7518    0.7580       552

    accuracy                         0.6785      1039
   macro avg     0.6397    0.5837    0.5968      1039
weighted avg     0.6820    0.6785    0.6759      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6000    0.3889    0.4719        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.5693    0.7175    0.6349       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5390    0.6484    0.5887       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5672    0.5700    0.5686       200
    Expansion.Instantiation     0.8955    0.5085    0.6486       118
      Expansion.Restatement     0.5021    0.5782    0.5374       211
      Expansion.Alternative     0.0000    0.0000    0.0000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5707      1039
                  macro avg     0.3339    0.3101    0.3136      1039
               weighted avg     0.5578    0.5707    0.5537      1039

Epoch [2/15]
top-down:TOP: Iter:    400,  Train Loss: 3.2e+01,  Train Acc: 68.75%,Val Loss:   5.1,  Val Acc: 67.38%, Val F1: 53.92% Time: 7.848618745803833 *
top-down:SEC: Iter:    400,  Train Loss: 3.2e+01,  Train Acc: 62.50%,Val Loss:   5.1,  Val Acc: 52.02%, Val F1: 27.16% Time: 7.848618745803833 *
top-down:CONN: Iter:    400,  Train Loss: 3.2e+01,  Train Acc: 21.88%,Val Loss:   5.1,  Val Acc: 29.10%, Val F1:  6.00% Time: 7.848618745803833 *
 
 
top-down:TOP: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 65.62%,Val Loss:   5.0,  Val Acc: 68.24%, Val F1: 57.56% Time: 85.25473666191101 *
top-down:SEC: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   5.0,  Val Acc: 53.05%, Val F1: 28.01% Time: 85.25473666191101 *
top-down:CONN: Iter:    500,  Train Loss: 2.6e+01,  Train Acc: 28.12%,Val Loss:   5.0,  Val Acc: 29.10%, Val F1:  6.24% Time: 85.25473666191101 *
 
 
top-down:TOP: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   4.9,  Val Acc: 66.70%, Val F1: 56.99% Time: 162.87317967414856 *
top-down:SEC: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 59.38%,Val Loss:   4.9,  Val Acc: 52.02%, Val F1: 32.26% Time: 162.87317967414856 *
top-down:CONN: Iter:    600,  Train Loss: 3.1e+01,  Train Acc: 40.62%,Val Loss:   4.9,  Val Acc: 32.27%, Val F1:  8.11% Time: 162.87317967414856 *
 
 
top-down:TOP: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   4.8,  Val Acc: 67.73%, Val F1: 55.88% Time: 239.8880331516266 *
top-down:SEC: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 75.00%,Val Loss:   4.8,  Val Acc: 53.82%, Val F1: 34.56% Time: 239.8880331516266 *
top-down:CONN: Iter:    700,  Train Loss: 3.1e+01,  Train Acc: 43.75%,Val Loss:   4.8,  Val Acc: 30.82%, Val F1:  7.62% Time: 239.8880331516266 *
 
 
Train time usage: 303.95842361450195
Test time usage: 1.6693699359893799
TOP: Test Loss:   4.4,  Test Acc: 70.16%, Test F1: 60.68%
SEC: Test Loss:   4.4,  Test Acc: 59.48%, Test F1: 38.23%
CONN: Test Loss:   4.4,  Test Acc: 29.84%, Test F1: 10.34%
consistency_top_sec: 55.05%,  consistency_sec_conn: 24.83%, consistency_top_sec_conn: 23.39%
              precision    recall  f1-score   support

    Temporal     0.7241    0.3088    0.4330        68
 Contingency     0.6696    0.5568    0.6080       273
  Comparison     0.5959    0.6000    0.5979       145
   Expansion     0.7363    0.8481    0.7882       553

    accuracy                         0.7016      1039
   macro avg     0.6815    0.5784    0.6068      1039
weighted avg     0.6984    0.7016    0.6911      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7632    0.5370    0.6304        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6087    0.6766    0.6408       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5455    0.6562    0.5957       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5509    0.5950    0.5721       200
    Expansion.Instantiation     0.6767    0.7627    0.7171       118
      Expansion.Restatement     0.5978    0.5071    0.5487       211
      Expansion.Alternative     0.3684    0.7778    0.5000         9
             Expansion.List     0.0000    0.0000    0.0000        12

                   accuracy                         0.5948      1039
                  macro avg     0.3737    0.4102    0.3823      1039
               weighted avg     0.5719    0.5948    0.5794      1039

Epoch [3/15]
top-down:TOP: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 75.00%,Val Loss:   4.8,  Val Acc: 68.58%, Val F1: 60.49% Time: 12.948265790939331 *
top-down:SEC: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 68.75%,Val Loss:   4.8,  Val Acc: 51.85%, Val F1: 32.50% Time: 12.948265790939331 *
top-down:CONN: Iter:    800,  Train Loss: 2.9e+01,  Train Acc: 37.50%,Val Loss:   4.8,  Val Acc: 32.02%, Val F1:  8.04% Time: 12.948265790939331 *
 
 
top-down:TOP: Iter:    900,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   4.8,  Val Acc: 68.76%, Val F1: 60.39% Time: 89.83858799934387 *
top-down:SEC: Iter:    900,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   4.8,  Val Acc: 54.25%, Val F1: 34.56% Time: 89.83858799934387 *
top-down:CONN: Iter:    900,  Train Loss: 2.6e+01,  Train Acc: 31.25%,Val Loss:   4.8,  Val Acc: 31.42%, Val F1:  8.51% Time: 89.83858799934387 *
 
 
top-down:TOP: Iter:   1000,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   4.8,  Val Acc: 68.24%, Val F1: 60.84% Time: 166.45618677139282 *
top-down:SEC: Iter:   1000,  Train Loss: 2.8e+01,  Train Acc: 62.50%,Val Loss:   4.8,  Val Acc: 55.28%, Val F1: 35.06% Time: 166.45618677139282 *
top-down:CONN: Iter:   1000,  Train Loss: 2.8e+01,  Train Acc: 40.62%,Val Loss:   4.8,  Val Acc: 33.39%, Val F1:  8.60% Time: 166.45618677139282 *
 
 
top-down:TOP: Iter:   1100,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   4.9,  Val Acc: 68.67%, Val F1: 58.72% Time: 242.47593688964844 
top-down:SEC: Iter:   1100,  Train Loss: 2.7e+01,  Train Acc: 65.62%,Val Loss:   4.9,  Val Acc: 54.59%, Val F1: 35.50% Time: 242.47593688964844 
top-down:CONN: Iter:   1100,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   4.9,  Val Acc: 32.53%, Val F1:  8.60% Time: 242.47593688964844 
 
 
Train time usage: 301.1328594684601
Test time usage: 1.667677879333496
TOP: Test Loss:   4.4,  Test Acc: 70.45%, Test F1: 63.33%
SEC: Test Loss:   4.4,  Test Acc: 60.06%, Test F1: 40.52%
CONN: Test Loss:   4.4,  Test Acc: 32.24%, Test F1: 10.78%
consistency_top_sec: 57.94%,  consistency_sec_conn: 26.08%, consistency_top_sec_conn: 25.51%
              precision    recall  f1-score   support

    Temporal     0.6923    0.3971    0.5047        68
 Contingency     0.6906    0.5641    0.6210       273
  Comparison     0.5436    0.7310    0.6235       145
   Expansion     0.7646    0.8047    0.7841       553

    accuracy                         0.7045      1039
   macro avg     0.6728    0.6242    0.6333      1039
weighted avg     0.7096    0.7045    0.7006      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6744    0.5370    0.5979        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6626    0.5985    0.6289       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.5027    0.7266    0.5942       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5423    0.5450    0.5436       200
    Expansion.Instantiation     0.8113    0.7288    0.7679       118
      Expansion.Restatement     0.5679    0.6540    0.6079       211
      Expansion.Alternative     0.4667    0.7778    0.5833         9
             Expansion.List     0.3333    0.0833    0.1333        12

                   accuracy                         0.6006      1039
                  macro avg     0.4147    0.4228    0.4052      1039
               weighted avg     0.5883    0.6006    0.5890      1039

Epoch [4/15]
top-down:TOP: Iter:   1200,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   5.0,  Val Acc: 68.50%, Val F1: 59.34% Time: 17.439124584197998 
top-down:SEC: Iter:   1200,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   5.0,  Val Acc: 54.59%, Val F1: 34.28% Time: 17.439124584197998 
top-down:CONN: Iter:   1200,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   5.0,  Val Acc: 31.85%, Val F1:  8.56% Time: 17.439124584197998 
 
 
top-down:TOP: Iter:   1300,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   5.0,  Val Acc: 68.76%, Val F1: 59.57% Time: 93.39399075508118 
top-down:SEC: Iter:   1300,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   5.0,  Val Acc: 54.59%, Val F1: 34.73% Time: 93.39399075508118 
top-down:CONN: Iter:   1300,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   5.0,  Val Acc: 32.53%, Val F1:  9.15% Time: 93.39399075508118 
 
 
top-down:TOP: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   4.9,  Val Acc: 70.21%, Val F1: 62.51% Time: 170.18937349319458 *
top-down:SEC: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   4.9,  Val Acc: 55.11%, Val F1: 34.59% Time: 170.18937349319458 *
top-down:CONN: Iter:   1400,  Train Loss: 2.8e+01,  Train Acc: 50.00%,Val Loss:   4.9,  Val Acc: 33.39%, Val F1:  9.47% Time: 170.18937349319458 *
 
 
top-down:TOP: Iter:   1500,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   4.9,  Val Acc: 68.33%, Val F1: 59.85% Time: 246.26668977737427 
top-down:SEC: Iter:   1500,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   4.9,  Val Acc: 55.19%, Val F1: 33.97% Time: 246.26668977737427 
top-down:CONN: Iter:   1500,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   4.9,  Val Acc: 32.62%, Val F1:  8.64% Time: 246.26668977737427 
 
 
Train time usage: 299.9301540851593
Test time usage: 1.6648986339569092
TOP: Test Loss:   4.4,  Test Acc: 72.38%, Test F1: 65.65%
SEC: Test Loss:   4.4,  Test Acc: 61.98%, Test F1: 43.17%
CONN: Test Loss:   4.4,  Test Acc: 32.24%, Test F1: 11.56%
consistency_top_sec: 59.67%,  consistency_sec_conn: 27.14%, consistency_top_sec_conn: 26.66%
              precision    recall  f1-score   support

    Temporal     0.5517    0.4706    0.5079        68
 Contingency     0.6904    0.6022    0.6433       274
  Comparison     0.6781    0.6828    0.6804       145
   Expansion     0.7651    0.8261    0.7944       552

    accuracy                         0.7238      1039
   macro avg     0.6713    0.6454    0.6565      1039
weighted avg     0.7193    0.7238    0.7199      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5862    0.6296    0.6071        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6617    0.6543    0.6579       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6149    0.7109    0.6594       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5928    0.5750    0.5838       200
    Expansion.Instantiation     0.7727    0.7203    0.7456       118
      Expansion.Restatement     0.5617    0.6256    0.5919       211
      Expansion.Alternative     0.4667    0.7778    0.5833         9
             Expansion.List     0.3077    0.3333    0.3200        12

                   accuracy                         0.6198      1039
                  macro avg     0.4149    0.4570    0.4317      1039
               weighted avg     0.6011    0.6198    0.6091      1039

Epoch [5/15]
top-down:TOP: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 90.62%,Val Loss:   5.0,  Val Acc: 68.15%, Val F1: 59.76% Time: 22.653380870819092 
top-down:SEC: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 71.88%,Val Loss:   5.0,  Val Acc: 55.28%, Val F1: 35.03% Time: 22.653380870819092 
top-down:CONN: Iter:   1600,  Train Loss: 3.1e+01,  Train Acc: 56.25%,Val Loss:   5.0,  Val Acc: 34.08%, Val F1:  9.50% Time: 22.653380870819092 
 
 
top-down:TOP: Iter:   1700,  Train Loss: 2.8e+01,  Train Acc: 78.12%,Val Loss:   5.2,  Val Acc: 69.18%, Val F1: 61.87% Time: 98.89157748222351 
top-down:SEC: Iter:   1700,  Train Loss: 2.8e+01,  Train Acc: 71.88%,Val Loss:   5.2,  Val Acc: 53.91%, Val F1: 34.82% Time: 98.89157748222351 
top-down:CONN: Iter:   1700,  Train Loss: 2.8e+01,  Train Acc: 46.88%,Val Loss:   5.2,  Val Acc: 33.05%, Val F1:  9.67% Time: 98.89157748222351 
 
 
top-down:TOP: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   5.3,  Val Acc: 68.15%, Val F1: 59.14% Time: 175.1323356628418 
top-down:SEC: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 84.38%,Val Loss:   5.3,  Val Acc: 54.85%, Val F1: 35.74% Time: 175.1323356628418 
top-down:CONN: Iter:   1800,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   5.3,  Val Acc: 32.88%, Val F1:  9.73% Time: 175.1323356628418 
 
 
top-down:TOP: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.2,  Val Acc: 67.73%, Val F1: 62.09% Time: 252.11802911758423 
top-down:SEC: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 78.12%,Val Loss:   5.2,  Val Acc: 55.54%, Val F1: 35.35% Time: 252.11802911758423 
top-down:CONN: Iter:   1900,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   5.2,  Val Acc: 33.99%, Val F1:  9.74% Time: 252.11802911758423 
 
 
Train time usage: 300.9965307712555
Test time usage: 1.6681692600250244
TOP: Test Loss:   4.6,  Test Acc: 71.61%, Test F1: 64.44%
SEC: Test Loss:   4.6,  Test Acc: 59.87%, Test F1: 39.47%
CONN: Test Loss:   4.6,  Test Acc: 32.05%, Test F1: 11.65%
consistency_top_sec: 58.04%,  consistency_sec_conn: 26.85%, consistency_top_sec_conn: 26.28%
              precision    recall  f1-score   support

    Temporal     0.5079    0.4706    0.4885        68
 Contingency     0.7048    0.5839    0.6387       274
  Comparison     0.6447    0.6759    0.6599       145
   Expansion     0.7605    0.8225    0.7903       552

    accuracy                         0.7161      1039
   macro avg     0.6545    0.6382    0.6444      1039
weighted avg     0.7131    0.7161    0.7124      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5536    0.5741    0.5636        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6612    0.6022    0.6304       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6084    0.6797    0.6421       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5000    0.6350    0.5595       200
    Expansion.Instantiation     0.8211    0.6610    0.7324       118
      Expansion.Restatement     0.5890    0.6114    0.6000       211
      Expansion.Alternative     0.3889    0.7778    0.5185         9
             Expansion.List     0.1111    0.0833    0.0952        12

                   accuracy                         0.5987      1039
                  macro avg     0.3848    0.4204    0.3947      1039
               weighted avg     0.5887    0.5987    0.5899      1039

Epoch [6/15]
top-down:TOP: Iter:   2000,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   5.3,  Val Acc: 68.93%, Val F1: 61.11% Time: 27.96096158027649 
top-down:SEC: Iter:   2000,  Train Loss: 3.2e+01,  Train Acc: 84.38%,Val Loss:   5.3,  Val Acc: 55.02%, Val F1: 35.14% Time: 27.96096158027649 
top-down:CONN: Iter:   2000,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   5.3,  Val Acc: 33.30%, Val F1: 10.06% Time: 27.96096158027649 
 
 
top-down:TOP: Iter:   2100,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   5.3,  Val Acc: 68.15%, Val F1: 60.74% Time: 104.28826212882996 
top-down:SEC: Iter:   2100,  Train Loss: 2.6e+01,  Train Acc: 90.62%,Val Loss:   5.3,  Val Acc: 54.94%, Val F1: 34.91% Time: 104.28826212882996 
top-down:CONN: Iter:   2100,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.3,  Val Acc: 33.13%, Val F1: 10.14% Time: 104.28826212882996 
 
 
top-down:TOP: Iter:   2200,  Train Loss: 2.8e+01,  Train Acc: 90.62%,Val Loss:   5.3,  Val Acc: 68.50%, Val F1: 62.01% Time: 180.7085521221161 *
top-down:SEC: Iter:   2200,  Train Loss: 2.8e+01,  Train Acc: 81.25%,Val Loss:   5.3,  Val Acc: 55.28%, Val F1: 36.28% Time: 180.7085521221161 *
top-down:CONN: Iter:   2200,  Train Loss: 2.8e+01,  Train Acc: 53.12%,Val Loss:   5.3,  Val Acc: 33.48%, Val F1:  9.85% Time: 180.7085521221161 *
 
 
top-down:TOP: Iter:   2300,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   5.3,  Val Acc: 69.53%, Val F1: 62.00% Time: 257.07385087013245 
top-down:SEC: Iter:   2300,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   5.3,  Val Acc: 54.68%, Val F1: 36.07% Time: 257.07385087013245 
top-down:CONN: Iter:   2300,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   5.3,  Val Acc: 31.24%, Val F1:  9.75% Time: 257.07385087013245 
 
 
Train time usage: 300.7811975479126
Test time usage: 1.6681575775146484
TOP: Test Loss:   4.7,  Test Acc: 72.38%, Test F1: 65.77%
SEC: Test Loss:   4.7,  Test Acc: 61.60%, Test F1: 42.61%
CONN: Test Loss:   4.7,  Test Acc: 33.30%, Test F1: 12.31%
consistency_top_sec: 60.35%,  consistency_sec_conn: 28.39%, consistency_top_sec_conn: 28.10%
              precision    recall  f1-score   support

    Temporal     0.5962    0.4559    0.5167        68
 Contingency     0.6705    0.6484    0.6592       273
  Comparison     0.7395    0.6069    0.6667       145
   Expansion     0.7550    0.8246    0.7882       553

    accuracy                         0.7238      1039
   macro avg     0.6903    0.6339    0.6577      1039
weighted avg     0.7202    0.7238    0.7196      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5849    0.5741    0.5794        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6498    0.6691    0.6593       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6752    0.6172    0.6449       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5020    0.6350    0.5607       200
    Expansion.Instantiation     0.7963    0.7288    0.7611       118
      Expansion.Restatement     0.6225    0.6019    0.6120       211
      Expansion.Alternative     0.5000    0.7778    0.6087         9
             Expansion.List     0.2727    0.2500    0.2609        12

                   accuracy                         0.6160      1039
                  macro avg     0.4185    0.4413    0.4261      1039
               weighted avg     0.6028    0.6160    0.6072      1039

Epoch [7/15]
top-down:TOP: Iter:   2400,  Train Loss: 2.2e+01,  Train Acc: 93.75%,Val Loss:   5.4,  Val Acc: 69.01%, Val F1: 61.75% Time: 33.95801329612732 *
top-down:SEC: Iter:   2400,  Train Loss: 2.2e+01,  Train Acc: 90.62%,Val Loss:   5.4,  Val Acc: 56.05%, Val F1: 36.65% Time: 33.95801329612732 *
top-down:CONN: Iter:   2400,  Train Loss: 2.2e+01,  Train Acc: 65.62%,Val Loss:   5.4,  Val Acc: 33.48%, Val F1: 10.19% Time: 33.95801329612732 *
 
 
top-down:TOP: Iter:   2500,  Train Loss: 2.3e+01,  Train Acc: 96.88%,Val Loss:   5.5,  Val Acc: 68.58%, Val F1: 61.26% Time: 109.91636061668396 
top-down:SEC: Iter:   2500,  Train Loss: 2.3e+01,  Train Acc: 93.75%,Val Loss:   5.5,  Val Acc: 54.25%, Val F1: 35.34% Time: 109.91636061668396 
top-down:CONN: Iter:   2500,  Train Loss: 2.3e+01,  Train Acc: 75.00%,Val Loss:   5.5,  Val Acc: 32.27%, Val F1:  9.69% Time: 109.91636061668396 
 
 
top-down:TOP: Iter:   2600,  Train Loss: 2.3e+01,  Train Acc: 100.00%,Val Loss:   5.5,  Val Acc: 69.44%, Val F1: 61.61% Time: 185.86025428771973 
top-down:SEC: Iter:   2600,  Train Loss: 2.3e+01,  Train Acc: 84.38%,Val Loss:   5.5,  Val Acc: 55.45%, Val F1: 36.09% Time: 185.86025428771973 
top-down:CONN: Iter:   2600,  Train Loss: 2.3e+01,  Train Acc: 65.62%,Val Loss:   5.5,  Val Acc: 32.96%, Val F1:  9.88% Time: 185.86025428771973 
 
 
top-down:TOP: Iter:   2700,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   5.6,  Val Acc: 68.15%, Val F1: 60.72% Time: 261.75136137008667 
top-down:SEC: Iter:   2700,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   5.6,  Val Acc: 55.11%, Val F1: 37.95% Time: 261.75136137008667 
top-down:CONN: Iter:   2700,  Train Loss: 2.7e+01,  Train Acc: 43.75%,Val Loss:   5.6,  Val Acc: 32.70%, Val F1:  9.62% Time: 261.75136137008667 
 
 
Train time usage: 299.7421410083771
Test time usage: 1.6635558605194092
TOP: Test Loss:   4.9,  Test Acc: 71.22%, Test F1: 64.72%
SEC: Test Loss:   4.9,  Test Acc: 60.54%, Test F1: 41.69%
CONN: Test Loss:   4.9,  Test Acc: 33.59%, Test F1: 12.26%
consistency_top_sec: 59.10%,  consistency_sec_conn: 28.39%, consistency_top_sec_conn: 27.82%
              precision    recall  f1-score   support

    Temporal     0.5614    0.4638    0.5079        69
 Contingency     0.6792    0.5949    0.6342       274
  Comparison     0.6533    0.6759    0.6644       145
   Expansion     0.7551    0.8113    0.7822       551

    accuracy                         0.7122      1039
   macro avg     0.6622    0.6364    0.6472      1039
weighted avg     0.7080    0.7122    0.7085      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5918    0.5370    0.5631        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6423    0.6208    0.6314       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6099    0.6719    0.6394       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5328    0.6100    0.5688       200
    Expansion.Instantiation     0.7768    0.7373    0.7565       118
      Expansion.Restatement     0.5811    0.6114    0.5958       211
      Expansion.Alternative     0.5455    0.6667    0.6000         9
             Expansion.List     0.2143    0.2500    0.2308        12

                   accuracy                         0.6054      1039
                  macro avg     0.4086    0.4277    0.4169      1039
               weighted avg     0.5882    0.6054    0.5958      1039

Epoch [8/15]
top-down:TOP: Iter:   2800,  Train Loss: 2.7e+01,  Train Acc: 96.88%,Val Loss:   5.6,  Val Acc: 69.96%, Val F1: 61.78% Time: 37.996978759765625 
top-down:SEC: Iter:   2800,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   5.6,  Val Acc: 54.25%, Val F1: 35.62% Time: 37.996978759765625 
top-down:CONN: Iter:   2800,  Train Loss: 2.7e+01,  Train Acc: 53.12%,Val Loss:   5.6,  Val Acc: 32.88%, Val F1:  9.79% Time: 37.996978759765625 
 
 
top-down:TOP: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   5.7,  Val Acc: 67.64%, Val F1: 59.34% Time: 113.83842825889587 
top-down:SEC: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   5.7,  Val Acc: 54.42%, Val F1: 35.61% Time: 113.83842825889587 
top-down:CONN: Iter:   2900,  Train Loss: 2.6e+01,  Train Acc: 62.50%,Val Loss:   5.7,  Val Acc: 32.02%, Val F1:  9.90% Time: 113.83842825889587 
 
 
top-down:TOP: Iter:   3000,  Train Loss: 2.4e+01,  Train Acc: 93.75%,Val Loss:   5.7,  Val Acc: 68.67%, Val F1: 61.27% Time: 190.29401922225952 
top-down:SEC: Iter:   3000,  Train Loss: 2.4e+01,  Train Acc: 84.38%,Val Loss:   5.7,  Val Acc: 53.99%, Val F1: 35.57% Time: 190.29401922225952 
top-down:CONN: Iter:   3000,  Train Loss: 2.4e+01,  Train Acc: 50.00%,Val Loss:   5.7,  Val Acc: 32.79%, Val F1: 10.32% Time: 190.29401922225952 
 
 
top-down:TOP: Iter:   3100,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   5.7,  Val Acc: 69.53%, Val F1: 60.84% Time: 267.1586015224457 
top-down:SEC: Iter:   3100,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   5.7,  Val Acc: 55.11%, Val F1: 35.81% Time: 267.1586015224457 
top-down:CONN: Iter:   3100,  Train Loss: 3.2e+01,  Train Acc: 65.62%,Val Loss:   5.7,  Val Acc: 33.30%, Val F1: 10.30% Time: 267.1586015224457 
 
 
Train time usage: 300.3214981555939
Test time usage: 1.6689412593841553
TOP: Test Loss:   5.1,  Test Acc: 71.61%, Test F1: 64.03%
SEC: Test Loss:   5.1,  Test Acc: 60.35%, Test F1: 41.67%
CONN: Test Loss:   5.1,  Test Acc: 31.67%, Test F1: 12.48%
consistency_top_sec: 59.38%,  consistency_sec_conn: 26.28%, consistency_top_sec_conn: 25.70%
              precision    recall  f1-score   support

    Temporal     0.5600    0.4058    0.4706        69
 Contingency     0.6667    0.6081    0.6360       273
  Comparison     0.6812    0.6483    0.6643       145
   Expansion     0.7575    0.8261    0.7903       552

    accuracy                         0.7161      1039
   macro avg     0.6663    0.6221    0.6403      1039
weighted avg     0.7099    0.7161    0.7109      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5870    0.5000    0.5400        54
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6440    0.6007    0.6216       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6385    0.6484    0.6434       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5686    0.5800    0.5743       200
    Expansion.Instantiation     0.7607    0.7479    0.7542       119
      Expansion.Restatement     0.5578    0.6635    0.6061       211
      Expansion.Alternative     0.5000    0.7778    0.6087         9
             Expansion.List     0.1818    0.3333    0.2353        12

                   accuracy                         0.6035      1039
                  macro avg     0.4035    0.4411    0.4167      1039
               weighted avg     0.5916    0.6035    0.5957      1039

Epoch [9/15]
top-down:TOP: Iter:   3200,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   5.8,  Val Acc: 68.50%, Val F1: 61.03% Time: 43.67212963104248 
top-down:SEC: Iter:   3200,  Train Loss: 2.6e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 55.02%, Val F1: 36.30% Time: 43.67212963104248 
top-down:CONN: Iter:   3200,  Train Loss: 2.6e+01,  Train Acc: 59.38%,Val Loss:   5.8,  Val Acc: 32.70%, Val F1:  9.47% Time: 43.67212963104248 
 
 
top-down:TOP: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   5.8,  Val Acc: 68.76%, Val F1: 61.43% Time: 119.65424132347107 
top-down:SEC: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 84.38%,Val Loss:   5.8,  Val Acc: 54.94%, Val F1: 34.57% Time: 119.65424132347107 
top-down:CONN: Iter:   3300,  Train Loss: 2.7e+01,  Train Acc: 59.38%,Val Loss:   5.8,  Val Acc: 33.13%, Val F1:  9.78% Time: 119.65424132347107 
 
 
top-down:TOP: Iter:   3400,  Train Loss: 3.2e+01,  Train Acc: 93.75%,Val Loss:   5.8,  Val Acc: 68.24%, Val F1: 60.70% Time: 195.39544892311096 
top-down:SEC: Iter:   3400,  Train Loss: 3.2e+01,  Train Acc: 71.88%,Val Loss:   5.8,  Val Acc: 54.42%, Val F1: 35.80% Time: 195.39544892311096 
top-down:CONN: Iter:   3400,  Train Loss: 3.2e+01,  Train Acc: 50.00%,Val Loss:   5.8,  Val Acc: 32.96%, Val F1: 10.20% Time: 195.39544892311096 
 
 
top-down:TOP: Iter:   3500,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   5.8,  Val Acc: 68.67%, Val F1: 61.08% Time: 271.241117477417 
top-down:SEC: Iter:   3500,  Train Loss: 2.9e+01,  Train Acc: 78.12%,Val Loss:   5.8,  Val Acc: 53.91%, Val F1: 34.80% Time: 271.241117477417 
top-down:CONN: Iter:   3500,  Train Loss: 2.9e+01,  Train Acc: 65.62%,Val Loss:   5.8,  Val Acc: 33.22%, Val F1: 10.32% Time: 271.241117477417 
 
 
Train time usage: 298.72276973724365
Test time usage: 1.6660950183868408
TOP: Test Loss:   5.2,  Test Acc: 72.28%, Test F1: 65.42%
SEC: Test Loss:   5.2,  Test Acc: 60.15%, Test F1: 40.77%
CONN: Test Loss:   5.2,  Test Acc: 32.63%, Test F1: 12.48%
consistency_top_sec: 58.61%,  consistency_sec_conn: 27.14%, consistency_top_sec_conn: 26.66%
              precision    recall  f1-score   support

    Temporal     0.6170    0.4203    0.5000        69
 Contingency     0.6776    0.6081    0.6409       273
  Comparison     0.7287    0.6483    0.6861       145
   Expansion     0.7476    0.8370    0.7897       552

    accuracy                         0.7228      1039
   macro avg     0.6927    0.6284    0.6542      1039
weighted avg     0.7179    0.7228    0.7169      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6087    0.5091    0.5545        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6576    0.6306    0.6438       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6613    0.6406    0.6508       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5130    0.5900    0.5488       200
    Expansion.Instantiation     0.7563    0.7627    0.7595       118
      Expansion.Restatement     0.5826    0.6019    0.5921       211
      Expansion.Alternative     0.3889    0.7778    0.5185         9
             Expansion.List     0.1600    0.3333    0.2162        12

                   accuracy                         0.6015      1039
                  macro avg     0.3935    0.4405    0.4077      1039
               weighted avg     0.5915    0.6015    0.5947      1039

Epoch [10/15]
top-down:TOP: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   5.9,  Val Acc: 68.58%, Val F1: 59.91% Time: 48.30153226852417 
top-down:SEC: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   5.9,  Val Acc: 54.59%, Val F1: 35.30% Time: 48.30153226852417 
top-down:CONN: Iter:   3600,  Train Loss: 2.6e+01,  Train Acc: 50.00%,Val Loss:   5.9,  Val Acc: 32.02%, Val F1: 10.13% Time: 48.30153226852417 
 
 
top-down:TOP: Iter:   3700,  Train Loss: 4e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 69.10%, Val F1: 60.80% Time: 124.28120470046997 
top-down:SEC: Iter:   3700,  Train Loss: 4e+01,  Train Acc: 87.50%,Val Loss:   6.0,  Val Acc: 54.33%, Val F1: 35.91% Time: 124.28120470046997 
top-down:CONN: Iter:   3700,  Train Loss: 4e+01,  Train Acc: 53.12%,Val Loss:   6.0,  Val Acc: 32.45%, Val F1: 10.35% Time: 124.28120470046997 
 
 
top-down:TOP: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   6.1,  Val Acc: 68.93%, Val F1: 60.56% Time: 200.61482095718384 
top-down:SEC: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   6.1,  Val Acc: 53.22%, Val F1: 32.85% Time: 200.61482095718384 
top-down:CONN: Iter:   3800,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   6.1,  Val Acc: 31.33%, Val F1: 10.06% Time: 200.61482095718384 
 
 
top-down:TOP: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.0,  Val Acc: 69.70%, Val F1: 62.00% Time: 276.97400999069214 
top-down:SEC: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 87.50%,Val Loss:   6.0,  Val Acc: 54.68%, Val F1: 34.80% Time: 276.97400999069214 
top-down:CONN: Iter:   3900,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.0,  Val Acc: 32.96%, Val F1: 10.54% Time: 276.97400999069214 
 
 
Train time usage: 299.4181327819824
Test time usage: 1.6664893627166748
TOP: Test Loss:   5.3,  Test Acc: 71.22%, Test F1: 63.89%
SEC: Test Loss:   5.3,  Test Acc: 59.67%, Test F1: 39.40%
CONN: Test Loss:   5.3,  Test Acc: 32.92%, Test F1: 12.75%
consistency_top_sec: 58.61%,  consistency_sec_conn: 26.18%, consistency_top_sec_conn: 25.99%
              precision    recall  f1-score   support

    Temporal     0.5370    0.4203    0.4715        69
 Contingency     0.6765    0.5897    0.6301       273
  Comparison     0.6690    0.6690    0.6690       145
   Expansion     0.7525    0.8207    0.7851       552

    accuracy                         0.7122      1039
   macro avg     0.6587    0.6249    0.6389      1039
weighted avg     0.7066    0.7122    0.7073      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5472    0.5273    0.5370        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6680    0.6082    0.6367       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6056    0.6719    0.6370       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4978    0.5650    0.5293       200
    Expansion.Instantiation     0.7982    0.7373    0.7665       118
      Expansion.Restatement     0.5726    0.6351    0.6022       211
      Expansion.Alternative     0.4118    0.7778    0.5385         9
             Expansion.List     0.0909    0.0833    0.0870        12

                   accuracy                         0.5967      1039
                  macro avg     0.3811    0.4187    0.3940      1039
               weighted avg     0.5833    0.5967    0.5881      1039

Epoch [11/15]
top-down:TOP: Iter:   4000,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   6.0,  Val Acc: 69.27%, Val F1: 60.42% Time: 54.05468559265137 
top-down:SEC: Iter:   4000,  Train Loss: 3.1e+01,  Train Acc: 87.50%,Val Loss:   6.0,  Val Acc: 55.19%, Val F1: 37.13% Time: 54.05468559265137 
top-down:CONN: Iter:   4000,  Train Loss: 3.1e+01,  Train Acc: 62.50%,Val Loss:   6.0,  Val Acc: 32.79%, Val F1: 10.37% Time: 54.05468559265137 
 
 
top-down:TOP: Iter:   4100,  Train Loss: 2.4e+01,  Train Acc: 96.88%,Val Loss:   6.0,  Val Acc: 69.27%, Val F1: 62.33% Time: 130.50741910934448 
top-down:SEC: Iter:   4100,  Train Loss: 2.4e+01,  Train Acc: 90.62%,Val Loss:   6.0,  Val Acc: 55.11%, Val F1: 35.88% Time: 130.50741910934448 
top-down:CONN: Iter:   4100,  Train Loss: 2.4e+01,  Train Acc: 71.88%,Val Loss:   6.0,  Val Acc: 32.36%, Val F1: 10.83% Time: 130.50741910934448 
 
 
top-down:TOP: Iter:   4200,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   6.1,  Val Acc: 69.87%, Val F1: 61.96% Time: 206.94577741622925 
top-down:SEC: Iter:   4200,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.1,  Val Acc: 54.85%, Val F1: 35.95% Time: 206.94577741622925 
top-down:CONN: Iter:   4200,  Train Loss: 2.6e+01,  Train Acc: 56.25%,Val Loss:   6.1,  Val Acc: 33.22%, Val F1: 10.34% Time: 206.94577741622925 
 
 
top-down:TOP: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 69.01%, Val F1: 61.30% Time: 283.1392705440521 
top-down:SEC: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 87.50%,Val Loss:   6.1,  Val Acc: 55.54%, Val F1: 36.64% Time: 283.1392705440521 
top-down:CONN: Iter:   4300,  Train Loss: 2.5e+01,  Train Acc: 50.00%,Val Loss:   6.1,  Val Acc: 32.02%, Val F1: 10.27% Time: 283.1392705440521 
 
 
Train time usage: 300.34984588623047
Test time usage: 1.6679868698120117
TOP: Test Loss:   5.3,  Test Acc: 71.41%, Test F1: 64.37%
SEC: Test Loss:   5.3,  Test Acc: 59.67%, Test F1: 40.07%
CONN: Test Loss:   5.3,  Test Acc: 33.01%, Test F1: 12.84%
consistency_top_sec: 58.71%,  consistency_sec_conn: 27.14%, consistency_top_sec_conn: 27.05%
              precision    recall  f1-score   support

    Temporal     0.5686    0.4203    0.4833        69
 Contingency     0.6502    0.6264    0.6381       273
  Comparison     0.6713    0.6621    0.6667       145
   Expansion     0.7663    0.8080    0.7866       552

    accuracy                         0.7141      1039
   macro avg     0.6641    0.6292    0.6437      1039
weighted avg     0.7094    0.7141    0.7107      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5957    0.5091    0.5490        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6397    0.6468    0.6433       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6250    0.6641    0.6439       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.4932    0.5400    0.5155       200
    Expansion.Instantiation     0.8252    0.7265    0.7727       117
      Expansion.Restatement     0.5848    0.6209    0.6023       211
      Expansion.Alternative     0.4118    0.7778    0.5385         9
             Expansion.List     0.1250    0.1667    0.1429        12

                   accuracy                         0.5967      1039
                  macro avg     0.3909    0.4229    0.4007      1039
               weighted avg     0.5858    0.5967    0.5898      1039

Epoch [12/15]
top-down:TOP: Iter:   4400,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 69.44%, Val F1: 62.33% Time: 59.88160562515259 *
top-down:SEC: Iter:   4400,  Train Loss: 3.2e+01,  Train Acc: 87.50%,Val Loss:   6.1,  Val Acc: 55.11%, Val F1: 36.55% Time: 59.88160562515259 *
top-down:CONN: Iter:   4400,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   6.1,  Val Acc: 33.30%, Val F1: 10.73% Time: 59.88160562515259 *
 
 
top-down:TOP: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 69.27%, Val F1: 61.85% Time: 136.11396098136902 
top-down:SEC: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 54.76%, Val F1: 37.07% Time: 136.11396098136902 
top-down:CONN: Iter:   4500,  Train Loss: 2.9e+01,  Train Acc: 53.12%,Val Loss:   6.1,  Val Acc: 32.45%, Val F1: 10.39% Time: 136.11396098136902 
 
 
top-down:TOP: Iter:   4600,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   6.1,  Val Acc: 69.01%, Val F1: 60.98% Time: 212.42538905143738 
top-down:SEC: Iter:   4600,  Train Loss: 3.1e+01,  Train Acc: 96.88%,Val Loss:   6.1,  Val Acc: 55.45%, Val F1: 37.25% Time: 212.42538905143738 
top-down:CONN: Iter:   4600,  Train Loss: 3.1e+01,  Train Acc: 53.12%,Val Loss:   6.1,  Val Acc: 32.10%, Val F1: 10.47% Time: 212.42538905143738 
 
 
top-down:TOP: Iter:   4700,  Train Loss: 2.7e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 68.41%, Val F1: 61.26% Time: 297.09650182724 
top-down:SEC: Iter:   4700,  Train Loss: 2.7e+01,  Train Acc: 87.50%,Val Loss:   6.1,  Val Acc: 54.59%, Val F1: 36.61% Time: 297.09650182724 
top-down:CONN: Iter:   4700,  Train Loss: 2.7e+01,  Train Acc: 40.62%,Val Loss:   6.1,  Val Acc: 32.45%, Val F1: 10.64% Time: 297.09650182724 
 
 
Train time usage: 311.8839981555939
Test time usage: 1.7066643238067627
TOP: Test Loss:   5.4,  Test Acc: 71.22%, Test F1: 63.61%
SEC: Test Loss:   5.4,  Test Acc: 59.96%, Test F1: 40.55%
CONN: Test Loss:   5.4,  Test Acc: 33.49%, Test F1: 13.07%
consistency_top_sec: 58.61%,  consistency_sec_conn: 28.01%, consistency_top_sec_conn: 27.53%
              precision    recall  f1-score   support

    Temporal     0.5306    0.3768    0.4407        69
 Contingency     0.6550    0.6190    0.6365       273
  Comparison     0.7111    0.6621    0.6857       145
   Expansion     0.7521    0.8134    0.7815       552

    accuracy                         0.7122      1039
   macro avg     0.6622    0.6178    0.6361      1039
weighted avg     0.7062    0.7122    0.7074      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5833    0.5091    0.5437        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6418    0.6394    0.6406       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6434    0.6484    0.6459       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5000    0.5900    0.5413       200
    Expansion.Instantiation     0.7890    0.7350    0.7611       117
      Expansion.Restatement     0.6087    0.5972    0.6029       211
      Expansion.Alternative     0.3889    0.7778    0.5185         9
             Expansion.List     0.1765    0.2500    0.2069        12

                   accuracy                         0.5996      1039
                  macro avg     0.3938    0.4315    0.4055      1039
               weighted avg     0.5904    0.5996    0.5934      1039

Epoch [13/15]
top-down:TOP: Iter:   4800,  Train Loss: 3.6e+01,  Train Acc: 93.75%,Val Loss:   6.1,  Val Acc: 68.84%, Val F1: 61.46% Time: 79.01466178894043 
top-down:SEC: Iter:   4800,  Train Loss: 3.6e+01,  Train Acc: 90.62%,Val Loss:   6.1,  Val Acc: 55.11%, Val F1: 37.59% Time: 79.01466178894043 
top-down:CONN: Iter:   4800,  Train Loss: 3.6e+01,  Train Acc: 50.00%,Val Loss:   6.1,  Val Acc: 32.88%, Val F1: 10.43% Time: 79.01466178894043 
 
 
top-down:TOP: Iter:   4900,  Train Loss: 2.8e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 68.58%, Val F1: 60.96% Time: 171.91213703155518 
top-down:SEC: Iter:   4900,  Train Loss: 2.8e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 55.11%, Val F1: 37.27% Time: 171.91213703155518 
top-down:CONN: Iter:   4900,  Train Loss: 2.8e+01,  Train Acc: 68.75%,Val Loss:   6.2,  Val Acc: 31.76%, Val F1: 10.59% Time: 171.91213703155518 
 
 
top-down:TOP: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 68.58%, Val F1: 60.99% Time: 264.0284969806671 
top-down:SEC: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 87.50%,Val Loss:   6.2,  Val Acc: 54.59%, Val F1: 37.13% Time: 264.0284969806671 
top-down:CONN: Iter:   5000,  Train Loss: 2.9e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 32.02%, Val F1: 10.83% Time: 264.0284969806671 
 
 
top-down:TOP: Iter:   5100,  Train Loss: 2.6e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 68.93%, Val F1: 61.27% Time: 356.15917801856995 
top-down:SEC: Iter:   5100,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 55.02%, Val F1: 36.60% Time: 356.15917801856995 
top-down:CONN: Iter:   5100,  Train Loss: 2.6e+01,  Train Acc: 71.88%,Val Loss:   6.2,  Val Acc: 31.93%, Val F1: 10.84% Time: 356.15917801856995 
 
 
Train time usage: 365.8669776916504
Test time usage: 1.9874801635742188
TOP: Test Loss:   5.5,  Test Acc: 71.51%, Test F1: 64.18%
SEC: Test Loss:   5.5,  Test Acc: 60.35%, Test F1: 40.15%
CONN: Test Loss:   5.5,  Test Acc: 33.11%, Test F1: 13.07%
consistency_top_sec: 58.90%,  consistency_sec_conn: 27.43%, consistency_top_sec_conn: 27.24%
              precision    recall  f1-score   support

    Temporal     0.5909    0.3768    0.4602        69
 Contingency     0.6600    0.6044    0.6310       273
  Comparison     0.7101    0.6759    0.6926       145
   Expansion     0.7479    0.8225    0.7834       552

    accuracy                         0.7151      1039
   macro avg     0.6772    0.6199    0.6418      1039
weighted avg     0.7091    0.7151    0.7092      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6098    0.4545    0.5208        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6466    0.6394    0.6430       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6391    0.6641    0.6513       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5109    0.5850    0.5455       200
    Expansion.Instantiation     0.8113    0.7350    0.7713       117
      Expansion.Restatement     0.5885    0.6303    0.6087       211
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.1875    0.2500    0.2143        12

                   accuracy                         0.6035      1039
                  macro avg     0.3951    0.4205    0.4015      1039
               weighted avg     0.5929    0.6035    0.5962      1039

Epoch [14/15]
top-down:TOP: Iter:   5200,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 68.76%, Val F1: 61.48% Time: 89.71160864830017 
top-down:SEC: Iter:   5200,  Train Loss: 2.8e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 54.42%, Val F1: 36.24% Time: 89.71160864830017 
top-down:CONN: Iter:   5200,  Train Loss: 2.8e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 32.36%, Val F1: 10.87% Time: 89.71160864830017 
 
 
top-down:TOP: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 90.62%,Val Loss:   6.2,  Val Acc: 68.84%, Val F1: 61.76% Time: 198.72223567962646 
top-down:SEC: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 54.25%, Val F1: 35.83% Time: 198.72223567962646 
top-down:CONN: Iter:   5300,  Train Loss: 3e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 32.88%, Val F1: 10.99% Time: 198.72223567962646 
 
 
top-down:TOP: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 69.10%, Val F1: 62.14% Time: 296.20339846611023 
top-down:SEC: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 90.62%,Val Loss:   6.2,  Val Acc: 54.85%, Val F1: 36.29% Time: 296.20339846611023 
top-down:CONN: Iter:   5400,  Train Loss: 3.2e+01,  Train Acc: 59.38%,Val Loss:   6.2,  Val Acc: 32.53%, Val F1: 11.08% Time: 296.20339846611023 
 
 
top-down:TOP: Iter:   5500,  Train Loss: 2.7e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 68.93%, Val F1: 60.90% Time: 388.49823355674744 
top-down:SEC: Iter:   5500,  Train Loss: 2.7e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 55.62%, Val F1: 37.39% Time: 388.49823355674744 
top-down:CONN: Iter:   5500,  Train Loss: 2.7e+01,  Train Acc: 81.25%,Val Loss:   6.2,  Val Acc: 32.19%, Val F1: 11.18% Time: 388.49823355674744 
 
 
Train time usage: 390.349009513855
Test time usage: 1.721330165863037
TOP: Test Loss:   5.5,  Test Acc: 71.99%, Test F1: 64.44%
SEC: Test Loss:   5.5,  Test Acc: 59.96%, Test F1: 39.75%
CONN: Test Loss:   5.5,  Test Acc: 33.49%, Test F1: 13.26%
consistency_top_sec: 59.00%,  consistency_sec_conn: 27.91%, consistency_top_sec_conn: 27.72%
              precision    recall  f1-score   support

    Temporal     0.6190    0.3768    0.4685        69
 Contingency     0.6790    0.6044    0.6395       273
  Comparison     0.6929    0.6690    0.6807       145
   Expansion     0.7492    0.8333    0.7890       552

    accuracy                         0.7199      1039
   macro avg     0.6850    0.6209    0.6444      1039
weighted avg     0.7142    0.7199    0.7133      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6500    0.4727    0.5474        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6545    0.6007    0.6265       268
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6204    0.6641    0.6415       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5111    0.5750    0.5412       200
    Expansion.Instantiation     0.7838    0.7373    0.7598       118
      Expansion.Restatement     0.5709    0.6682    0.6157       211
      Expansion.Alternative     0.3750    0.6667    0.4800         9
             Expansion.List     0.1538    0.1667    0.1600        12

                   accuracy                         0.5996      1039
                  macro avg     0.3927    0.4138    0.3975      1039
               weighted avg     0.5880    0.5996    0.5911      1039

Epoch [15/15]
top-down:TOP: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 68.93%, Val F1: 60.85% Time: 89.66485452651978 
top-down:SEC: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 54.51%, Val F1: 36.07% Time: 89.66485452651978 
top-down:CONN: Iter:   5600,  Train Loss: 2.9e+01,  Train Acc: 84.38%,Val Loss:   6.2,  Val Acc: 32.36%, Val F1: 11.04% Time: 89.66485452651978 
 
 
top-down:TOP: Iter:   5700,  Train Loss: 2.6e+01,  Train Acc: 96.88%,Val Loss:   6.2,  Val Acc: 69.10%, Val F1: 61.62% Time: 181.8790831565857 
top-down:SEC: Iter:   5700,  Train Loss: 2.6e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 55.11%, Val F1: 36.59% Time: 181.8790831565857 
top-down:CONN: Iter:   5700,  Train Loss: 2.6e+01,  Train Acc: 75.00%,Val Loss:   6.2,  Val Acc: 32.19%, Val F1: 11.17% Time: 181.8790831565857 
 
 
top-down:TOP: Iter:   5800,  Train Loss: 3.1e+01,  Train Acc: 100.00%,Val Loss:   6.2,  Val Acc: 69.01%, Val F1: 61.42% Time: 264.07606530189514 
top-down:SEC: Iter:   5800,  Train Loss: 3.1e+01,  Train Acc: 93.75%,Val Loss:   6.2,  Val Acc: 55.36%, Val F1: 36.75% Time: 264.07606530189514 
top-down:CONN: Iter:   5800,  Train Loss: 3.1e+01,  Train Acc: 78.12%,Val Loss:   6.2,  Val Acc: 32.27%, Val F1: 11.07% Time: 264.07606530189514 
 
 
Train time usage: 349.89478635787964
Test time usage: 1.7421915531158447
TOP: Test Loss:   5.5,  Test Acc: 71.61%, Test F1: 64.07%
SEC: Test Loss:   5.5,  Test Acc: 60.35%, Test F1: 39.63%
CONN: Test Loss:   5.5,  Test Acc: 32.82%, Test F1: 13.00%
consistency_top_sec: 59.19%,  consistency_sec_conn: 27.05%, consistency_top_sec_conn: 26.76%
              precision    recall  f1-score   support

    Temporal     0.5652    0.3768    0.4522        69
 Contingency     0.6602    0.6264    0.6429       273
  Comparison     0.6901    0.6759    0.6829       145
   Expansion     0.7584    0.8134    0.7850       552

    accuracy                         0.7161      1039
   macro avg     0.6685    0.6231    0.6407      1039
weighted avg     0.7103    0.7161    0.7113      1039

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.5909    0.4727    0.5253        55
         Temporal.Synchrony     0.0000    0.0000    0.0000        14
          Contingency.Cause     0.6384    0.6431    0.6407       269
Contingency.Pragmatic cause     0.0000    0.0000    0.0000         7
        Comparison.Contrast     0.6391    0.6641    0.6513       128
      Comparison.Concession     0.0000    0.0000    0.0000        17
      Expansion.Conjunction     0.5158    0.5700    0.5416       200
    Expansion.Instantiation     0.7963    0.7350    0.7644       117
      Expansion.Restatement     0.6027    0.6398    0.6207       211
      Expansion.Alternative     0.3529    0.6667    0.4615         9
             Expansion.List     0.1429    0.1667    0.1538        12

                   accuracy                         0.6035      1039
                  macro avg     0.3890    0.4144    0.3963      1039
               weighted avg     0.5914    0.6035    0.5961      1039

dev_best_acc_top: 69.44%,  dev_best_f1_top: 62.33%, 
dev_best_acc_sec: 55.11%,  dev_best_f1_sec: 36.55%, 
dev_best_acc_conn: 33.30%,  dev_best_f1_conn: 10.73%
