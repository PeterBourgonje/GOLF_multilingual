/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:47: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'pyg-lib'. "
/home/VD/peterb/python/lib/python3.10/site-packages/torch_geometric/typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /local/home/VD/peterb/python/lib/python3.10/site-packages/libpyg.so)
  warnings.warn(f"An issue occurred while importing 'torch-sparse'. "
{'cuda': 0, 'seed': 0, 'data_file': 'tur.pdtb.tdb/data/', 'log_file': 'tur.pdtb.tdb/log/', 'save_file': 'tur.pdtb.tdb/saved_dict/', 'model_name_or_path': 'xlm-roberta-base', 'freeze_bert': False, 'temperature': 0.1, 'num_co_attention_layer': 2, 'num_gcn_layer': 2, 'gcn_dropout': 0.1, 'label_embedding_size': 100, 'lambda_global': 0.1, 'lambda_local': 1.0, 'pad_size': 100, 'batch_size': 32, 'epoch': 20, 'lr': 1e-05, 'warmup_ratio': 0.05, 'evaluate_steps': 100, 'require_improvement': 10000, 'i2top': '', 'top2i': '', 'n_top': 4, 'i2sec': '', 'sec2i': '', 'n_sec': 11, 'i2conn': '', 'conn2i': '', 'n_conn': 102, 'label_num': 117, 'tokenizer': '', 'config': '', 't': 'February20-13:08:53', 'log': 'tur.pdtb.tdb/log/February20-13:08:53.log', 'device': device(type='cuda', index=0)}
Loading data...
0it [00:00, ?it/s]147it [00:00, 1469.19it/s]400it [00:00, 2091.43it/s]671it [00:00, 2366.47it/s]908it [00:00, 2311.56it/s]1140it [00:00, 2261.77it/s]1348it [00:00, 2253.96it/s]
0it [00:00, ?it/s]168it [00:00, 1670.97it/s]193it [00:00, 1503.69it/s]
0it [00:00, ?it/s]239it [00:00, 2383.53it/s]268it [00:00, 2330.98it/s]
Time usage: 10.462511539459229
https://huggingface.co:443 "HEAD /xlm-roberta-base/resolve/main/config.json HTTP/1.1" 200 0
Epoch [1/20]
Train time usage: 44.06543278694153
Test time usage: 0.5545749664306641
TOP: Test Loss:   3.0,  Test Acc: 42.91%, Test F1: 15.01%
SEC: Test Loss:   3.0,  Test Acc: 39.93%, Test F1:  9.51%
CONN: Test Loss:   3.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 10.30%,  consistency_sec_conn: 10.30%, consistency_top_sec_conn: 10.30%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        59
 Contingency     0.0000    0.0000    0.0000        41
  Comparison     0.0000    0.0000    0.0000        53
   Expansion     0.4291    1.0000    0.6005       115

    accuracy                         0.4291       268
   macro avg     0.1073    0.2500    0.1501       268
weighted avg     0.1841    0.4291    0.2577       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        59
         Temporal.Synchrony     0.0000    0.0000    0.0000        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.3993    1.0000    0.5707       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.3993       268
                  macro avg     0.0665    0.1667    0.0951       268
               weighted avg     0.1594    0.3993    0.2278       268

Epoch [2/20]
Train time usage: 33.253762006759644
Test time usage: 0.5703940391540527
TOP: Test Loss:   3.0,  Test Acc: 42.91%, Test F1: 15.01%
SEC: Test Loss:   3.0,  Test Acc: 39.93%, Test F1:  9.51%
CONN: Test Loss:   3.0,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 10.30%,  consistency_sec_conn: 10.30%, consistency_top_sec_conn: 10.30%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        59
 Contingency     0.0000    0.0000    0.0000        41
  Comparison     0.0000    0.0000    0.0000        53
   Expansion     0.4291    1.0000    0.6005       115

    accuracy                         0.4291       268
   macro avg     0.1073    0.2500    0.1501       268
weighted avg     0.1841    0.4291    0.2577       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        59
         Temporal.Synchrony     0.0000    0.0000    0.0000        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.3993    1.0000    0.5707       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.3993       268
                  macro avg     0.0665    0.1667    0.0951       268
               weighted avg     0.1594    0.3993    0.2278       268

Epoch [3/20]
top-down:TOP: Iter:    100,  Train Loss: 5.6e+01,  Train Acc: 28.12%,Val Loss:   2.6,  Val Acc: 45.08%, Val F1: 15.54% Time: 14.290163516998291 *
top-down:SEC: Iter:    100,  Train Loss: 5.6e+01,  Train Acc: 25.00%,Val Loss:   2.6,  Val Acc: 42.49%, Val F1:  9.94% Time: 14.290163516998291 *
top-down:CONN: Iter:    100,  Train Loss: 5.6e+01,  Train Acc: 100.00%,Val Loss:   2.6,  Val Acc: 100.00%, Val F1: 100.00% Time: 14.290163516998291 *
 
 
Train time usage: 37.34808349609375
Test time usage: 0.5114989280700684
TOP: Test Loss:   2.8,  Test Acc: 42.91%, Test F1: 15.01%
SEC: Test Loss:   2.8,  Test Acc: 39.93%, Test F1:  9.51%
CONN: Test Loss:   2.8,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 10.30%,  consistency_sec_conn: 10.30%, consistency_top_sec_conn: 10.30%
              precision    recall  f1-score   support

    Temporal     0.0000    0.0000    0.0000        59
 Contingency     0.0000    0.0000    0.0000        41
  Comparison     0.0000    0.0000    0.0000        53
   Expansion     0.4291    1.0000    0.6005       115

    accuracy                         0.4291       268
   macro avg     0.1073    0.2500    0.1501       268
weighted avg     0.1841    0.4291    0.2577       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.0000    0.0000    0.0000        59
         Temporal.Synchrony     0.0000    0.0000    0.0000        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.3993    1.0000    0.5707       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.3993       268
                  macro avg     0.0665    0.1667    0.0951       268
               weighted avg     0.1594    0.3993    0.2278       268

Epoch [4/20]
Train time usage: 37.79839873313904
Test time usage: 0.6811363697052002
TOP: Test Loss:   2.6,  Test Acc: 52.24%, Test F1: 31.57%
SEC: Test Loss:   2.6,  Test Acc: 49.25%, Test F1: 20.65%
CONN: Test Loss:   2.6,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 12.51%,  consistency_sec_conn: 12.70%, consistency_top_sec_conn: 12.51%
              precision    recall  f1-score   support

    Temporal     0.7442    0.5424    0.6275        59
 Contingency     0.0000    0.0000    0.0000        41
  Comparison     0.0000    0.0000    0.0000        53
   Expansion     0.4800    0.9391    0.6353       115

    accuracy                         0.5224       268
   macro avg     0.3060    0.3704    0.3157       268
weighted avg     0.3698    0.5224    0.4107       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7234    0.5763    0.6415        59
         Temporal.Synchrony     0.0000    0.0000    0.0000        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4434    0.9159    0.5976       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.4925       268
                  macro avg     0.1945    0.2487    0.2065       268
               weighted avg     0.3363    0.4925    0.3798       268

Epoch [5/20]
top-down:TOP: Iter:    200,  Train Loss: 5.7e+01,  Train Acc: 46.88%,Val Loss:   2.4,  Val Acc: 52.33%, Val F1: 30.60% Time: 27.633387327194214 *
top-down:SEC: Iter:    200,  Train Loss: 5.7e+01,  Train Acc: 46.88%,Val Loss:   2.4,  Val Acc: 50.26%, Val F1: 20.25% Time: 27.633387327194214 *
top-down:CONN: Iter:    200,  Train Loss: 5.7e+01,  Train Acc: 100.00%,Val Loss:   2.4,  Val Acc: 100.00%, Val F1: 100.00% Time: 27.633387327194214 *
 
 
Train time usage: 42.34913372993469
Test time usage: 0.6036906242370605
TOP: Test Loss:   2.3,  Test Acc: 53.36%, Test F1: 33.32%
SEC: Test Loss:   2.3,  Test Acc: 50.37%, Test F1: 21.61%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 12.99%,  consistency_sec_conn: 12.99%, consistency_top_sec_conn: 12.99%
              precision    recall  f1-score   support

    Temporal     0.6571    0.7797    0.7132        59
 Contingency     0.0000    0.0000    0.0000        41
  Comparison     0.0000    0.0000    0.0000        53
   Expansion     0.4899    0.8435    0.6198       115

    accuracy                         0.5336       268
   macro avg     0.2868    0.4058    0.3332       268
weighted avg     0.3549    0.5336    0.4230       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6571    0.7797    0.7132        59
         Temporal.Synchrony     0.0000    0.0000    0.0000        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4495    0.8318    0.5836       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5037       268
                  macro avg     0.1844    0.2686    0.2161       268
               weighted avg     0.3241    0.5037    0.3900       268

Epoch [6/20]
Train time usage: 40.36594080924988
Test time usage: 0.6547653675079346
TOP: Test Loss:   2.2,  Test Acc: 55.97%, Test F1: 35.41%
SEC: Test Loss:   2.2,  Test Acc: 52.61%, Test F1: 22.76%
CONN: Test Loss:   2.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.57%,  consistency_sec_conn: 13.57%, consistency_top_sec_conn: 13.57%
              precision    recall  f1-score   support

    Temporal     0.7027    0.8814    0.7820        59
 Contingency     0.0000    0.0000    0.0000        41
  Comparison     0.0000    0.0000    0.0000        53
   Expansion     0.5052    0.8522    0.6343       115

    accuracy                         0.5597       268
   macro avg     0.3020    0.4334    0.3541       268
weighted avg     0.3715    0.5597    0.4443       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6842    0.8814    0.7704        59
         Temporal.Synchrony     0.0000    0.0000    0.0000        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4635    0.8318    0.5953       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5261       268
                  macro avg     0.1913    0.2855    0.2276       268
               weighted avg     0.3357    0.5261    0.4073       268

Epoch [7/20]
top-down:TOP: Iter:    300,  Train Loss: 6.8e+01,  Train Acc: 65.62%,Val Loss:   2.3,  Val Acc: 52.33%, Val F1: 30.43% Time: 38.22236180305481 
top-down:SEC: Iter:    300,  Train Loss: 6.8e+01,  Train Acc: 62.50%,Val Loss:   2.3,  Val Acc: 49.74%, Val F1: 19.83% Time: 38.22236180305481 
top-down:CONN: Iter:    300,  Train Loss: 6.8e+01,  Train Acc: 100.00%,Val Loss:   2.3,  Val Acc: 100.00%, Val F1: 100.00% Time: 38.22236180305481 
 
 
Train time usage: 40.115965843200684
Test time usage: 0.6009092330932617
TOP: Test Loss:   2.2,  Test Acc: 54.85%, Test F1: 34.38%
SEC: Test Loss:   2.2,  Test Acc: 51.87%, Test F1: 22.33%
CONN: Test Loss:   2.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.38%,  consistency_sec_conn: 13.38%, consistency_top_sec_conn: 13.38%
              precision    recall  f1-score   support

    Temporal     0.6970    0.7797    0.7360        59
 Contingency     0.0000    0.0000    0.0000        41
  Comparison     0.0000    0.0000    0.0000        53
   Expansion     0.5025    0.8783    0.6392       115

    accuracy                         0.5485       268
   macro avg     0.2999    0.4145    0.3438       268
weighted avg     0.3691    0.5485    0.4363       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.6970    0.7797    0.7360        59
         Temporal.Synchrony     0.0000    0.0000    0.0000        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.4627    0.8692    0.6039       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5187       268
                  macro avg     0.1933    0.2748    0.2233       268
               weighted avg     0.3382    0.5187    0.4031       268

Epoch [8/20]
Train time usage: 39.33772420883179
Test time usage: 0.596306562423706
TOP: Test Loss:   2.2,  Test Acc: 52.61%, Test F1: 46.67%
SEC: Test Loss:   2.2,  Test Acc: 50.00%, Test F1: 28.66%
CONN: Test Loss:   2.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 12.70%,  consistency_sec_conn: 12.90%, consistency_top_sec_conn: 12.70%
              precision    recall  f1-score   support

    Temporal     0.7368    0.7119    0.7241        59
 Contingency     0.3176    0.6585    0.4286        41
  Comparison     1.0000    0.0755    0.1404        53
   Expansion     0.5574    0.5913    0.5738       115

    accuracy                         0.5261       268
   macro avg     0.6530    0.5093    0.4667       268
weighted avg     0.6477    0.5261    0.4990       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7368    0.7119    0.7241        59
         Temporal.Synchrony     0.3222    0.7073    0.4427        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.0000    0.0000    0.0000        32
        Comparison.Contrast     0.5207    0.5888    0.5526       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5000       268
                  macro avg     0.2633    0.3347    0.2866       268
               weighted avg     0.4194    0.5000    0.4478       268

Epoch [9/20]
Train time usage: 37.61579966545105
Test time usage: 0.5281190872192383
TOP: Test Loss:   2.3,  Test Acc: 63.06%, Test F1: 54.90%
SEC: Test Loss:   2.3,  Test Acc: 54.85%, Test F1: 32.38%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 13.67%,  consistency_sec_conn: 14.15%, consistency_top_sec_conn: 13.67%
              precision    recall  f1-score   support

    Temporal     0.7347    0.6102    0.6667        59
 Contingency     0.4286    0.2195    0.2903        41
  Comparison     0.8333    0.3774    0.5195        53
   Expansion     0.5977    0.9043    0.7197       115

    accuracy                         0.6306       268
   macro avg     0.6486    0.5278    0.5490       268
weighted avg     0.6486    0.6306    0.6028       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7400    0.6271    0.6789        59
         Temporal.Synchrony     0.4286    0.2927    0.3478        41
          Contingency.Cause     0.3333    0.1429    0.2000        21
Contingency.Pragmatic cause     0.5000    0.0312    0.0588        32
        Comparison.Contrast     0.5251    0.8785    0.6573       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5485       268
                  macro avg     0.4212    0.3287    0.3238       268
               weighted avg     0.5240    0.5485    0.4878       268

Epoch [10/20]
top-down:TOP: Iter:    400,  Train Loss: 5.6e+01,  Train Acc: 78.12%,Val Loss:   2.3,  Val Acc: 54.92%, Val F1: 47.47% Time: 12.033645153045654 *
top-down:SEC: Iter:    400,  Train Loss: 5.6e+01,  Train Acc: 75.00%,Val Loss:   2.3,  Val Acc: 50.78%, Val F1: 28.89% Time: 12.033645153045654 *
top-down:CONN: Iter:    400,  Train Loss: 5.6e+01,  Train Acc: 100.00%,Val Loss:   2.3,  Val Acc: 100.00%, Val F1: 100.00% Time: 12.033645153045654 *
 
 
Train time usage: 37.06019330024719
Test time usage: 0.6498298645019531
TOP: Test Loss:   2.2,  Test Acc: 62.69%, Test F1: 59.38%
SEC: Test Loss:   2.2,  Test Acc: 58.96%, Test F1: 39.70%
CONN: Test Loss:   2.2,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 14.92%,  consistency_sec_conn: 15.21%, consistency_top_sec_conn: 14.92%
              precision    recall  f1-score   support

    Temporal     0.8095    0.5763    0.6733        59
 Contingency     0.3443    0.5122    0.4118        41
  Comparison     0.8000    0.4528    0.5783        53
   Expansion     0.6593    0.7739    0.7120       115

    accuracy                         0.6269       268
   macro avg     0.6533    0.5788    0.5938       268
weighted avg     0.6720    0.6269    0.6311       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.8095    0.5763    0.6733        59
         Temporal.Synchrony     0.3239    0.5610    0.4107        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.6538    0.5312    0.5862        32
        Comparison.Contrast     0.6512    0.7850    0.7119       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5896       268
                  macro avg     0.4064    0.4089    0.3970       268
               weighted avg     0.5658    0.5896    0.5653       268

Epoch [11/20]
Train time usage: 38.55902457237244
Test time usage: 0.6013405323028564
TOP: Test Loss:   2.3,  Test Acc: 64.55%, Test F1: 63.96%
SEC: Test Loss:   2.3,  Test Acc: 60.45%, Test F1: 42.40%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.30%,  consistency_sec_conn: 15.59%, consistency_top_sec_conn: 15.30%
              precision    recall  f1-score   support

    Temporal     0.7581    0.7966    0.7769        59
 Contingency     0.3378    0.6098    0.4348        41
  Comparison     0.7619    0.6038    0.6737        53
   Expansion     0.7667    0.6000    0.6732       115

    accuracy                         0.6455       268
   macro avg     0.6561    0.6525    0.6396       268
weighted avg     0.6982    0.6455    0.6596       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7541    0.7797    0.7667        59
         Temporal.Synchrony     0.3333    0.6098    0.4310        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.6389    0.7188    0.6765        32
        Comparison.Contrast     0.7083    0.6355    0.6700       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6045       268
                  macro avg     0.4058    0.4573    0.4240       268
               weighted avg     0.5761    0.6045    0.5830       268

Epoch [12/20]
top-down:TOP: Iter:    500,  Train Loss: 6.7e+01,  Train Acc: 71.88%,Val Loss:   2.5,  Val Acc: 56.99%, Val F1: 54.69% Time: 24.219801425933838 *
top-down:SEC: Iter:    500,  Train Loss: 6.7e+01,  Train Acc: 62.50%,Val Loss:   2.5,  Val Acc: 51.81%, Val F1: 35.78% Time: 24.219801425933838 *
top-down:CONN: Iter:    500,  Train Loss: 6.7e+01,  Train Acc: 100.00%,Val Loss:   2.5,  Val Acc: 100.00%, Val F1: 100.00% Time: 24.219801425933838 *
 
 
Train time usage: 38.74960517883301
Test time usage: 0.5908732414245605
TOP: Test Loss:   2.3,  Test Acc: 66.79%, Test F1: 63.39%
SEC: Test Loss:   2.3,  Test Acc: 59.33%, Test F1: 42.60%
CONN: Test Loss:   2.3,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.21%,  consistency_sec_conn: 15.30%, consistency_top_sec_conn: 15.21%
              precision    recall  f1-score   support

    Temporal     0.7647    0.6610    0.7091        59
 Contingency     0.3750    0.4390    0.4045        41
  Comparison     0.7500    0.6226    0.6804        53
   Expansion     0.7120    0.7739    0.7417       115

    accuracy                         0.6679       268
   macro avg     0.6504    0.6241    0.6339       268
weighted avg     0.6796    0.6679    0.6708       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7647    0.6610    0.7091        59
         Temporal.Synchrony     0.3750    0.4390    0.4045        41
          Contingency.Cause     0.2941    0.2381    0.2632        21
Contingency.Pragmatic cause     0.5417    0.4062    0.4643        32
        Comparison.Contrast     0.6562    0.7850    0.7149       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.5933       268
                  macro avg     0.4386    0.4216    0.4260       268
               weighted avg     0.5755    0.5933    0.5795       268

Epoch [13/20]
Train time usage: 37.966450929641724
Test time usage: 0.5585401058197021
TOP: Test Loss:   2.4,  Test Acc: 64.55%, Test F1: 62.23%
SEC: Test Loss:   2.4,  Test Acc: 61.57%, Test F1: 45.17%
CONN: Test Loss:   2.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.50%,  consistency_sec_conn: 15.88%, consistency_top_sec_conn: 15.50%
              precision    recall  f1-score   support

    Temporal     0.7358    0.6610    0.6964        59
 Contingency     0.3231    0.5122    0.3962        41
  Comparison     0.7442    0.6038    0.6667        53
   Expansion     0.7570    0.7043    0.7297       115

    accuracy                         0.6455       268
   macro avg     0.6400    0.6203    0.6223       268
weighted avg     0.6834    0.6455    0.6589       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7358    0.6610    0.6964        59
         Temporal.Synchrony     0.3382    0.5610    0.4220        41
          Contingency.Cause     0.4000    0.0952    0.1538        21
Contingency.Pragmatic cause     0.7188    0.7188    0.7188        32
        Comparison.Contrast     0.7091    0.7290    0.7189       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6157       268
                  macro avg     0.4837    0.4608    0.4517       268
               weighted avg     0.6140    0.6157    0.6028       268

Epoch [14/20]
top-down:TOP: Iter:    600,  Train Loss: 5.8e+01,  Train Acc: 87.50%,Val Loss:   2.7,  Val Acc: 59.07%, Val F1: 55.34% Time: 35.429221630096436 *
top-down:SEC: Iter:    600,  Train Loss: 5.8e+01,  Train Acc: 78.12%,Val Loss:   2.7,  Val Acc: 54.40%, Val F1: 37.41% Time: 35.429221630096436 *
top-down:CONN: Iter:    600,  Train Loss: 5.8e+01,  Train Acc: 100.00%,Val Loss:   2.7,  Val Acc: 100.00%, Val F1: 100.00% Time: 35.429221630096436 *
 
 
Train time usage: 38.02687191963196
Test time usage: 0.9501404762268066
TOP: Test Loss:   2.4,  Test Acc: 67.16%, Test F1: 64.14%
SEC: Test Loss:   2.4,  Test Acc: 62.31%, Test F1: 42.28%
CONN: Test Loss:   2.4,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.78%,  consistency_sec_conn: 16.07%, consistency_top_sec_conn: 15.78%
              precision    recall  f1-score   support

    Temporal     0.7500    0.6610    0.7027        59
 Contingency     0.3846    0.4878    0.4301        41
  Comparison     0.8378    0.5849    0.6889        53
   Expansion     0.7087    0.7826    0.7438       115

    accuracy                         0.6716       268
   macro avg     0.6703    0.6291    0.6414       268
weighted avg     0.6937    0.6716    0.6759       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7500    0.6610    0.7027        59
         Temporal.Synchrony     0.3922    0.4878    0.4348        41
          Contingency.Cause     0.0000    0.0000    0.0000        21
Contingency.Pragmatic cause     0.7000    0.6562    0.6774        32
        Comparison.Contrast     0.6493    0.8131    0.7220       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6231       268
                  macro avg     0.4152    0.4364    0.4228       268
               weighted avg     0.5679    0.6231    0.5904       268

Epoch [15/20]
Train time usage: 38.873531103134155
Test time usage: 0.6038913726806641
TOP: Test Loss:   2.6,  Test Acc: 64.93%, Test F1: 61.79%
SEC: Test Loss:   2.6,  Test Acc: 61.19%, Test F1: 44.22%
CONN: Test Loss:   2.6,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.40%,  consistency_sec_conn: 15.78%, consistency_top_sec_conn: 15.40%
              precision    recall  f1-score   support

    Temporal     0.7600    0.6441    0.6972        59
 Contingency     0.3273    0.4390    0.3750        41
  Comparison     0.7174    0.6226    0.6667        53
   Expansion     0.7265    0.7391    0.7328       115

    accuracy                         0.6493       268
   macro avg     0.6328    0.6112    0.6179       268
weighted avg     0.6710    0.6493    0.6571       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7600    0.6441    0.6972        59
         Temporal.Synchrony     0.3455    0.4634    0.3958        41
          Contingency.Cause     0.6667    0.0952    0.1667        21
Contingency.Pragmatic cause     0.6571    0.7188    0.6866        32
        Comparison.Contrast     0.6560    0.7664    0.7069       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6119       268
                  macro avg     0.5142    0.4480    0.4422       268
               weighted avg     0.6128    0.6119    0.5913       268

Epoch [16/20]
Train time usage: 38.39244484901428
Test time usage: 0.5936200618743896
TOP: Test Loss:   2.5,  Test Acc: 67.54%, Test F1: 64.83%
SEC: Test Loss:   2.5,  Test Acc: 61.94%, Test F1: 44.72%
CONN: Test Loss:   2.5,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.78%,  consistency_sec_conn: 15.98%, consistency_top_sec_conn: 15.78%
              precision    recall  f1-score   support

    Temporal     0.7692    0.6780    0.7207        59
 Contingency     0.3889    0.5122    0.4421        41
  Comparison     0.8378    0.5849    0.6889        53
   Expansion     0.7120    0.7739    0.7417       115

    accuracy                         0.6754       268
   macro avg     0.6770    0.6372    0.6483       268
weighted avg     0.7001    0.6754    0.6808       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7843    0.6780    0.7273        59
         Temporal.Synchrony     0.3750    0.5122    0.4330        41
          Contingency.Cause     0.3333    0.1429    0.2000        21
Contingency.Pragmatic cause     0.6800    0.5312    0.5965        32
        Comparison.Contrast     0.6693    0.7944    0.7265       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6194       268
                  macro avg     0.4737    0.4431    0.4472       268
               weighted avg     0.6046    0.6194    0.6033       268

Epoch [17/20]
top-down:TOP: Iter:    700,  Train Loss: 5.8e+01,  Train Acc: 84.38%,Val Loss:   3.0,  Val Acc: 59.07%, Val F1: 56.48% Time: 12.205453157424927 *
top-down:SEC: Iter:    700,  Train Loss: 5.8e+01,  Train Acc: 78.12%,Val Loss:   3.0,  Val Acc: 53.89%, Val F1: 41.31% Time: 12.205453157424927 *
top-down:CONN: Iter:    700,  Train Loss: 5.8e+01,  Train Acc: 100.00%,Val Loss:   3.0,  Val Acc: 100.00%, Val F1: 100.00% Time: 12.205453157424927 *
 
 
Train time usage: 39.61556434631348
Test time usage: 0.5788130760192871
TOP: Test Loss:   2.7,  Test Acc: 67.91%, Test F1: 66.26%
SEC: Test Loss:   2.7,  Test Acc: 62.31%, Test F1: 47.74%
CONN: Test Loss:   2.7,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.07%,  consistency_sec_conn: 16.07%, consistency_top_sec_conn: 16.07%
              precision    recall  f1-score   support

    Temporal     0.7843    0.6780    0.7273        59
 Contingency     0.4000    0.6341    0.4906        41
  Comparison     0.7857    0.6226    0.6947        53
   Expansion     0.7545    0.7217    0.7378       115

    accuracy                         0.6791       268
   macro avg     0.6811    0.6641    0.6626       268
weighted avg     0.7130    0.6791    0.6891       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7800    0.6610    0.7156        59
         Temporal.Synchrony     0.3824    0.6341    0.4771        41
          Contingency.Cause     0.5000    0.1905    0.2759        21
Contingency.Pragmatic cause     0.7097    0.6875    0.6984        32
        Comparison.Contrast     0.6847    0.7103    0.6972       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6231       268
                  macro avg     0.5095    0.4806    0.4774       268
               weighted avg     0.6275    0.6231    0.6139       268

Epoch [18/20]
Train time usage: 36.28884768486023
Test time usage: 0.6015791893005371
TOP: Test Loss:   2.6,  Test Acc: 68.66%, Test F1: 65.43%
SEC: Test Loss:   2.6,  Test Acc: 65.30%, Test F1: 48.95%
CONN: Test Loss:   2.6,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 16.55%,  consistency_sec_conn: 16.84%, consistency_top_sec_conn: 16.55%
              precision    recall  f1-score   support

    Temporal     0.7692    0.6780    0.7207        59
 Contingency     0.4255    0.4878    0.4545        41
  Comparison     0.8378    0.5849    0.6889        53
   Expansion     0.7045    0.8087    0.7530       115

    accuracy                         0.6866       268
   macro avg     0.6843    0.6398    0.6543       268
weighted avg     0.7025    0.6866    0.6876       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7843    0.6780    0.7273        59
         Temporal.Synchrony     0.4286    0.5122    0.4667        41
          Contingency.Cause     0.8000    0.1905    0.3077        21
Contingency.Pragmatic cause     0.7500    0.6562    0.7000        32
        Comparison.Contrast     0.6593    0.8318    0.7355       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6530       268
                  macro avg     0.5704    0.4781    0.4895       268
               weighted avg     0.6537    0.6530    0.6329       268

Epoch [19/20]
top-down:TOP: Iter:    800,  Train Loss: 5.4e+01,  Train Acc: 96.88%,Val Loss:   3.1,  Val Acc: 56.48%, Val F1: 53.97% Time: 20.625184774398804 
top-down:SEC: Iter:    800,  Train Loss: 5.4e+01,  Train Acc: 84.38%,Val Loss:   3.1,  Val Acc: 51.30%, Val F1: 36.39% Time: 20.625184774398804 
top-down:CONN: Iter:    800,  Train Loss: 5.4e+01,  Train Acc: 100.00%,Val Loss:   3.1,  Val Acc: 100.00%, Val F1: 100.00% Time: 20.625184774398804 
 
 
Train time usage: 34.23115301132202
Test time usage: 0.5016672611236572
TOP: Test Loss:   2.7,  Test Acc: 66.42%, Test F1: 64.13%
SEC: Test Loss:   2.7,  Test Acc: 63.06%, Test F1: 47.87%
CONN: Test Loss:   2.7,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.98%,  consistency_sec_conn: 16.27%, consistency_top_sec_conn: 15.98%
              precision    recall  f1-score   support

    Temporal     0.7407    0.6780    0.7080        59
 Contingency     0.3860    0.5366    0.4490        41
  Comparison     0.7805    0.6038    0.6809        53
   Expansion     0.7241    0.7304    0.7273       115

    accuracy                         0.6642       268
   macro avg     0.6578    0.6372    0.6413       268
weighted avg     0.6872    0.6642    0.6713       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7547    0.6780    0.7143        59
         Temporal.Synchrony     0.3966    0.5610    0.4646        41
          Contingency.Cause     0.5000    0.1905    0.2759        21
Contingency.Pragmatic cause     0.7333    0.6875    0.7097        32
        Comparison.Contrast     0.6723    0.7477    0.7080       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6306       268
                  macro avg     0.5095    0.4774    0.4787       268
               weighted avg     0.6220    0.6306    0.6173       268

Epoch [20/20]
Train time usage: 32.36518740653992
Test time usage: 0.5842688083648682
TOP: Test Loss:   2.7,  Test Acc: 66.79%, Test F1: 64.57%
SEC: Test Loss:   2.7,  Test Acc: 63.06%, Test F1: 47.88%
CONN: Test Loss:   2.7,  Test Acc: 100.00%, Test F1: 100.00%
consistency_top_sec: 15.98%,  consistency_sec_conn: 16.27%, consistency_top_sec_conn: 15.98%
              precision    recall  f1-score   support

    Temporal     0.7547    0.6780    0.7143        59
 Contingency     0.3729    0.5366    0.4400        41
  Comparison     0.8205    0.6038    0.6957        53
   Expansion     0.7265    0.7391    0.7328       115

    accuracy                         0.6679       268
   macro avg     0.6687    0.6394    0.6457       268
weighted avg     0.6972    0.6679    0.6766       268

                             precision    recall  f1-score   support

      Temporal.Asynchronous     0.7692    0.6780    0.7207        59
         Temporal.Synchrony     0.3860    0.5366    0.4490        41
          Contingency.Cause     0.5714    0.1905    0.2857        21
Contingency.Pragmatic cause     0.7333    0.6875    0.7097        32
        Comparison.Contrast     0.6639    0.7570    0.7074       107
      Comparison.Concession     0.0000    0.0000    0.0000         8

                   accuracy                         0.6306       268
                  macro avg     0.5206    0.4749    0.4788       268
               weighted avg     0.6258    0.6306    0.6169       268

dev_best_acc_top: 59.07%,  dev_best_f1_top: 56.48%, 
dev_best_acc_sec: 53.89%,  dev_best_f1_sec: 41.31%, 
dev_best_acc_conn: 100.00%,  dev_best_f1_conn: 100.00%
